{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY= os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_TRACING_V2= os.getenv('LANGCHAIN_TRACING_V2')\n",
    "LANGCHAIN_ENDPOINT= os.getenv('LANGCHAIN_ENDPOINT')\n",
    "LANGCHAIN_API_KEY= os.getenv('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "## Load documents\n",
    "loader= WebBaseLoader(\n",
    "    web_paths= (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs= dict(\n",
    "        parse_only= bs4.SoupStrainer(\n",
    "            class_= (\"post-content\", \"post_title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "## Load content\n",
    "blog_docs= loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the docs and creating a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter= RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "splits= text_splitter.split_documents(blog_docs)\n",
    "\n",
    "##Retriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore= Chroma.from_documents(documents= splits,\n",
    "                                   embedding= OpenAIEmbeddings())\n",
    "retriever= vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Multi Query\n",
    "* Here we learn how multi query concept is used to pick the best chunks of data\n",
    "* Usually there may be questions which might not have exact meaning that user wants to ask\n",
    "* So In Multi query We will create a chian link which says as below:\n",
    "    * Take the question\n",
    "    * Create 5 different forms of same question\n",
    "    * Loop each question\n",
    "        * Get the related chunks of data of question\n",
    "        * Append all the chunks (unique chunk of data)\n",
    "        * Create a chain which gives related chunks of data (just as we create from llm output)\n",
    "    * Get all the chunks of data and append to the context and pass it to LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create prompt and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "## Multi Query\n",
    "template= \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five different versions of the\n",
    "given user question to retrieve relevant documents from a vector database. By generating multiple \n",
    "perspectives on the user question, your goal is to help the user overcome some of the limitations of \n",
    "the distance-based similarity search.\n",
    "Provide these alternative questions seperated by new line.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "prompt_perspective= ChatPromptTemplate.from_template(template)\n",
    "\n",
    "##Create chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries= (\n",
    "    prompt_perspective\n",
    "    | ChatOpenAI(temperature=0.1)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\")) ## This will return list of questions with different perspectives\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the questions loop every question and get the required docs from vectorstore db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/langchain_core-0.1.33rc1-py3.10.egg/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import loads, dumps\n",
    "\n",
    "##Function to get the list of docs\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\"Unique union of required docs\"\"\"\n",
    "    ## Flatten the list of list and convert each document to string\n",
    "    flatten_docs= [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    ## Get unique docs\n",
    "    unique_docs= list(set(flatten_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "##Retriever\n",
    "question= \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain= (\n",
    "    generate_queries\n",
    "    | retriever.map() | get_unique_union\n",
    ")\n",
    "docs= retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now get these docs and check question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals. This enables the agent to efficiently handle complex tasks by dividing them into smaller and simpler steps. Task decomposition can be achieved through techniques such as Chain of Thought (CoT) and Tree of Thoughts, as well as through simple prompting or task-specific instructions.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "## RAG\n",
    "template= \"\"\"  \n",
    "Answer the following question based on this context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt= ChatPromptTemplate.from_template(template)\n",
    "llm= ChatOpenAI(temperature=0.1)\n",
    "final_rag_chain=(\n",
    "    {\"context\": retrieval_chain,\n",
    "        \"question\":itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# final_rag_chain.invoke( question)\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Fusion\n",
    "* This is just like Multi query where it asks to create different questions out of single provided question.\n",
    "* But when the docs are retrieved it will rank those documents using Reciprocal rank fusion\n",
    "* Picks the best docs and then respond back to the actual question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# Rag Fusion related prompt\n",
    "template=\"\"\" \n",
    "You are a helpful assistant that generates multiple search queries based on a single query input query.\n",
    "Generate multiple search queries related to: {question}\\n\n",
    "output (4 queries):\n",
    "\"\"\"\n",
    "prompt_rag_fusion= ChatPromptTemplate.from_template(template= template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate queries\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatOpenAI(temperature=0.1)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: TypeError('sequence item 0: expected str instance, ReadTimeoutError found')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps. This can be achieved through techniques like Chain of Thought (CoT) and Tree of Thoughts, which prompt the model to think step by step and explore multiple reasoning possibilities at each step. Task decomposition can be done using simple prompting, task-specific instructions, or with human inputs to help the LLM agent effectively plan and execute tasks.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template= \"\"\"\n",
    "    Answer the following question based on this context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt= ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain= (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition\n",
    "\n",
    "* We will split the question into sub questions and pass it to llm to get docs\n",
    "    * Questions we get will be decomposed to multiple questions\n",
    "    * Eg: What is the structure of Ameoba in sea?\n",
    "    * The above question is divided into 3 parts(3 is dynamic)\n",
    "        * In what sea do we find Ameoba\n",
    "        * What is the life history of Ameoba\n",
    "        * Waht is the structure of Ameoba\n",
    "    * The prompt dynamically changes for each question\n",
    "    * eg: We have got first question please find the answer for that and over all context\n",
    "    * For third question: We have got details of first and second question and over all context, please answer for the third question\n",
    "    * and so on..\n",
    "    * The above method is call Answering recursively\n",
    "* One more method is answering individually where the answers we take from different questions doesn't need to recursively go to next question context. Instead it will append one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "template=\"\"\"\n",
    "        You are helpful assitant that generates multiple sub-questions related to an input question.\n",
    "        The goal is to break down the input into a set of Sub-problems / Sub-questions that can be answered in isolation.\n",
    "        \\n Generate multiple search queries related to: {question} \\n\n",
    "        Output (3 queries):\n",
    "\"\"\"\n",
    "prompt_decomposition= ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "## llm\n",
    "llm= ChatOpenAI(temperature=0.1)\n",
    "#Chain\n",
    "generate_queries_decomposition= (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split('\\n'))\n",
    ")\n",
    "# Run\n",
    "question= \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions= generate_queries_decomposition.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the role of machine learning in an LLM-powered autonomous agent system?',\n",
       " '2. How do the components of an LLM-powered autonomous agent system work together?',\n",
       " '3. What are the specific functions of each component in an LLM-powered autonomous agent system?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering recursively\n",
    "* We create a template which answers recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

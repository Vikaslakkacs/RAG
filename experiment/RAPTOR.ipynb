{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAPTOR Overview](../RAPTOR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that return number of tokens from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name:str) -> int:\n",
    "    ## We will use tiktoken to get the token details\n",
    "    encoding= tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens= len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"Hello, I am Vikas\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Langchain docs LCEL docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader= RecursiveUrlLoader(\n",
    "    url= url,\n",
    "    max_depth= 20,\n",
    "    extractor= lambda x:Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains (we‚Äôve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We‚Äôre currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it‚Äôs often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it‚Äôs available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment\\nAny chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousWeb scrapingNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'content_type': 'text/html; charset=utf-8', 'title': 'LangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nParallel: Format data | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguagePrimitivesParallel: Format dataOn this pageFormatting inputs & outputThe RunnableParallel primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.It is useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.Here the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key.%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())retrieval_chain.invoke(\"where did harrison work?\")\\'Harrison worked at Kensho.\\'::: {.callout-tip}\\nNote that when composing a RunnableParallel with another Runnable we don\\'t even need to wrap our dictionary in the RunnableParallel class ‚Äî\\xa0the type conversion is handled for us. In the context of a chain, these are equivalent:\\n:::{\"context\": retriever, \"question\": RunnablePassthrough()}RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})RunnableParallel(context=retriever, question=RunnablePassthrough())Using itemgetter as shorthand\\u200bNote that you can use Python\\'s itemgetter as shorthand to extract data from the map when combining with RunnableParallel. You can find more information about itemgetter in the Python Documentation. In the example below, we use itemgetter to extract specific keys from the map:from operator import itemgetterfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}Answer in the following language: {language}\"\"\"prompt = ChatPromptTemplate.from_template(template)chain = (    {        \"context\": itemgetter(\"question\") | retriever,        \"question\": itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\\'Harrison ha lavorato a Kensho.\\'Parallelize steps\\u200bRunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.from langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableParallelfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | modelpoem_chain = (    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model)map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)map_chain.invoke({\"topic\": \"bear\"}){\\'joke\\': AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"), \\'poem\\': AIMessage(content=\"In the wild\\'s embrace, bear roams free,\\\\nStrength and grace, a majestic decree.\")}Parallelism\\u200bRunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two.%%timeitjoke_chain.invoke({\"topic\": \"bear\"})958 ms ¬± 402 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)%%timeitpoem_chain.invoke({\"topic\": \"bear\"})1.22 s ¬± 508 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)%%timeitmap_chain.invoke({\"topic\": \"bear\"})1.15 s ¬± 119 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)Help us out by providing feedback on this documentation page:PreviousSequences: Chaining runnablesNextBinding: Attach runtime argsUsing itemgetter as shorthandParallelize stepsParallelismCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/primitives/parallel/', 'content_type': 'text/html; charset=utf-8', 'title': 'Parallel: Format data | ü¶úÔ∏èüîó LangChain', 'description': 'The RunnableParallel primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nConfigure runtime chain internals | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguagePrimitivesConfigure runtime chain internalsOn this pageConfigure chain internals at runtimeOftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things.\\nIn order to make this experience as easy as possible, we have defined two methods.First, a configurable_fields method.\\nThis lets you configure particular fields of a runnable.Second, a configurable_alternatives method.\\nWith this method, you can list out alternatives for any particular runnable that can be set during runtime.Configuration Fields\\u200bWith LLMs\\u200bWith LLMs we can configure things like temperature%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_core.prompts import PromptTemplatefrom langchain_core.runnables import ConfigurableFieldfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(temperature=0).configurable_fields(    temperature=ConfigurableField(        id=\"llm_temperature\",        name=\"LLM Temperature\",        description=\"The temperature of the LLM\",    ))model.invoke(\"pick a random number\")AIMessage(content=\\'7\\')model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"pick a random number\")AIMessage(content=\\'34\\')We can also do this when its used as part of a chainprompt = PromptTemplate.from_template(\"Pick a random number above {x}\")chain = prompt | modelchain.invoke({\"x\": 0})AIMessage(content=\\'57\\')chain.with_config(configurable={\"llm_temperature\": 0.9}).invoke({\"x\": 0})AIMessage(content=\\'6\\')With HubRunnables\\u200bThis is useful to allow for switching of promptsfrom langchain.runnables.hub import HubRunnableprompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(    owner_repo_commit=ConfigurableField(        id=\"hub_commit\",        name=\"Hub Commit\",        description=\"The Hub commit to pull from\",    ))prompt.invoke({\"question\": \"foo\", \"context\": \"bar\"})ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\\\nQuestion: foo \\\\nContext: bar \\\\nAnswer:\")])prompt.with_config(configurable={\"hub_commit\": \"rlm/rag-prompt-llama\"}).invoke(    {\"question\": \"foo\", \"context\": \"bar\"})ChatPromptValue(messages=[HumanMessage(content=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.<</SYS>> \\\\nQuestion: foo \\\\nContext: bar \\\\nAnswer: [/INST]\")])Configurable Alternatives\\u200bWith LLMs\\u200bLet\\'s take a look at doing this with LLMsfrom langchain_community.chat_models import ChatAnthropicfrom langchain_core.prompts import PromptTemplatefrom langchain_core.runnables import ConfigurableFieldfrom langchain_openai import ChatOpenAIllm = ChatAnthropic(temperature=0).configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"llm\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"anthropic\",    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`    openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # You can add more configuration options here)prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")chain = prompt | llm# By default it will call Anthropicchain.invoke({\"topic\": \"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do you call a bear with no teeth?\\\\nA gummy bear!\")# We can use `.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to usechain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")# If we use the `default_key` then it uses the defaultchain.with_config(configurable={\"llm\": \"anthropic\"}).invoke({\"topic\": \"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do you call a bear with no teeth?\\\\nA gummy bear!\")With Prompts\\u200bWe can do a similar thing, but alternate between promptsllm = ChatAnthropic(temperature=0)prompt = PromptTemplate.from_template(    \"Tell me a joke about {topic}\").configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"prompt\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"joke\",    # This adds a new option, with name `poem`    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),    # You can add more configuration options here)chain = prompt | llm# By default it will write a jokechain.invoke({\"topic\": \"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do you call a bear with no teeth?\\\\nA gummy bear!\")# We can configure it write a poemchain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"bears\"})AIMessage(content=\\' Here is a short poem about bears:\\\\n\\\\nThe bears awaken from their sleep\\\\nAnd lumber out into the deep\\\\nForests filled with trees so tall\\\\nForaging for food before nightfall \\\\nTheir furry coats and claws so sharp\\\\nSniffing for berries and fish to nab\\\\nLumbering about without a care\\\\nThe mighty grizzly and black bear\\\\nProud creatures, wild and free\\\\nRuling their domain majestically\\\\nWandering the woods they call their own\\\\nBefore returning to their dens alone\\')With Prompts and LLMs\\u200bWe can also have multiple things configurable!\\nHere\\'s an example doing that with both prompts and LLMs.llm = ChatAnthropic(temperature=0).configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"llm\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"anthropic\",    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`    openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # You can add more configuration options here)prompt = PromptTemplate.from_template(    \"Tell me a joke about {topic}\").configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"prompt\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"joke\",    # This adds a new option, with name `poem`    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),    # You can add more configuration options here)chain = prompt | llm# We can configure it write a poem with OpenAIchain.with_config(configurable={\"prompt\": \"poem\", \"llm\": \"openai\"}).invoke(    {\"topic\": \"bears\"})AIMessage(content=\"In the forest, where tall trees sway,\\\\nA creature roams, both fierce and gray.\\\\nWith mighty paws and piercing eyes,\\\\nThe bear, a symbol of strength, defies.\\\\n\\\\nThrough snow-kissed mountains, it does roam,\\\\nA guardian of its woodland home.\\\\nWith fur so thick, a shield of might,\\\\nIt braves the coldest winter night.\\\\n\\\\nA gentle giant, yet wild and free,\\\\nThe bear commands respect, you see.\\\\nWith every step, it leaves a trace,\\\\nOf untamed power and ancient grace.\\\\n\\\\nFrom honeyed feast to salmon\\'s leap,\\\\nIt takes its place, in nature\\'s keep.\\\\nA symbol of untamed delight,\\\\nThe bear, a wonder, day and night.\\\\n\\\\nSo let us honor this noble beast,\\\\nIn forests where its soul finds peace.\\\\nFor in its presence, we come to know,\\\\nThe untamed spirit that in us also flows.\")# We can always just configure only one if we wantchain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")Saving configurations\\u200bWe can also easily save configured chains as their own objectsopenai_joke = chain.with_config(configurable={\"llm\": \"openai\"})openai_joke.invoke({\"topic\": \"bears\"})AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")Help us out by providing feedback on this documentation page:PreviousAssign: Add values to stateNextPrimitivesConfiguration FieldsWith LLMsWith HubRunnablesConfigurable AlternativesWith LLMsWith PromptsWith Prompts and LLMsSaving configurationsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/primitives/configure/', 'content_type': 'text/html; charset=utf-8', 'title': 'Configure runtime chain internals | ü¶úÔ∏èüîó LangChain', 'description': 'Oftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nPrimitives | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguagePrimitivesPrimitivesIn addition to various components that are usable with LCEL, LangChain also includes various primitives\\nthat help pass around and format data, bind arguments, invoke custom logic, and more.This section goes into greater depth on where and how some of these components are useful.üìÑÔ∏è Sequences: Chaining runnablesOne key advantage of the Runnable interface is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable\\'s .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing. The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or piped just like any other runnable.üìÑÔ∏è Parallel: Format dataThe RunnableParallel primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.üìÑÔ∏è Binding: Attach runtime argsSometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to pass these arguments in.üìÑÔ∏è Lambda: Run custom functionsYou can use arbitrary functions in the pipeline.üìÑÔ∏è Passthrough: Pass through inputsRunnablePassthrough on its own allows you to pass inputs unchanged. This typically is used in conjuction with RunnableParallel to pass data through to a new key in the map.üìÑÔ∏è Assign: Add values to stateThe RunnablePassthrough.assign(...) static method takes an input value and adds the extra arguments passed to the assign function.üìÑÔ∏è Configure runtime chain internalsOftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things.Help us out by providing feedback on this documentation page:PreviousRunnable interfaceNextSequences: Chaining runnablesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/primitives/', 'content_type': 'text/html; charset=utf-8', 'title': 'Primitives | ü¶úÔ∏èüîó LangChain', 'description': 'In addition to various components that are usable with LCEL, LangChain also includes various primitives', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nStreaming | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageStreamingOn this pageStreaming With LangChainStreaming is critical in making applications based on LLMs feel responsive to end-users.Important LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface.This interface provides two general approaches to stream content:sync stream and async astream: a default implementation of streaming that streams the final output from the chain.async astream_events and async astream_log: these provide a way to stream both intermediate steps and final output from the chain.Let\\'s take a look at both approaches, and try to understand how to use them. ü•∑Using Stream\\u200bAll Runnable objects implement a sync method called stream and an async variant called astream. These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.Streaming is only possible if all steps in the program know how to process an input stream; i.e., process an input chunk one at a time, and yield a corresponding output chunk.The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.The best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!LLMs and Chat Models\\u200bLarge language models and their chat variants are the primary bottleneck in LLM based apps. üôäLarge language models can take several seconds to generate a complete response to a query. This is far slower than the ~200-300 ms threshold at which an application feels responsive to an end user.The key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model token by token.We will show examples of streaming using the chat model from Anthropic. To use the model, you will need to install the langchain-anthropic package. You can do this with the following command:pip install -qU langchain-anthropic# Showing the example using anthropic, but you can use# your favorite chat model!from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic()chunks = []async for chunk in model.astream(\"hello. tell me something about yourself\"):    chunks.append(chunk)    print(chunk.content, end=\"|\", flush=True) Hello|!| My| name| is| Claude|.| I|\\'m| an| AI| assistant| created| by| An|throp|ic| to| be| helpful|,| harmless|,| and| honest|.||Let\\'s inspect one of the chunkschunks[0]AIMessageChunk(content=\\' Hello\\')We got back something called an AIMessageChunk. This chunk represents a part of an AIMessage.Message chunks are additive by design -- one can simply add them up to get the state of the response so far!chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]AIMessageChunk(content=\\' Hello! My name is\\')Chains\\u200bVirtually all LLM applications involve more steps than just a call to a language model.Let\\'s build a simple chain using LangChain Expression Language (LCEL) that combines a prompt, model and a parser and verify that streaming works.We will use StrOutputParser to parse the output from the model. This is a simple parser that extracts the content field from an AIMessageChunk, giving us the token returned by the model.tipLCEL is a declarative way to specify a \"program\" by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of stream and astream allowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = StrOutputParser()chain = prompt | model | parserasync for chunk in chain.astream({\"topic\": \"parrot\"}):    print(chunk, end=\"|\", flush=True) Here|\\'s| a| silly| joke| about| a| par|rot|:|What| kind| of| teacher| gives| good| advice|?| An| ap|-|parent| (|app|arent|)| one|!||You might notice above that parser actually doesn\\'t block the streaming output from the model, and instead processes each chunk individually. Many of the LCEL primitives also support this kind of transform-style passthrough streaming, which can be very convenient when constructing apps.Certain runnables, like prompt templates and chat models, cannot process individual chunks and instead aggregate all previous steps. This will interrupt the streaming process. Custom functions can be designed to return generators, whichnoteIf the above functionality is not relevant to what you\\'re building, you do not have to use the LangChain Expression Language to use LangChain and can instead rely on a standard imperative programming approach by\\ncaling invoke, batch or stream on each component individually, assigning the results to variables and then using them downstream as you see fit.If that works for your needs, then that\\'s fine by us üëå!Working with Input Streams\\u200bWhat if you wanted to stream JSON from the output as it was being generated?If you were to rely on json.loads to parse the partial json, the parsing would fail as the partial json wouldn\\'t be valid json.You\\'d likely be at a complete loss of what to do and claim that it wasn\\'t possible to stream JSON.Well, turns out there is a way to do it -- the parser needs to operate on the input stream, and attempt to \"auto-complete\" the partial json into a valid state.Let\\'s see such a parser in action to understand what this means.from langchain_core.output_parsers import JsonOutputParserchain = (    model | JsonOutputParser())  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some modelsasync for text in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'):    print(text, flush=True){}{\\'countries\\': []}{\\'countries\\': [{}]}{\\'countries\\': [{\\'name\\': \\'\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 6739}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 673915}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Sp\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 4675}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 467547}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\', \\'population\\': 12}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\', \\'population\\': 12647}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\', \\'population\\': 1264764}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\', \\'population\\': 126476461}]}Now, let\\'s break streaming. We\\'ll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON.dangerAny steps in the chain that operate on finalized inputs rather than on input streams can break streaming functionality via stream or astream.tipLater, we will discuss the astream_events API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on finalized inputs.from langchain_core.output_parsers import (    JsonOutputParser,)# A function that operates on finalized inputs# rather than on an input_streamdef _extract_country_names(inputs):    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if \"countries\" not in inputs:        return \"\"    countries = inputs[\"countries\"]    if not isinstance(countries, list):        return \"\"    country_names = [        country.get(\"name\") for country in countries if isinstance(country, dict)    ]    return country_nameschain = model | JsonOutputParser() | _extract_country_namesasync for text in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'):    print(text, end=\"|\", flush=True)[\\'France\\', \\'Spain\\', \\'Japan\\']|Generator Functions\\u200bLe\\'ts fix the streaming using a generator function that can operate on the input stream.tipA generator function (a function that uses yield) allows writing code that operators on input streamsfrom langchain_core.output_parsers import JsonOutputParserasync def _extract_country_names_streaming(input_stream):    \"\"\"A function that operates on input streams.\"\"\"    country_names_so_far = set()    async for input in input_stream:        if not isinstance(input, dict):            continue        if \"countries\" not in input:            continue        countries = input[\"countries\"]        if not isinstance(countries, list):            continue        for country in countries:            name = country.get(\"name\")            if not name:                continue            if name not in country_names_so_far:                yield name                country_names_so_far.add(name)chain = model | JsonOutputParser() | _extract_country_names_streamingasync for text in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'):    print(text, end=\"|\", flush=True)France|Sp|Spain|Japan|noteBecause the code above is relying on JSON auto-completion, you may see partial names of countries (e.g., Sp and Spain), which is not what one would want for an extraction result!We\\'re focusing on streaming concepts, not necessarily the results of the chains.Non-streaming components\\u200bSome built-in components like Retrievers do not offer any streaming. What happens if we try to stream them? ü§®from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    [\"harrison worked at kensho\", \"harrison likes spicy food\"],    embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]chunks[[Document(page_content=\\'harrison worked at kensho\\'),  Document(page_content=\\'harrison likes spicy food\\')]]Stream just yielded the final result from that component.This is OK \\U0001f979! Not all components have to implement streaming -- in some cases streaming is either unnecessary, difficult or just doesn\\'t make sense.tipAn LCEL chain constructed using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.retrieval_chain = (    {        \"context\": retriever.with_config(run_name=\"Docs\"),        \"question\": RunnablePassthrough(),    }    | prompt    | model    | StrOutputParser())for chunk in retrieval_chain.stream(    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"):    print(chunk, end=\"|\", flush=True) Based| on| the| given| context|,| the| only| information| provided| about| where| Harrison| worked| is| that| he| worked| at| Ken|sh|o|.| Since| there| are| no| other| details| provided| about| Ken|sh|o|,| I| do| not| have| enough| information| to| write| 3| additional| made| up| sentences| about| this| place|.| I| can| only| state| that| Harrison| worked| at| Ken|sh|o|.||Now that we\\'ve seen how stream and astream work, let\\'s venture into the world of streaming events. üèûÔ∏èUsing Stream Events\\u200bEvent Streaming is a beta API. This API may change a bit based on feedback.noteIntroduced in langchain-core 0.1.14.import langchain_corelangchain_core.__version__\\'0.1.18\\'For the astream_events API to work properly:Use async throughout the code to the extent possible (e.g., async tools etc)Propagate callbacks if defining custom functions / runnablesWhenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens.Let us know if anything doesn\\'t work as expected! :)Event Reference\\u200bBelow is a reference table that shows some events that might be emitted by the various Runnable objects.noteWhen streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that inputs will often be included only for end events and rather than for start events.eventnamechunkinputoutputon_chat_model_start[model name]{\"messages\": [[SystemMessage, HumanMessage]]}on_chat_model_stream[model name]AIMessageChunk(content=\"hello\")on_chat_model_end[model name]{\"messages\": [[SystemMessage, HumanMessage]]}{\"generations\": [...], \"llm_output\": None, ...}on_llm_start[model name]{\\'input\\': \\'hello\\'}on_llm_stream[model name]\\'Hello\\'on_llm_end[model name]\\'Hello human!\\'on_chain_startformat_docson_chain_streamformat_docs\"hello world!, goodbye world!\"on_chain_endformat_docs[Document(...)]\"hello world!, goodbye world!\"on_tool_startsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_streamsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_endsome_tool{\"x\": 1, \"y\": \"2\"}on_retriever_start[retriever name]{\"query\": \"hello\"}on_retriever_chunk[retriever name]{documents: [...]}on_retriever_end[retriever name]{\"query\": \"hello\"}{documents: [...]}on_prompt_start[template_name]{\"question\": \"hello\"}on_prompt_end[template_name]{\"question\": \"hello\"}ChatPromptValue(messages: [SystemMessage, ...])Chat Model\\u200bLet\\'s start off by looking at the events produced by a chat model.events = []async for event in model.astream_events(\"hello\", version=\"v1\"):    events.append(event)/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.  warn_beta(noteHey what\\'s that funny version=\"v1\" parameter in the API?! üòæThis is a beta API, and we\\'re almost certainly going to make some changes to it.This version parameter will allow us to minimize such breaking changes to your code. In short, we are annoying you now, so we don\\'t have to annoy you later.Let\\'s take a look at the few of the start event and a few of the end events.events[:3][{\\'event\\': \\'on_chat_model_start\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'name\\': \\'ChatAnthropic\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'data\\': {\\'input\\': \\'hello\\'}}, {\\'event\\': \\'on_chat_model_stream\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'name\\': \\'ChatAnthropic\\',  \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' Hello\\')}}, {\\'event\\': \\'on_chat_model_stream\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'name\\': \\'ChatAnthropic\\',  \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\'!\\')}}]events[-2:][{\\'event\\': \\'on_chat_model_stream\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'name\\': \\'ChatAnthropic\\',  \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\'\\')}}, {\\'event\\': \\'on_chat_model_end\\',  \\'name\\': \\'ChatAnthropic\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'data\\': {\\'output\\': AIMessageChunk(content=\\' Hello!\\')}}]Chain\\u200bLet\\'s revisit the example chain that parsed streaming JSON to explore the streaming events API.chain = (    model | JsonOutputParser())  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some modelsevents = [    event    async for event in chain.astream_events(        \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',        version=\"v1\",    )]If you examine at the first few events, you\\'ll notice that there are 3 different start events rather than 2 start events.The three start events correspond to:The chain (model + parser)The modelThe parserevents[:3][{\\'event\\': \\'on_chain_start\\',  \\'run_id\\': \\'b1074bff-2a17-458b-9e7b-625211710df4\\',  \\'name\\': \\'RunnableSequence\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'data\\': {\\'input\\': \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'}}, {\\'event\\': \\'on_chat_model_start\\',  \\'name\\': \\'ChatAnthropic\\',  \\'run_id\\': \\'6072be59-1f43-4f1c-9470-3b92e8406a99\\',  \\'tags\\': [\\'seq:step:1\\'],  \\'metadata\\': {},  \\'data\\': {\\'input\\': {\\'messages\\': [[HumanMessage(content=\\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\')]]}}}, {\\'event\\': \\'on_parser_start\\',  \\'name\\': \\'JsonOutputParser\\',  \\'run_id\\': \\'bf978194-0eda-4494-ad15-3a5bfe69cd59\\',  \\'tags\\': [\\'seq:step:2\\'],  \\'metadata\\': {},  \\'data\\': {}}]What do you think you\\'d see if you looked at the last 3 events? what about the middle?Let\\'s use this API to take output the stream events from the model and the parser. We\\'re ignoring start events, end events and events from the chain.num_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(            f\"Chat model chunk: {repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        )    if kind == \"on_parser_stream\":        print(f\"Parser chunk: {event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if num_events > 30:        # Truncate the output        print(\"...\")        breakChat model chunk: \\' Here\\'Chat model chunk: \\' is\\'Chat model chunk: \\' the\\'Chat model chunk: \\' JSON\\'Chat model chunk: \\' with\\'Chat model chunk: \\' the\\'Chat model chunk: \\' requested\\'Chat model chunk: \\' countries\\'Chat model chunk: \\' and\\'Chat model chunk: \\' their\\'Chat model chunk: \\' populations\\'Chat model chunk: \\':\\'Chat model chunk: \\'\\\\n\\\\n```\\'Chat model chunk: \\'json\\'Parser chunk: {}Chat model chunk: \\'\\\\n{\\'Chat model chunk: \\'\\\\n \\'Chat model chunk: \\' \"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: {\\'countries\\': []}Chat model chunk: \\' [\\'Chat model chunk: \\'\\\\n   \\'Parser chunk: {\\'countries\\': [{}]}Chat model chunk: \\' {\\'...Because both the model and the parser support streaming, we see sreaming events from both components in real time! Kind of cool isn\\'t it? ü¶úFiltering Events\\u200bBecause this API produces so many events, it is useful to be able to filter on events.You can filter by either component name, component tags or component type.By Name\\u200bchain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(    {\"run_name\": \"my_parser\"})max_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",    include_names=[\"my_parser\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_parser_start\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': []}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'\\'}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\'}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 6739}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 673915}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {}]}}}...By Type\\u200bchain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(    {\"run_name\": \"my_parser\"})max_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",    include_types=[\"chat_model\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_chat_model_start\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'input\\': {\\'messages\\': [[HumanMessage(content=\\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\')]]}}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' Here\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' is\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' the\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' JSON\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' with\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' the\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' requested\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' countries\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' and\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' their\\')}}...By Tags\\u200bcautionTags are inherited by child components of a given runnable. If you\\'re using tags to filter, make sure that this is what you want.chain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})max_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",    include_tags=[\"my_chain\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_chain_start\\', \\'run_id\\': \\'190875f3-3fb7-49ad-9b6e-f49da22f3e49\\', \\'name\\': \\'RunnableSequence\\', \\'tags\\': [\\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'}}{\\'event\\': \\'on_chat_model_start\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'input\\': {\\'messages\\': [[HumanMessage(content=\\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\')]]}}}{\\'event\\': \\'on_parser_start\\', \\'name\\': \\'JsonOutputParser\\', \\'run_id\\': \\'3b5e4ca1-40fe-4a02-9a19-ba2a43a6115c\\', \\'tags\\': [\\'seq:step:2\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' Here\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' is\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' the\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' JSON\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' with\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' the\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' requested\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' countries\\')}}...Non-streaming components\\u200bRemember how some components don\\'t stream well because they don\\'t operate on input streams?While such components can break streaming of the final output when using astream, astream_events will still yield streaming events from intermediate steps that support streaming!# Function that does not support streaming.# It operates on the finalizes inputs rather than# operating on the input stream.def _extract_country_names(inputs):    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if \"countries\" not in inputs:        return \"\"    countries = inputs[\"countries\"]    if not isinstance(countries, list):        return \"\"    country_names = [        country.get(\"name\") for country in countries if isinstance(country, dict)    ]    return country_nameschain = (    model | JsonOutputParser() | _extract_country_names)  # This parser only works with OpenAI right nowAs expected, the astream API doesn\\'t work correctly because _extract_country_names doesn\\'t operate on streams.async for chunk in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',):    print(chunk, flush=True)[\\'France\\', \\'Spain\\', \\'Japan\\']Now, let\\'s confirm that with astream_events we\\'re still seeing streaming output from the model and the parser.num_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(            f\"Chat model chunk: {repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        )    if kind == \"on_parser_stream\":        print(f\"Parser chunk: {event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if num_events > 30:        # Truncate the output        print(\"...\")        breakChat model chunk: \\' Here\\'Chat model chunk: \\' is\\'Chat model chunk: \\' the\\'Chat model chunk: \\' JSON\\'Chat model chunk: \\' with\\'Chat model chunk: \\' the\\'Chat model chunk: \\' requested\\'Chat model chunk: \\' countries\\'Chat model chunk: \\' and\\'Chat model chunk: \\' their\\'Chat model chunk: \\' populations\\'Chat model chunk: \\':\\'Chat model chunk: \\'\\\\n\\\\n```\\'Chat model chunk: \\'json\\'Parser chunk: {}Chat model chunk: \\'\\\\n{\\'Chat model chunk: \\'\\\\n \\'Chat model chunk: \\' \"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: {\\'countries\\': []}Chat model chunk: \\' [\\'Chat model chunk: \\'\\\\n   \\'Parser chunk: {\\'countries\\': [{}]}Chat model chunk: \\' {\\'Chat model chunk: \\'\\\\n     \\'Chat model chunk: \\' \"\\'...Propagating Callbacks\\u200bcautionIf you\\'re using invoking runnables inside your tools, you need to propagate callbacks to the runnable; otherwise, no stream events will be generated.noteWhen using RunnableLambdas or @chain decorator, callbacks are propagated automatically behind the scenes.from langchain_core.runnables import RunnableLambdafrom langchain_core.tools import tooldef reverse_word(word: str):    return word[::-1]reverse_word = RunnableLambda(reverse_word)@tooldef bad_tool(word: str):    \"\"\"Custom tool that doesn\\'t propagate callbacks.\"\"\"    return reverse_word.invoke(word)async for event in bad_tool.astream_events(\"hello\", version=\"v1\"):    print(event){\\'event\\': \\'on_tool_start\\', \\'run_id\\': \\'ae7690f8-ebc9-4886-9bbe-cb336ff274f2\\', \\'name\\': \\'bad_tool\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\'}}{\\'event\\': \\'on_tool_stream\\', \\'run_id\\': \\'ae7690f8-ebc9-4886-9bbe-cb336ff274f2\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'bad_tool\\', \\'data\\': {\\'chunk\\': \\'olleh\\'}}{\\'event\\': \\'on_tool_end\\', \\'name\\': \\'bad_tool\\', \\'run_id\\': \\'ae7690f8-ebc9-4886-9bbe-cb336ff274f2\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'olleh\\'}}Here\\'s a re-implementation that does propagate callbacks correctly. You\\'ll notice that now we\\'re getting events from the reverse_word runnable as well.@tooldef correct_tool(word: str, callbacks):    \"\"\"A tool that correctly propagates callbacks.\"\"\"    return reverse_word.invoke(word, {\"callbacks\": callbacks})async for event in correct_tool.astream_events(\"hello\", version=\"v1\"):    print(event){\\'event\\': \\'on_tool_start\\', \\'run_id\\': \\'384f1710-612e-4022-a6d4-8a7bb0cc757e\\', \\'name\\': \\'correct_tool\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\'}}{\\'event\\': \\'on_chain_start\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'c4882303-8867-4dff-b031-7d9499b39dda\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'c4882303-8867-4dff-b031-7d9499b39dda\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\', \\'output\\': \\'olleh\\'}}{\\'event\\': \\'on_tool_stream\\', \\'run_id\\': \\'384f1710-612e-4022-a6d4-8a7bb0cc757e\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'correct_tool\\', \\'data\\': {\\'chunk\\': \\'olleh\\'}}{\\'event\\': \\'on_tool_end\\', \\'name\\': \\'correct_tool\\', \\'run_id\\': \\'384f1710-612e-4022-a6d4-8a7bb0cc757e\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'olleh\\'}}If you\\'re invoking runnables from within Runnable Lambdas or @chains, then callbacks will be passed automatically on your behalf.from langchain_core.runnables import RunnableLambdaasync def reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * 2reverse_and_double = RunnableLambda(reverse_and_double)await reverse_and_double.ainvoke(\"1234\")async for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):    print(event){\\'event\\': \\'on_chain_start\\', \\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'name\\': \\'reverse_and_double\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_start\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'335fe781-8944-4464-8d2e-81f61d1f85f5\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'335fe781-8944-4464-8d2e-81f61d1f85f5\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\', \\'output\\': \\'4321\\'}}{\\'event\\': \\'on_chain_stream\\', \\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'reverse_and_double\\', \\'data\\': {\\'chunk\\': \\'43214321\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_and_double\\', \\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'43214321\\'}}And with the @chain decorator:from langchain_core.runnables import chain@chainasync def reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * 2await reverse_and_double.ainvoke(\"1234\")async for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):    print(event){\\'event\\': \\'on_chain_start\\', \\'run_id\\': \\'7485eedb-1854-429c-a2f8-03d01452daef\\', \\'name\\': \\'reverse_and_double\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_start\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'e7cddab2-9b95-4e80-abaf-4b2429117835\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'e7cddab2-9b95-4e80-abaf-4b2429117835\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\', \\'output\\': \\'4321\\'}}{\\'event\\': \\'on_chain_stream\\', \\'run_id\\': \\'7485eedb-1854-429c-a2f8-03d01452daef\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'reverse_and_double\\', \\'data\\': {\\'chunk\\': \\'43214321\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_and_double\\', \\'run_id\\': \\'7485eedb-1854-429c-a2f8-03d01452daef\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'43214321\\'}}Help us out by providing feedback on this documentation page:PreviousAdvantages of LCELNextAdd message history (memory)Using StreamLLMs and Chat ModelsChainsWorking with Input StreamsNon-streaming componentsUsing Stream EventsEvent ReferenceChat ModelChainFiltering EventsNon-streaming componentsPropagating CallbacksCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/streaming/', 'content_type': 'text/html; charset=utf-8', 'title': 'Streaming | ü¶úÔ∏èüîó LangChain', 'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nRoute logic based on input | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageMoreRoute logic based on inputOn this pageDynamically route logic based on inputThis notebook covers how to do routing in the LangChain Expression Language.Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs.There are two ways to perform routing:Conditionally return runnables from a RunnableLambda (recommended)Using a RunnableBranch.We\\'ll illustrate both methods using a two step sequence where the first step classifies an input question as being about LangChain, Anthropic, or Other, then routes to a corresponding prompt chain.Example Setup\\u200bFirst, let\\'s create a chain that will identify incoming questions as being about LangChain, Anthropic, or Other:from langchain_anthropic import ChatAnthropicfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import PromptTemplatechain = (    PromptTemplate.from_template(        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.Do not respond with more than one word.<question>{question}</question>Classification:\"\"\"    )    | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")    | StrOutputParser())chain.invoke({\"question\": \"how do I call Anthropic?\"})\\'Anthropic\\'Now, let\\'s create three sub chains:langchain_chain = PromptTemplate.from_template(    \"\"\"You are an expert in langchain. \\\\Always answer questions starting with \"As Harrison Chase told me\". \\\\Respond to the following question:Question: {question}Answer:\"\"\") | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")anthropic_chain = PromptTemplate.from_template(    \"\"\"You are an expert in anthropic. \\\\Always answer questions starting with \"As Dario Amodei told me\". \\\\Respond to the following question:Question: {question}Answer:\"\"\") | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")general_chain = PromptTemplate.from_template(    \"\"\"Respond to the following question:Question: {question}Answer:\"\"\") | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")Using a custom function (Recommended)\\u200bYou can also use a custom function to route between different outputs. Here\\'s an example:def route(info):    if \"anthropic\" in info[\"topic\"].lower():        return anthropic_chain    elif \"langchain\" in info[\"topic\"].lower():        return langchain_chain    else:        return general_chainfrom langchain_core.runnables import RunnableLambdafull_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(    route)full_chain.invoke({\"question\": \"how do I use Anthropic?\"})AIMessage(content=\"As Dario Amodei told me, to use Anthropic, you can start by exploring the company\\'s website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \\\\n\\\\nDepending on your specific needs, you can look into Anthropic\\'s AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\\\\n\\\\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic\\'s approach to AI development.\\\\n\\\\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I\\'d encourage you to reach out to them directly to discuss how they can best support your specific requirements.\", response_metadata={\\'id\\': \\'msg_01CtLFgFSwvTaJomrihE87Ra\\', \\'content\\': [ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you can start by exploring the company\\'s website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \\\\n\\\\nDepending on your specific needs, you can look into Anthropic\\'s AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\\\\n\\\\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic\\'s approach to AI development.\\\\n\\\\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I\\'d encourage you to reach out to them directly to discuss how they can best support your specific requirements.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=53, output_tokens=219)})full_chain.invoke({\"question\": \"how do I use LangChain?\"})AIMessage(content=\"As Harrison Chase told me, using LangChain involves a few key steps:\\\\n\\\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\\\n\\\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\\\n\\\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\\\n\\\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\\\n\\\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI\\'s GPT-3 or Anthropic\\'s models. Connect your chosen language model to your LangChain application.\\\\n\\\\n6. **Implement your application logic**: Use LangChain\\'s building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\\\n\\\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\\\n\\\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", response_metadata={\\'id\\': \\'msg_01H3UXAAHG4TwxJLpxwuuVU7\\', \\'content\\': [ContentBlock(text=\"As Harrison Chase told me, using LangChain involves a few key steps:\\\\n\\\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\\\n\\\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\\\n\\\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\\\n\\\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\\\n\\\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI\\'s GPT-3 or Anthropic\\'s models. Connect your chosen language model to your LangChain application.\\\\n\\\\n6. **Implement your application logic**: Use LangChain\\'s building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\\\n\\\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\\\n\\\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=50, output_tokens=400)})full_chain.invoke({\"question\": \"whats 2 + 2\"})AIMessage(content=\\'4\\', response_metadata={\\'id\\': \\'msg_01UAKP81jTZu9fyiyFYhsbHc\\', \\'content\\': [ContentBlock(text=\\'4\\', type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=28, output_tokens=5)})Using a RunnableBranch\\u200bA RunnableBranch is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does not offer anything that you can\\'t achieve in a custom function as described above, so we recommend using a custom function instead.A RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable.Here\\'s an example of what it looks like in action:from langchain_core.runnables import RunnableBranchbranch = RunnableBranch(    (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),    general_chain,)full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branchfull_chain.invoke({\"question\": \"how do I use Anthropic?\"})AIMessage(content=\"As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \\\\n\\\\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic\\'s technology and services can support your specific needs.\\\\n\\\\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We\\'re here to collaborate and help you harness the power of advanced AI responsibly.\", response_metadata={\\'id\\': \\'msg_0187BVnpniPDJnVvwf3M1LdY\\', \\'content\\': [ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \\\\n\\\\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic\\'s technology and services can support your specific needs.\\\\n\\\\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We\\'re here to collaborate and help you harness the power of advanced AI responsibly.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=53, output_tokens=160)})full_chain.invoke({\"question\": \"how do I use LangChain?\"})AIMessage(content=\"As Harrison Chase told me, using LangChain involves several key steps. First, you\\'ll need to install the LangChain library and import the necessary modules. Then, you\\'ll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \\\\n\\\\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \\\\n\\\\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I\\'d recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I\\'m always happy to share more insights from my conversations with Harrison.\", response_metadata={\\'id\\': \\'msg_01T1naS99wGPkEAP4LME8iAv\\', \\'content\\': [ContentBlock(text=\"As Harrison Chase told me, using LangChain involves several key steps. First, you\\'ll need to install the LangChain library and import the necessary modules. Then, you\\'ll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \\\\n\\\\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \\\\n\\\\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I\\'d recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I\\'m always happy to share more insights from my conversations with Harrison.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=50, output_tokens=205)})full_chain.invoke({\"question\": \"whats 2 + 2\"})AIMessage(content=\\'4\\', response_metadata={\\'id\\': \\'msg_01T6T3TS6hRCtU8JayN93QEi\\', \\'content\\': [ContentBlock(text=\\'4\\', type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=28, output_tokens=5)})Routing by semantic similarityOne especially useful technique is to use embeddings to route a query to the most relevant prompt. Here\\'s an example.from langchain.utils.math import cosine_similarityfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_core.runnables import RunnableLambda, RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsphysics_template = \"\"\"You are a very smart physics professor. \\\\You are great at answering questions about physics in a concise and easy to understand manner. \\\\When you don\\'t know the answer to a question you admit that you don\\'t know.Here is a question:{query}\"\"\"math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\\You are so good because you are able to break down hard problems into their component parts, \\\\answer the component parts, and then put them together to answer the broader question.Here is a question:{query}\"\"\"embeddings = OpenAIEmbeddings()prompt_templates = [physics_template, math_template]prompt_embeddings = embeddings.embed_documents(prompt_templates)def prompt_router(input):    query_embedding = embeddings.embed_query(input[\"query\"])    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]    most_similar = prompt_templates[similarity.argmax()]    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")    return PromptTemplate.from_template(most_similar)chain = (    {\"query\": RunnablePassthrough()}    | RunnableLambda(prompt_router)    | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")    | StrOutputParser())print(chain.invoke(\"What\\'s a black hole\"))Using PHYSICSAs a physics professor, I would be happy to provide a concise and easy-to-understand explanation of what a black hole is.A black hole is an incredibly dense region of space-time where the gravitational pull is so strong that nothing, not even light, can escape from it. This means that if you were to get too close to a black hole, you would be pulled in and crushed by the intense gravitational forces.The formation of a black hole occurs when a massive star, much larger than our Sun, reaches the end of its life and collapses in on itself. This collapse causes the matter to become extremely dense, and the gravitational force becomes so strong that it creates a point of no return, known as the event horizon.Beyond the event horizon, the laws of physics as we know them break down, and the intense gravitational forces create a singularity, which is a point of infinite density and curvature in space-time.Black holes are fascinating and mysterious objects, and there is still much to be learned about their properties and behavior. If I were unsure about any specific details or aspects of black holes, I would readily admit that I do not have a complete understanding and would encourage further research and investigation.print(chain.invoke(\"What\\'s a path integral\"))Using MATHA path integral is a powerful mathematical concept in physics, particularly in the field of quantum mechanics. It was developed by the renowned physicist Richard Feynman as an alternative formulation of quantum mechanics.In a path integral, instead of considering a single, definite path that a particle might take from one point to another, as in classical mechanics, the particle is considered to take all possible paths simultaneously. Each path is assigned a complex-valued weight, and the total probability amplitude for the particle to go from one point to another is calculated by summing (integrating) over all possible paths.The key ideas behind the path integral formulation are:1. Superposition principle: In quantum mechanics, particles can exist in a superposition of multiple states or paths simultaneously.2. Probability amplitude: The probability amplitude for a particle to go from one point to another is calculated by summing the complex-valued weights of all possible paths.3. Weighting of paths: Each path is assigned a weight based on the action (the time integral of the Lagrangian) along that path. Paths with lower action have a greater weight.4. Feynman\\'s approach: Feynman developed the path integral formulation as an alternative to the traditional wave function approach in quantum mechanics, providing a more intuitive and conceptual understanding of quantum phenomena.The path integral approach is particularly useful in quantum field theory, where it provides a powerful framework for calculating transition probabilities and understanding the behavior of quantum systems. It has also found applications in various areas of physics, such as condensed matter, statistical mechanics, and even in finance (the path integral approach to option pricing).The mathematical construction of the path integral involves the use of advanced concepts from functional analysis and measure theory, making it a powerful and sophisticated tool in the physicist\\'s arsenal.Help us out by providing feedback on this documentation page:PreviousAdd message history (memory)NextInspect your runnablesExample SetupUsing a custom function (Recommended)Using a RunnableBranchCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/how_to/routing/', 'content_type': 'text/html; charset=utf-8', 'title': 'Route logic based on input | ü¶úÔ∏èüîó LangChain', 'description': 'This notebook covers how to do routing in the LangChain Expression Language.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nManaging prompt size | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageMoreManaging prompt sizeManaging prompt sizeAgents dynamically call tools. The results of those tool calls are added back to the prompt, so that the agent can plan the next action. Depending on what tools are being used and how they\\'re being called, the agent prompt can easily grow larger than the model context window.With LCEL, it\\'s easy to add custom functionality for managing the size of prompts within your chain or agent. Let\\'s look at simple agent example that can search Wikipedia for information.%pip install --upgrade --quiet  langchain langchain-openai wikipediafrom operator import itemgetterfrom langchain.agents import AgentExecutor, load_toolsfrom langchain.agents.format_scratchpad import format_to_openai_function_messagesfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParserfrom langchain_community.tools import WikipediaQueryRunfrom langchain_community.utilities import WikipediaAPIWrapperfrom langchain_core.prompt_values import ChatPromptValuefrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai import ChatOpenAIwiki = WikipediaQueryRun(    api_wrapper=WikipediaAPIWrapper(top_k_results=5, doc_content_chars_max=10_000))tools = [wiki]prompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant\"),        (\"user\", \"{input}\"),        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),    ])llm = ChatOpenAI(model=\"gpt-3.5-turbo\")Let\\'s try a many-step question without any prompt size handling:agent = (    {        \"input\": itemgetter(\"input\"),        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(            x[\"intermediate_steps\"]        ),    }    | prompt    | llm.bind_functions(tools)    | OpenAIFunctionsAgentOutputParser())agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)agent_executor.invoke(    {        \"input\": \"Who is the current US president? What\\'s their home state? What\\'s their home state\\'s bird? What\\'s that bird\\'s scientific name?\"    })\\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `List of presidents of the United States`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: List of presidents of the United StatesSummary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of individuals who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party.Page: List of presidents of the United States by ageSummary: In this list of presidents of the United States by age, the first table charts the age of each president of the United States at the time of presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated up to January 25, 2024.Page: List of vice presidents of the United StatesSummary: There have been 49 vice presidents of the United States since the office was created in 1789. Originally, the vice president was the person who received the second-most votes for president in the Electoral College. But after the election of 1800 produced a tie between Thomas Jefferson and Aaron Burr, requiring the House of Representatives to choose between them, lawmakers acted to prevent such a situation from recurring. The Twelfth Amendment was added to the Constitution in 1804, creating the current system where electors cast a separate ballot for the vice presidency.The vice president is the first person in the presidential line of succession‚Äîthat is, they assume the presidency if the president dies, resigns, or is impeached and removed from office. Nine vice presidents have ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, Harry S. Truman, and Lyndon B. Johnson) through the president\\'s death and one (Gerald Ford) through the president\\'s resignation. The vice president also serves as the president of the Senate and may choose to cast a tie-breaking vote on decisions made by the Senate. Vice presidents have exercised this latter power to varying extents over the years.Before adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the office of the vice president could not be filled until the next post-election inauguration. Several such vacancies occurred: seven vice presidents died, one resigned and eight succeeded to the presidency. This amendment allowed for a vacancy to be filled through appointment by the president and confirmation by both chambers of the Congress. Since its ratification, the vice presidency has been vacant twice (both in the context of scandals surrounding the Nixon administration) and was filled both times through this process, namely in 1973 following Spiro Agnew\\'s resignation, and again in 1974 after Gerald Ford succeeded to the presidency. The amendment also established a procedure whereby a vice president may, if the president is unable to discharge the powers and duties of the office, temporarily assume the powers and duties of the office as acting president. Three vice presidents have briefly acted as president under the 25th Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, and on July 21, 2007; and Kamala Harris on November 19, 2021.The persons who have served as vice president were born in or primarily affiliated with 27 states plus the District of Columbia. New York has produced the most of any state as eight have been born there and three others considered it their home state. Most vice presidents have been in their 50s or 60s and had political experience before assuming the office. Two vice presidents‚ÄîGeorge Clinton and John C. Calhoun‚Äîserved under more than one president. Ill with tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. King, by an Act of Congress, was allowed to take the oath outside the United States. He is the only vice president to take his oath of office in a foreign country.Page: List of presidents of the United States by net worthSummary: The list of presidents of the United States by net worth at peak varies greatly. Debt and depreciation often means that presidents\\' net worth is less than $0 at the time of death. Most presidents before 1845 were extremely wealthy, especially Andrew Jackson and George Washington.    Presidents since 1929, when Herbert Hoover took office, have generally been wealthier than presidents of the late nineteenth and early twentieth centuries; with the exception of Harry S. Truman, all presidents since this time have been millionaires. These presidents have often received income from autobiographies and other writing. Except for Franklin D. Roosevelt and John F. Kennedy (both of whom died while in office), all presidents beginning with Calvin Coolidge have written autobiographies. In addition, many presidents‚Äîincluding Bill Clinton‚Äîhave earned considerable income from public speaking after leaving office.The richest president in history may be Donald Trump. However, his net worth is not precisely known because the Trump Organization is privately held.Truman was among the poorest U.S. presidents, with a net worth considerably less than $1 million. His financial situation contributed to the doubling of the presidential salary to $100,000 in 1949. In addition, the presidential pension was created in 1958 when Truman was again experiencing financial difficulties. Harry and Bess Truman received the first Medicare cards in 1966 via the Social Security Act of 1965.Page: List of presidents of the United States by home stateSummary: These lists give the states of primary affiliation and of birth for each president of the United States.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Joe Biden`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Joe BidenSummary: Joseph Robinette Biden Jr. (  BY-d…ôn; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. He became the oldest president in U.S. history, and the first to have a female vice president.As president, Biden signed the American Rescue Plan Act in response to the COVID-19 pandemic and subsequent recession. He signed bipartisan bills on infrastructure and manufacturing. He proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that he signed into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme Court. He worked with congressional Republicans to resolve the 2023 United States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America\\'s membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the Israel‚ÄìHamas war, Biden announced military support for Israel, and condemned the actions of Hamas and other Palestinian militants as terrorism. In April 2023, Biden announced his candidacy for the Democratic nomination in the 2024 presidential election.Page: Presidency of Joe BidenSummary: Joe Biden\\'s tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president for two terms under president Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Biden won the presidency with a popular vote of over 81 million, the highest number of votes cast for a single United States presidential candidate. Upon his inauguration, he became the oldest president in American history, breaking the record set by his predecessor Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump\\'s energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump\\'s border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden alongside Democrats and Republicans in Congress, to invest in American roads, bridges, public transit, ports and broadband access. Biden signed the Juneteenth National Independence Day Act, making Juneteenth a federal holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. Supreme Court‚Äîthe first Black woman to serve on the court. After The Supreme Court overturned Roe v. Wade, Biden took executive actions, such as the signing of Executive Order 14076, to preserve and protect women\\'s health rights nationwide, against abortion bans in Republican led states. Biden proposed a significant expansion of the U.S. social safety net through the Build Back Better Act, but those efforts, along with voting rights legislation, failed in Congress. However, in August 2022, Biden signed the Inflation Reduction Act of 2022, a domestic appropriations bill that included some of the provisions of the Build Back Better Act after the entire bill failed to pass. It included significant federal investment in climate and domestic clean energy production, tax credits for solar panels, electric cars and other home energy programs as well as a three-year extension of Affordable Care Act subsidies. The administration\\'s economic policies, known as \"Bidenomics\", were inspired and designed by Trickle-up economics. Described as growing the economy from the middle out and bottom up and growing the middle class. Biden signed the CHIPS and Science Act, bolstering the semiconductor and manufacturing industry, the Honoring our PACT Act, expanding health care for US veterans, the Bipartisan Safer Communities Act and the Electoral Count Reform and Presidential Transition Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, which repealed the Defense of Marriage Act and codified same-sex and interracial marriage in the United States. In response to the debt-ceiling crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of 2023, which restrains federal spending for fiscal years 2024 and 2025, implements minor changes to SNAP and TANF, includes energy permitting reform, claws back some IRS funding and unspent money for COVID-19, and suspends the debt ceiling to January 1, 2025. Biden established the American Climate Corps and created the first ever White House Office of Gun Violence Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers picket line during the 2023 United Auto Workers strike, making him the first US president to visit one.The foreign policy goal of the Biden administration is to restore the US to a \"position of trusted leadership\" among global democracies in order to address the challenges posed by Russia and China. In foreign policy, Biden completed the withdrawal of U.S. military forces from Afghanistan, declaring an end to nation-building efforts and shifting U.S. foreign policy toward strategic competition with China and, to a lesser extent, Russia. However, during the withdrawal, the Afghan government collapsed and the Taliban seized control, leading to Biden receiving bipartisan criticism. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia as well as providing Ukraine with over $100 billion in combined military, economic, and humanitarian aid. Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi al-Qurashi, the leader of the Islamic State, and approved a drone strike which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed and created AUKUS, an international security alliance, together with Australia and the United Kingdom. Biden called for the expansion of NATO with the addition of Finland and Sweden, and rallied NATO allies in support of Ukraine. During the 2023 Israel‚ÄìHamas war, Biden condemned Hamas and other Palestinian militants as terrorism and announced American military support for Israel; Biden also showed his support and sympathy towards Palestinians affected by the war, sent humanitarian aid, and brokered a four-day temporary pause and hostage exchange.Page: Family of Joe BidenSummary: Joe Biden, the 46th and current president of the United States, has family members who are prominent in law, education, activism and politics. Biden\\'s immediate family became the first family of the United States on his inauguration on January 20, 2021. His immediate family circle was also the second family of the United States from 2009 to 2017, when Biden was vice president. Biden\\'s family is mostly descended from the British Isles, with most of their ancestors coming from Ireland and England, and a smaller number descending from the French.Of Joe Biden\\'s sixteen great-great-grandparents, ten were born in Ireland. He is descended from the Blewitts of County Mayo and the Finnegans of County Louth. One of Biden\\'s great-great-great-grandfathers was born in Sussex, England, and emigrated to Maryland in the United States by 1820.Page: Inauguration of Joe BidenSummary: The inauguration of Joe Biden as the 46th president of the United States took place on Wednesday, January 20, 2021, marking the start of the four-year term of Joe Biden as president and Kamala Harris as vice president. The 59th presidential inauguration took place on the West Front of the United States Capitol in Washington, D.C. Biden took the presidential oath of office, before which Harris took the vice presidential oath of office.The inauguration took place amidst extraordinary political, public health, economic, and national security crises, including the ongoing COVID-19 pandemic; outgoing President Donald Trump\\'s attempts to overturn the 2020 United States presidential election, which provoked an attack on the United States Capitol on January 6; Trump\\'\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Delaware`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: DelawareSummary: Delaware (  DEL-…ô-wair) is a state in the northeast and Mid-Atlantic regions of the United States. It borders Maryland to its south and west, Pennsylvania to its north, New Jersey to its northeast, and the Atlantic Ocean to its east. The state\\'s name derives from the adjacent Delaware Bay, which in turn was named after Thomas West, 3rd Baron De La Warr, an English nobleman and the Colony of Virginia\\'s first colonial-era governor.Delaware occupies the northeastern portion of the Delmarva Peninsula, and some islands and territory within the Delaware River. It is the 2nd smallest and 6th least populous state, but also the 6th most densely populated. Delaware\\'s most populous city is Wilmington, and the state\\'s capital is Dover, the 2nd most populous city in Delaware. The state is divided into three counties, the fewest number of counties of any of the 50 U.S. states; from north to south, the three counties are: New Castle County, Kent County, and Sussex County.The southern two counties, Kent and Sussex counties, historically have been predominantly agrarian economies. New Castle is more urbanized and is considered part of the Delaware Valley metropolitan statistical area that surrounds and includes Philadelphia, the nation\\'s 6th most populous city. Delaware is considered part of the Southern United States by the U.S. Census Bureau, but the state\\'s geography, culture, and history are a hybrid of the Mid-Atlantic and Northeastern regions of the country.Before Delaware coastline was explored and developed by Europeans in the 16th century, the state was inhabited by several Native Americans tribes, including the Lenape in the north and Nanticoke in the south. The state was first colonized by Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in 1631.Delaware was one of the Thirteen Colonies that participated in the American Revolution and American Revolutionary War, in which the American Continental Army, led by George Washington, defeated the British, ended British colonization and establishing the United States as a sovereign and independent nation.On December 7, 1787, Delaware was the first state to ratify the Constitution of the United States, earning it the nickname \"The First State\".Since the turn of the 20th century, Delaware has become an onshore corporate haven whose corporate laws are deemed appealing to corporations; over half of all New York Stock Exchange-listed corporations and over three-fifths of the Fortune 500 is legally incorporated in the state.Page: Delaware City, DelawareSummary: Delaware City is a city in New Castle County, Delaware, United States. The population was 1,885 as of 2020. It is a small port town on the eastern terminus of the Chesapeake and Delaware Canal and is the location of the Forts Ferry Crossing to Fort Delaware on Pea Patch Island.Page: Delaware RiverSummary: The Delaware River is a major river in the Mid-Atlantic region of the United States and is the longest free-flowing (undammed) river in the Eastern United States. From the meeting of its branches in Hancock, New York, the river flows for 282 miles (454 km) along the borders of New York, Pennsylvania, New Jersey, and Delaware, before emptying into Delaware Bay.The river has been recognized by the National Wildlife Federation as one of the country\\'s Great Waters and has been called the \"Lifeblood of the Northeast\" by American Rivers. Its watershed drains an area of 13,539 square miles (35,070 km2) and provides drinking water for 17 million people, including half of New York City via the Delaware Aqueduct.The Delaware River has two branches that rise in the Catskill Mountains of New York: the West Branch at Mount Jefferson in Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware County. The branches merge to form the main Delaware River at Hancock, New York. Flowing south, the river remains relatively undeveloped, with 152 miles (245 km) protected as the Upper, Middle, and Lower Delaware National Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, navigable, and significantly more industrial. This section forms the backbone of the Delaware Valley metropolitan area, serving the port cities of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the bay\\'s outlet to the Atlantic Ocean between Cape May and Cape Henlopen.Before the arrival of European settlers, the river was the homeland of the Lenape native people. They called the river Lenapewihittuk, or Lenape River, and Kithanne, meaning the largest river in this part of the country.In 1609, the river was visited by a Dutch East India Company expedition led by Henry Hudson. Hudson, an English navigator, was hired to find a western route to Cathay (China), but his encounters set the stage for Dutch colonization of North America in the 17th century. Early Dutch and Swedish settlements were established along the lower section of the river and Delaware Bay. Both colonial powers called the river the South River (Zuidrivier), compared to the Hudson River, which was known as the North River. After the English expelled the Dutch and took control of the New Netherland colony in 1664, the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, an English nobleman and the Virginia colony\\'s first royal governor, who defended the colony during the First Anglo-Powhatan War.Page: University of DelawareSummary: The University of Delaware (colloquially known as UD or Delaware) is a privately governed, state-assisted land-grant research university located in Newark, Delaware. UD is the largest university in Delaware. It offers three associate\\'s programs, 148 bachelor\\'s programs, 121 master\\'s programs (with 13 joint degrees), and 55 doctoral programs across its eight colleges. The main campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, and Georgetown. It is considered a large institution with approximately 18,200 undergraduate and 4,200 graduate students. It is a privately governed university which receives public funding for being a land-grant, sea-grant, and space-grant state-supported research institution.UD is classified among \"R1: Doctoral Universities ‚Äì Very high research activity\". According to the National Science Foundation, UD spent $186 million on research and development in 2018, ranking it 119th in the nation.  It is recognized with the Community Engagement Classification by the Carnegie Foundation for the Advancement of Teaching.UD students, alumni, and sports teams are known as the \"Fightin\\' Blue Hens\", more commonly shortened to \"Blue Hens\", and the school colors are Delaware blue and gold. UD sponsors 21 men\\'s and women\\'s NCAA Division-I sports teams and have competed in the Colonial Athletic Association (CAA) since 2001.Page: LenapeSummary: The Lenape (English: , , ; Lenape languages: [l…ônaÀêpe]), also called the Lenni Lenape and Delaware people, are an Indigenous people of the Northeastern Woodlands, who live in the United States and Canada.The Lenape\\'s historical territory includes present-day northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, western Long Island, and the lower Hudson Valley in New York state. Today they are based in Oklahoma, Wisconsin, and Ontario.During the last decades of the 18th century, European settlers and the effects of the American Revolutionary War displaced most Lenape from their homelands and pushed them north and west. In the 1860s, under the Indian removal policy, the U.S. federal government relocated most Lenape remaining in the Eastern United States to the Indian Territory and surrounding regions. Lenape people currently belong to the Delaware Nation and Delaware Tribe of Indians in Oklahoma, the Stockbridge‚ÄìMunsee Community in Wisconsin, and the Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of Six Nations in Ontario.\\x1b[0m---------------------------------------------------------------------------``````outputBadRequestError                           Traceback (most recent call last)``````outputCell In[11], line 14      1 agent = (      2     {      3         \"input\": itemgetter(\"input\"),   (...)     10     | OpenAIFunctionsAgentOutputParser()     11 )     13 agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)---> 14 agent_executor.invoke(     15     {     16         \"input\": \"Who is the current US president? What\\'s their home state? What\\'s their home state\\'s bird? What\\'s that bird\\'s scientific name?\"     17     }     18 )``````outputFile ~/langchain/libs/langchain/langchain/chains/base.py:162, in Chain.invoke(self, input, config, **kwargs)    160 except BaseException as e:    161     run_manager.on_chain_error(e)--> 162     raise e    163 run_manager.on_chain_end(outputs)    164 final_outputs: Dict[str, Any] = self.prep_outputs(    165     inputs, outputs, return_only_outputs    166 )``````outputFile ~/langchain/libs/langchain/langchain/chains/base.py:156, in Chain.invoke(self, input, config, **kwargs)    149 run_manager = callback_manager.on_chain_start(    150     dumpd(self),    151     inputs,    152     name=run_name,    153 )    154 try:    155     outputs = (--> 156         self._call(inputs, run_manager=run_manager)    157         if new_arg_supported    158         else self._call(inputs)    159     )    160 except BaseException as e:    161     run_manager.on_chain_error(e)``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1391, in AgentExecutor._call(self, inputs, run_manager)   1389 # We now enter the agent loop (until it returns something).   1390 while self._should_continue(iterations, time_elapsed):-> 1391     next_step_output = self._take_next_step(   1392         name_to_tool_map,   1393         color_mapping,   1394         inputs,   1395         intermediate_steps,   1396         run_manager=run_manager,   1397     )   1398     if isinstance(next_step_output, AgentFinish):   1399         return self._return(   1400             next_step_output, intermediate_steps, run_manager=run_manager   1401         )``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1097, in AgentExecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)   1088 def _take_next_step(   1089     self,   1090     name_to_tool_map: Dict[str, BaseTool],   (...)   1094     run_manager: Optional[CallbackManagerForChainRun] = None,   1095 ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:   1096     return self._consume_next_step(-> 1097         [   1098             a   1099             for a in self._iter_next_step(   1100                 name_to_tool_map,   1101                 color_mapping,   1102                 inputs,   1103                 intermediate_steps,   1104                 run_manager,   1105             )   1106         ]   1107     )``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1097, in <listcomp>(.0)   1088 def _take_next_step(   1089     self,   1090     name_to_tool_map: Dict[str, BaseTool],   (...)   1094     run_manager: Optional[CallbackManagerForChainRun] = None,   1095 ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:   1096     return self._consume_next_step(-> 1097         [   1098             a   1099             for a in self._iter_next_step(   1100                 name_to_tool_map,   1101                 color_mapping,   1102                 inputs,   1103                 intermediate_steps,   1104                 run_manager,   1105             )   1106         ]   1107     )``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1125, in AgentExecutor._iter_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)   1122     intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)   1124     # Call the LLM to see what to do.-> 1125     output = self.agent.plan(   1126         intermediate_steps,   1127         callbacks=run_manager.get_child() if run_manager else None,   1128         **inputs,   1129     )   1130 except OutputParserException as e:   1131     if isinstance(self.handle_parsing_errors, bool):``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:387, in RunnableAgent.plan(self, intermediate_steps, callbacks, **kwargs)    381 # Use streaming to make sure that the underlying LLM is invoked in a streaming    382 # fashion to make it possible to get access to the individual LLM tokens    383 # when using stream_log with the Agent Executor.    384 # Because the response from the plan is not a generator, we need to    385 # accumulate the output into final output and return that.    386 final_output: Any = None--> 387 for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):    388     if final_output is None:    389         final_output = chunk``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:2424, in RunnableSequence.stream(self, input, config, **kwargs)   2418 def stream(   2419     self,   2420     input: Input,   2421     config: Optional[RunnableConfig] = None,   2422     **kwargs: Optional[Any],   2423 ) -> Iterator[Output]:-> 2424     yield from self.transform(iter([input]), config, **kwargs)``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:2411, in RunnableSequence.transform(self, input, config, **kwargs)   2405 def transform(   2406     self,   2407     input: Iterator[Input],   2408     config: Optional[RunnableConfig] = None,   2409     **kwargs: Optional[Any],   2410 ) -> Iterator[Output]:-> 2411     yield from self._transform_stream_with_config(   2412         input,   2413         self._transform,   2414         patch_config(config, run_name=(config or {}).get(\"run_name\") or self.name),   2415         **kwargs,   2416     )``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:1497, in Runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)   1495 try:   1496     while True:-> 1497         chunk: Output = context.run(next, iterator)  # type: ignore   1498         yield chunk   1499         if final_output_supported:``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:2375, in RunnableSequence._transform(self, input, run_manager, config)   2366 for step in steps:   2367     final_pipeline = step.transform(   2368         final_pipeline,   2369         patch_config(   (...)   2372         ),   2373     )-> 2375 for output in final_pipeline:   2376     yield output``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:1035, in Runnable.transform(self, input, config, **kwargs)   1032 final: Input   1033 got_first_val = False-> 1035 for chunk in input:   1036     if not got_first_val:   1037         final = chunk``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:3991, in RunnableBindingBase.transform(self, input, config, **kwargs)   3985 def transform(   3986     self,   3987     input: Iterator[Input],   3988     config: Optional[RunnableConfig] = None,   3989     **kwargs: Any,   3990 ) -> Iterator[Output]:-> 3991     yield from self.bound.transform(   3992         input,   3993         self._merge_configs(config),   3994         **{**self.kwargs, **kwargs},   3995     )``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:1045, in Runnable.transform(self, input, config, **kwargs)   1042         final = final + chunk  # type: ignore[operator]   1044 if got_first_val:-> 1045     yield from self.stream(final, config, **kwargs)``````outputFile ~/langchain/libs/core/langchain_core/language_models/chat_models.py:249, in BaseChatModel.stream(self, input, config, stop, **kwargs)    242 except BaseException as e:    243     run_manager.on_llm_error(    244         e,    245         response=LLMResult(    246             generations=[[generation]] if generation else []    247         ),    248     )--> 249     raise e    250 else:    251     run_manager.on_llm_end(LLMResult(generations=[[generation]]))``````outputFile ~/langchain/libs/core/langchain_core/language_models/chat_models.py:233, in BaseChatModel.stream(self, input, config, stop, **kwargs)    231 generation: Optional[ChatGenerationChunk] = None    232 try:--> 233     for chunk in self._stream(    234         messages, stop=stop, run_manager=run_manager, **kwargs    235     ):    236         yield chunk.message    237         if generation is None:``````outputFile ~/langchain/libs/partners/openai/langchain_openai/chat_models/base.py:403, in ChatOpenAI._stream(self, messages, stop, run_manager, **kwargs)    400 params = {**params, **kwargs, \"stream\": True}    402 default_chunk_class = AIMessageChunk--> 403 for chunk in self.client.create(messages=message_dicts, **params):    404     if not isinstance(chunk, dict):    405         chunk = chunk.dict()``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/_utils/_utils.py:271, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)    269             msg = f\"Missing required argument: {quote(missing[0])}\"    270     raise TypeError(msg)--> 271 return func(*args, **kwargs)``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/resources/chat/completions.py:648, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)    599 @required_args([\"messages\", \"model\"], [\"messages\", \"model\", \"stream\"])    600 def create(    601     self,   (...)    646     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,    647 ) -> ChatCompletion | Stream[ChatCompletionChunk]:--> 648     return self._post(    649         \"/chat/completions\",    650         body=maybe_transform(    651             {    652                 \"messages\": messages,    653                 \"model\": model,    654                 \"frequency_penalty\": frequency_penalty,    655                 \"function_call\": function_call,    656                 \"functions\": functions,    657                 \"logit_bias\": logit_bias,    658                 \"logprobs\": logprobs,    659                 \"max_tokens\": max_tokens,    660                 \"n\": n,    661                 \"presence_penalty\": presence_penalty,    662                 \"response_format\": response_format,    663                 \"seed\": seed,    664                 \"stop\": stop,    665                 \"stream\": stream,    666                 \"temperature\": temperature,    667                 \"tool_choice\": tool_choice,    668                 \"tools\": tools,    669                 \"top_logprobs\": top_logprobs,    670                 \"top_p\": top_p,    671                 \"user\": user,    672             },    673             completion_create_params.CompletionCreateParams,    674         ),    675         options=make_request_options(    676             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout    677         ),    678         cast_to=ChatCompletion,    679         stream=stream or False,    680         stream_cls=Stream[ChatCompletionChunk],    681     )``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:1179, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)   1165 def post(   1166     self,   1167     path: str,   (...)   1174     stream_cls: type[_StreamT] | None = None,   1175 ) -> ResponseT | _StreamT:   1176     opts = FinalRequestOptions.construct(   1177         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options   1178     )-> 1179     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:868, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)    859 def request(    860     self,    861     cast_to: Type[ResponseT],   (...)    866     stream_cls: type[_StreamT] | None = None,    867 ) -> ResponseT | _StreamT:--> 868     return self._request(    869         cast_to=cast_to,    870         options=options,    871         stream=stream,    872         stream_cls=stream_cls,    873         remaining_retries=remaining_retries,    874     )``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:959, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)    956         err.response.read()    958     log.debug(\"Re-raising status error\")--> 959     raise self._make_status_error_from_response(err.response) from None    961 return self._process_response(    962     cast_to=cast_to,    963     options=options,   (...)    966     stream_cls=stream_cls,    967 )``````outputBadRequestError: Error code: 400 - {\\'error\\': {\\'message\\': \"This model\\'s maximum context length is 4097 tokens. However, your messages resulted in 5487 tokens (5419 in the messages, 68 in the functions). Please reduce the length of the messages or functions.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'messages\\', \\'code\\': \\'context_length_exceeded\\'}}tipLangSmith traceUnfortunately we run out of space in our model\\'s context window before we the agent can get to the final answer. Now let\\'s add some prompt handling logic. To keep things simple, if our messages have too many tokens we\\'ll start dropping the earliest AI, Function message pairs (this is the model tool invocation message and the subsequent tool output message) in the chat history.def condense_prompt(prompt: ChatPromptValue) -> ChatPromptValue:    messages = prompt.to_messages()    num_tokens = llm.get_num_tokens_from_messages(messages)    ai_function_messages = messages[2:]    while num_tokens > 4_000:        ai_function_messages = ai_function_messages[2:]        num_tokens = llm.get_num_tokens_from_messages(            messages[:2] + ai_function_messages        )    messages = messages[:2] + ai_function_messages    return ChatPromptValue(messages=messages)agent = (    {        \"input\": itemgetter(\"input\"),        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(            x[\"intermediate_steps\"]        ),    }    | prompt    | condense_prompt    | llm.bind_functions(tools)    | OpenAIFunctionsAgentOutputParser())agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)agent_executor.invoke(    {        \"input\": \"Who is the current US president? What\\'s their home state? What\\'s their home state\\'s bird? What\\'s that bird\\'s scientific name?\"    })\\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `List of presidents of the United States`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: List of presidents of the United StatesSummary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of individuals who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party.Page: List of presidents of the United States by ageSummary: In this list of presidents of the United States by age, the first table charts the age of each president of the United States at the time of presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated up to January 25, 2024.Page: List of vice presidents of the United StatesSummary: There have been 49 vice presidents of the United States since the office was created in 1789. Originally, the vice president was the person who received the second-most votes for president in the Electoral College. But after the election of 1800 produced a tie between Thomas Jefferson and Aaron Burr, requiring the House of Representatives to choose between them, lawmakers acted to prevent such a situation from recurring. The Twelfth Amendment was added to the Constitution in 1804, creating the current system where electors cast a separate ballot for the vice presidency.The vice president is the first person in the presidential line of succession‚Äîthat is, they assume the presidency if the president dies, resigns, or is impeached and removed from office. Nine vice presidents have ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, Harry S. Truman, and Lyndon B. Johnson) through the president\\'s death and one (Gerald Ford) through the president\\'s resignation. The vice president also serves as the president of the Senate and may choose to cast a tie-breaking vote on decisions made by the Senate. Vice presidents have exercised this latter power to varying extents over the years.Before adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the office of the vice president could not be filled until the next post-election inauguration. Several such vacancies occurred: seven vice presidents died, one resigned and eight succeeded to the presidency. This amendment allowed for a vacancy to be filled through appointment by the president and confirmation by both chambers of the Congress. Since its ratification, the vice presidency has been vacant twice (both in the context of scandals surrounding the Nixon administration) and was filled both times through this process, namely in 1973 following Spiro Agnew\\'s resignation, and again in 1974 after Gerald Ford succeeded to the presidency. The amendment also established a procedure whereby a vice president may, if the president is unable to discharge the powers and duties of the office, temporarily assume the powers and duties of the office as acting president. Three vice presidents have briefly acted as president under the 25th Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, and on July 21, 2007; and Kamala Harris on November 19, 2021.The persons who have served as vice president were born in or primarily affiliated with 27 states plus the District of Columbia. New York has produced the most of any state as eight have been born there and three others considered it their home state. Most vice presidents have been in their 50s or 60s and had political experience before assuming the office. Two vice presidents‚ÄîGeorge Clinton and John C. Calhoun‚Äîserved under more than one president. Ill with tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. King, by an Act of Congress, was allowed to take the oath outside the United States. He is the only vice president to take his oath of office in a foreign country.Page: List of presidents of the United States by net worthSummary: The list of presidents of the United States by net worth at peak varies greatly. Debt and depreciation often means that presidents\\' net worth is less than $0 at the time of death. Most presidents before 1845 were extremely wealthy, especially Andrew Jackson and George Washington.    Presidents since 1929, when Herbert Hoover took office, have generally been wealthier than presidents of the late nineteenth and early twentieth centuries; with the exception of Harry S. Truman, all presidents since this time have been millionaires. These presidents have often received income from autobiographies and other writing. Except for Franklin D. Roosevelt and John F. Kennedy (both of whom died while in office), all presidents beginning with Calvin Coolidge have written autobiographies. In addition, many presidents‚Äîincluding Bill Clinton‚Äîhave earned considerable income from public speaking after leaving office.The richest president in history may be Donald Trump. However, his net worth is not precisely known because the Trump Organization is privately held.Truman was among the poorest U.S. presidents, with a net worth considerably less than $1 million. His financial situation contributed to the doubling of the presidential salary to $100,000 in 1949. In addition, the presidential pension was created in 1958 when Truman was again experiencing financial difficulties. Harry and Bess Truman received the first Medicare cards in 1966 via the Social Security Act of 1965.Page: List of presidents of the United States by home stateSummary: These lists give the states of primary affiliation and of birth for each president of the United States.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Joe Biden`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Joe BidenSummary: Joseph Robinette Biden Jr. (  BY-d…ôn; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. He became the oldest president in U.S. history, and the first to have a female vice president.As president, Biden signed the American Rescue Plan Act in response to the COVID-19 pandemic and subsequent recession. He signed bipartisan bills on infrastructure and manufacturing. He proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that he signed into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme Court. He worked with congressional Republicans to resolve the 2023 United States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America\\'s membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the Israel‚ÄìHamas war, Biden announced military support for Israel, and condemned the actions of Hamas and other Palestinian militants as terrorism. In April 2023, Biden announced his candidacy for the Democratic nomination in the 2024 presidential election.Page: Presidency of Joe BidenSummary: Joe Biden\\'s tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president for two terms under president Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Biden won the presidency with a popular vote of over 81 million, the highest number of votes cast for a single United States presidential candidate. Upon his inauguration, he became the oldest president in American history, breaking the record set by his predecessor Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump\\'s energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump\\'s border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden alongside Democrats and Republicans in Congress, to invest in American roads, bridges, public transit, ports and broadband access. Biden signed the Juneteenth National Independence Day Act, making Juneteenth a federal holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. Supreme Court‚Äîthe first Black woman to serve on the court. After The Supreme Court overturned Roe v. Wade, Biden took executive actions, such as the signing of Executive Order 14076, to preserve and protect women\\'s health rights nationwide, against abortion bans in Republican led states. Biden proposed a significant expansion of the U.S. social safety net through the Build Back Better Act, but those efforts, along with voting rights legislation, failed in Congress. However, in August 2022, Biden signed the Inflation Reduction Act of 2022, a domestic appropriations bill that included some of the provisions of the Build Back Better Act after the entire bill failed to pass. It included significant federal investment in climate and domestic clean energy production, tax credits for solar panels, electric cars and other home energy programs as well as a three-year extension of Affordable Care Act subsidies. The administration\\'s economic policies, known as \"Bidenomics\", were inspired and designed by Trickle-up economics. Described as growing the economy from the middle out and bottom up and growing the middle class. Biden signed the CHIPS and Science Act, bolstering the semiconductor and manufacturing industry, the Honoring our PACT Act, expanding health care for US veterans, the Bipartisan Safer Communities Act and the Electoral Count Reform and Presidential Transition Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, which repealed the Defense of Marriage Act and codified same-sex and interracial marriage in the United States. In response to the debt-ceiling crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of 2023, which restrains federal spending for fiscal years 2024 and 2025, implements minor changes to SNAP and TANF, includes energy permitting reform, claws back some IRS funding and unspent money for COVID-19, and suspends the debt ceiling to January 1, 2025. Biden established the American Climate Corps and created the first ever White House Office of Gun Violence Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers picket line during the 2023 United Auto Workers strike, making him the first US president to visit one.The foreign policy goal of the Biden administration is to restore the US to a \"position of trusted leadership\" among global democracies in order to address the challenges posed by Russia and China. In foreign policy, Biden completed the withdrawal of U.S. military forces from Afghanistan, declaring an end to nation-building efforts and shifting U.S. foreign policy toward strategic competition with China and, to a lesser extent, Russia. However, during the withdrawal, the Afghan government collapsed and the Taliban seized control, leading to Biden receiving bipartisan criticism. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia as well as providing Ukraine with over $100 billion in combined military, economic, and humanitarian aid. Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi al-Qurashi, the leader of the Islamic State, and approved a drone strike which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed and created AUKUS, an international security alliance, together with Australia and the United Kingdom. Biden called for the expansion of NATO with the addition of Finland and Sweden, and rallied NATO allies in support of Ukraine. During the 2023 Israel‚ÄìHamas war, Biden condemned Hamas and other Palestinian militants as terrorism and announced American military support for Israel; Biden also showed his support and sympathy towards Palestinians affected by the war, sent humanitarian aid, and brokered a four-day temporary pause and hostage exchange.Page: Family of Joe BidenSummary: Joe Biden, the 46th and current president of the United States, has family members who are prominent in law, education, activism and politics. Biden\\'s immediate family became the first family of the United States on his inauguration on January 20, 2021. His immediate family circle was also the second family of the United States from 2009 to 2017, when Biden was vice president. Biden\\'s family is mostly descended from the British Isles, with most of their ancestors coming from Ireland and England, and a smaller number descending from the French.Of Joe Biden\\'s sixteen great-great-grandparents, ten were born in Ireland. He is descended from the Blewitts of County Mayo and the Finnegans of County Louth. One of Biden\\'s great-great-great-grandfathers was born in Sussex, England, and emigrated to Maryland in the United States by 1820.Page: Inauguration of Joe BidenSummary: The inauguration of Joe Biden as the 46th president of the United States took place on Wednesday, January 20, 2021, marking the start of the four-year term of Joe Biden as president and Kamala Harris as vice president. The 59th presidential inauguration took place on the West Front of the United States Capitol in Washington, D.C. Biden took the presidential oath of office, before which Harris took the vice presidential oath of office.The inauguration took place amidst extraordinary political, public health, economic, and national security crises, including the ongoing COVID-19 pandemic; outgoing President Donald Trump\\'s attempts to overturn the 2020 United States presidential election, which provoked an attack on the United States Capitol on January 6; Trump\\'\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Delaware`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: DelawareSummary: Delaware (  DEL-…ô-wair) is a state in the northeast and Mid-Atlantic regions of the United States. It borders Maryland to its south and west, Pennsylvania to its north, New Jersey to its northeast, and the Atlantic Ocean to its east. The state\\'s name derives from the adjacent Delaware Bay, which in turn was named after Thomas West, 3rd Baron De La Warr, an English nobleman and the Colony of Virginia\\'s first colonial-era governor.Delaware occupies the northeastern portion of the Delmarva Peninsula, and some islands and territory within the Delaware River. It is the 2nd smallest and 6th least populous state, but also the 6th most densely populated. Delaware\\'s most populous city is Wilmington, and the state\\'s capital is Dover, the 2nd most populous city in Delaware. The state is divided into three counties, the fewest number of counties of any of the 50 U.S. states; from north to south, the three counties are: New Castle County, Kent County, and Sussex County.The southern two counties, Kent and Sussex counties, historically have been predominantly agrarian economies. New Castle is more urbanized and is considered part of the Delaware Valley metropolitan statistical area that surrounds and includes Philadelphia, the nation\\'s 6th most populous city. Delaware is considered part of the Southern United States by the U.S. Census Bureau, but the state\\'s geography, culture, and history are a hybrid of the Mid-Atlantic and Northeastern regions of the country.Before Delaware coastline was explored and developed by Europeans in the 16th century, the state was inhabited by several Native Americans tribes, including the Lenape in the north and Nanticoke in the south. The state was first colonized by Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in 1631.Delaware was one of the Thirteen Colonies that participated in the American Revolution and American Revolutionary War, in which the American Continental Army, led by George Washington, defeated the British, ended British colonization and establishing the United States as a sovereign and independent nation.On December 7, 1787, Delaware was the first state to ratify the Constitution of the United States, earning it the nickname \"The First State\".Since the turn of the 20th century, Delaware has become an onshore corporate haven whose corporate laws are deemed appealing to corporations; over half of all New York Stock Exchange-listed corporations and over three-fifths of the Fortune 500 is legally incorporated in the state.Page: Delaware City, DelawareSummary: Delaware City is a city in New Castle County, Delaware, United States. The population was 1,885 as of 2020. It is a small port town on the eastern terminus of the Chesapeake and Delaware Canal and is the location of the Forts Ferry Crossing to Fort Delaware on Pea Patch Island.Page: Delaware RiverSummary: The Delaware River is a major river in the Mid-Atlantic region of the United States and is the longest free-flowing (undammed) river in the Eastern United States. From the meeting of its branches in Hancock, New York, the river flows for 282 miles (454 km) along the borders of New York, Pennsylvania, New Jersey, and Delaware, before emptying into Delaware Bay.The river has been recognized by the National Wildlife Federation as one of the country\\'s Great Waters and has been called the \"Lifeblood of the Northeast\" by American Rivers. Its watershed drains an area of 13,539 square miles (35,070 km2) and provides drinking water for 17 million people, including half of New York City via the Delaware Aqueduct.The Delaware River has two branches that rise in the Catskill Mountains of New York: the West Branch at Mount Jefferson in Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware County. The branches merge to form the main Delaware River at Hancock, New York. Flowing south, the river remains relatively undeveloped, with 152 miles (245 km) protected as the Upper, Middle, and Lower Delaware National Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, navigable, and significantly more industrial. This section forms the backbone of the Delaware Valley metropolitan area, serving the port cities of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the bay\\'s outlet to the Atlantic Ocean between Cape May and Cape Henlopen.Before the arrival of European settlers, the river was the homeland of the Lenape native people. They called the river Lenapewihittuk, or Lenape River, and Kithanne, meaning the largest river in this part of the country.In 1609, the river was visited by a Dutch East India Company expedition led by Henry Hudson. Hudson, an English navigator, was hired to find a western route to Cathay (China), but his encounters set the stage for Dutch colonization of North America in the 17th century. Early Dutch and Swedish settlements were established along the lower section of the river and Delaware Bay. Both colonial powers called the river the South River (Zuidrivier), compared to the Hudson River, which was known as the North River. After the English expelled the Dutch and took control of the New Netherland colony in 1664, the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, an English nobleman and the Virginia colony\\'s first royal governor, who defended the colony during the First Anglo-Powhatan War.Page: University of DelawareSummary: The University of Delaware (colloquially known as UD or Delaware) is a privately governed, state-assisted land-grant research university located in Newark, Delaware. UD is the largest university in Delaware. It offers three associate\\'s programs, 148 bachelor\\'s programs, 121 master\\'s programs (with 13 joint degrees), and 55 doctoral programs across its eight colleges. The main campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, and Georgetown. It is considered a large institution with approximately 18,200 undergraduate and 4,200 graduate students. It is a privately governed university which receives public funding for being a land-grant, sea-grant, and space-grant state-supported research institution.UD is classified among \"R1: Doctoral Universities ‚Äì Very high research activity\". According to the National Science Foundation, UD spent $186 million on research and development in 2018, ranking it 119th in the nation.  It is recognized with the Community Engagement Classification by the Carnegie Foundation for the Advancement of Teaching.UD students, alumni, and sports teams are known as the \"Fightin\\' Blue Hens\", more commonly shortened to \"Blue Hens\", and the school colors are Delaware blue and gold. UD sponsors 21 men\\'s and women\\'s NCAA Division-I sports teams and have competed in the Colonial Athletic Association (CAA) since 2001.Page: LenapeSummary: The Lenape (English: , , ; Lenape languages: [l…ônaÀêpe]), also called the Lenni Lenape and Delaware people, are an Indigenous people of the Northeastern Woodlands, who live in the United States and Canada.The Lenape\\'s historical territory includes present-day northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, western Long Island, and the lower Hudson Valley in New York state. Today they are based in Oklahoma, Wisconsin, and Ontario.During the last decades of the 18th century, European settlers and the effects of the American Revolutionary War displaced most Lenape from their homelands and pushed them north and west. In the 1860s, under the Indian removal policy, the U.S. federal government relocated most Lenape remaining in the Eastern United States to the Indian Territory and surrounding regions. Lenape people currently belong to the Delaware Nation and Delaware Tribe of Indians in Oklahoma, the Stockbridge‚ÄìMunsee Community in Wisconsin, and the Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of Six Nations in Ontario.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Blue hen chicken`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Delaware Blue HenSummary: The Delaware Blue Hen or Blue Hen of Delaware is a blue strain of American gamecock. Under the name Blue Hen Chicken it is the official bird of the State of Delaware. It is the emblem or mascot of several institutions in the state, among them the sports teams of the University of Delaware.Page: Delaware Fightin\\' Blue HensSummary: The Delaware Fightin\\' Blue Hens are the athletic teams of the University of Delaware (UD) of Newark, Delaware, in the United States. The Blue Hens compete in the Football Championship Subdivision (FCS) of Division I of the National Collegiate Athletic Association (NCAA) as members of the Coastal Athletic Association and its technically separate football league, CAA Football.On November 28, 2023, UD and Conference USA (CUSA) jointly announced that UD would start a transition to the Division I Football Bowl Subdivision (FBS) in 2024 and join CUSA in 2025. UD will continue to compete in both sides of the CAA in 2024‚Äì25; it will be ineligible for the FCS playoffs due to NCAA rules for transitioning programs, but will be eligible for all non-football CAA championships. Upon joining CUSA, UD will be eligible for all conference championship events except the football championship game; it will become eligible for that event upon completing the FBS transition in 2026. At the same time, UD also announced it would add one women\\'s sport due to Title IX considerations, and would also be seeking conference homes for the seven sports that UD sponsors but CUSA does not. The new women\\'s sport would later be announced as ice hockey; UD will join College Hockey America for its first season of varsity play in 2025‚Äì26.Page: Brahma chickenSummary: The Brahma is an American breed of chicken. It was bred in the United States from birds imported from the Chinese port of Shanghai,:\\u200a78\\u200a and was the principal American meat breed from the 1850s until about 1930.Page: SilkieSummary: The Silkie (also known as the Silky or Chinese silk chicken) is a breed of chicken named for its atypically fluffy plumage, which is said to feel like silk and satin. The breed has several other unusual qualities, such as black skin and bones, blue earlobes, and five toes on each foot, whereas most chickens have only four. They are often exhibited in poultry shows, and also appear in various colors. In addition to their distinctive physical characteristics, Silkies are well known for their calm and friendly temperament. It is among the most docile of poultry. Hens are also exceptionally broody, and care for young well. Although they are fair layers themselves, laying only about three eggs a week, they are commonly used to hatch eggs from other breeds and bird species due to their broody nature. Silkie chickens have been bred to have a wide variety of colors which include but are not limited to: Black, Blue, Buff, Partridge, Splash, White, Lavender, Paint and Porcelain.Page: Silverudd BlueSummary: The Silverudd Blue, Swedish: Silverudds Bl√•, is a Swedish breed of chicken. It was developed by Martin Silverudd in Sm√•land, in southern Sweden. Hens lay blue/green eggs, weighing 50‚Äì65 grams. The flock-book for the breed is kept by the Svenska Kulturh√∂nsf√∂reningen ‚Äì the Swedish Cultural Hen Association. It was initially known by various names including Isbar, Blue Isbar and Svensk Gr√∂nv√§rpare, or \"Swedish green egg layer\"; in 2016 it was renamed to \\'Silverudd Blue\\' after its creator.\\x1b[0m\\x1b[32;1m\\x1b[1;3mThe current US president is Joe Biden. His home state is Delaware. The home state bird of Delaware is the Delaware Blue Hen. The scientific name of the Delaware Blue Hen is Gallus gallus domesticus.\\x1b[0m\\x1b[1m> Finished chain.\\x1b[0m{\\'input\\': \"Who is the current US president? What\\'s their home state? What\\'s their home state\\'s bird? What\\'s that bird\\'s scientific name?\", \\'output\\': \\'The current US president is Joe Biden. His home state is Delaware. The home state bird of Delaware is the Delaware Blue Hen. The scientific name of the Delaware Blue Hen is Gallus gallus domesticus.\\'}tipLangSmith traceHelp us out by providing feedback on this documentation page:PreviousCreate a runnable with the @chain decoratorNextMultiple chainsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/prompt_size/', 'content_type': 'text/html; charset=utf-8', 'title': 'Managing prompt size | ü¶úÔ∏èüîó LangChain', 'description': \"Agents dynamically call tools. The results of those tool calls are added back to the prompt, so that the agent can plan the next action. Depending on what tools are being used and how they're being called, the agent prompt can easily grow larger than the model context window.\", 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nInspect your runnables | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageMoreInspect your runnablesOn this pageInspect your runnablesOnce you create a runnable with LCEL, you may often want to inspect it to get a better sense for what is going on. This notebook covers some methods for doing so.First, let\\'s create an example LCEL. We will create one that does retrieval%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktokenfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())Get a graph\\u200bYou can get a graph of the runnablechain.get_graph()Print a graph\\u200bWhile that is not super legible, you can print it to get a display that\\'s easier to understandchain.get_graph().print_ascii()           +---------------------------------+                    | Parallel<context,question>Input |                    +---------------------------------+                             **               **                                 ***                   ***                            **                         **           +----------------------+              +-------------+  | VectorStoreRetriever |              | Passthrough |  +----------------------+              +-------------+                      **               **                                      ***         ***                                           **     **                                +----------------------------------+                   | Parallel<context,question>Output |                   +----------------------------------+                                     *                                                      *                                                      *                                           +--------------------+                                 | ChatPromptTemplate |                                 +--------------------+                                            *                                                      *                                                      *                                               +------------+                                         | ChatOpenAI |                                         +------------+                                                *                                                      *                                                      *                                            +-----------------+                                    | StrOutputParser |                                    +-----------------+                                              *                                                      *                                                      *                                         +-----------------------+                              | StrOutputParserOutput |                              +-----------------------+Get the prompts\\u200bAn important part of every chain is the prompts that are used. You can get the prompts present in the chain:chain.get_prompts()[ChatPromptTemplate(input_variables=[\\'context\\', \\'question\\'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\\'context\\', \\'question\\'], template=\\'Answer the question based only on the following context:\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\n\\'))])]Help us out by providing feedback on this documentation page:PreviousRoute logic based on inputNextCreate a runnable with the @chain decoratorGet a graphPrint a graphGet the promptsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/how_to/inspect/', 'content_type': 'text/html; charset=utf-8', 'title': 'Inspect your runnables | ü¶úÔ∏èüîó LangChain', 'description': 'Once you create a runnable with LCEL, you may often want to inspect it to get a better sense for what is going on. This notebook covers some methods for doing so.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nGet started | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageGet startedOn this pageGet startedLCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.Basic example: prompt + model + output parser\\u200bThe most basic and common use case is chaining a prompt template and a model together. To see how this works, let\\'s create a chain that takes a topic and generates a joke:%pip install --upgrade --quiet  langchain-core langchain-community langchain-openaiOpenAIAnthropicGoogleCohereFireworksAIMistralAITogetherAIInstall dependenciespip install -qU langchain-openaiSet environment variablesimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4\")Install dependenciespip install -qU langchain-anthropicSet environment variablesimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")Install dependenciespip install -qU langchain-google-vertexaiSet environment variablesimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAImodel = ChatVertexAI(model=\"gemini-pro\")Install dependenciespip install -qU langchain-cohereSet environment variablesimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoheremodel = ChatCohere(model=\"command-r\")Install dependenciespip install -qU langchain-fireworksSet environment variablesimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksmodel = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")Install dependenciespip install -qU langchain-mistralaiSet environment variablesimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAImodel = ChatMistralAI(model=\"mistral-large-latest\")Install dependenciespip install -qU langchain-openaiSet environment variablesimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)# | output: false# | echo: falsefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4\")from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")output_parser = StrOutputParser()chain = prompt | model | output_parserchain.invoke({\"topic\": \"ice cream\"})\"Why don\\'t ice creams ever get invited to parties?\\\\n\\\\nBecause they always drip when things heat up!\"Notice this line of the code, where we piece together these different components into a single chain using LCEL:chain = prompt | model | output_parserThe | symbol is similar to a unix pipe operator, which chains together the different components, feeding the output from one component as input into the next component. In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. Let\\'s take a look at each component individually to really understand what\\'s going on.1. Prompt\\u200bprompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string.prompt_value = prompt.invoke({\"topic\": \"ice cream\"})prompt_valueChatPromptValue(messages=[HumanMessage(content=\\'tell me a short joke about ice cream\\')])prompt_value.to_messages()[HumanMessage(content=\\'tell me a short joke about ice cream\\')]prompt_value.to_string()\\'Human: tell me a short joke about ice cream\\'2. Model\\u200bThe PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage.message = model.invoke(prompt_value)messageAIMessage(content=\"Why don\\'t ice creams ever get invited to parties?\\\\n\\\\nBecause they always bring a melt down!\")If our model was an LLM, it would output a string.from langchain_openai import OpenAIllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")llm.invoke(prompt_value)\\'\\\\n\\\\nRobot: Why did the ice cream truck break down? Because it had a meltdown!\\'3. Output parser\\u200bAnd lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a\\nBaseMessage as input. The specific StrOutputParser simply converts any input into a string.output_parser.invoke(message)\"Why did the ice cream go to therapy? \\\\n\\\\nBecause it had too many toppings and couldn\\'t find its cone-fidence!\"4. Entire Pipeline\\u200bTo follow the steps along:We pass in user input on the desired topic as {\"topic\": \"ice cream\"}The prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt. The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method. infoNote that if you‚Äôre curious about the output of any components, you can always test out a smaller version of the chain such as prompt or prompt | model to see the intermediate results:input = {\"topic\": \"ice cream\"}prompt.invoke(input)# > ChatPromptValue(messages=[HumanMessage(content=\\'tell me a short joke about ice cream\\')])(prompt | model).invoke(input)# > AIMessage(content=\"Why did the ice cream go to therapy?\\\\nBecause it had too many toppings and couldn\\'t cone-trol itself!\")RAG Search Example\\u200bFor our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions.OpenAIAnthropicGoogleCohereFireworksAIMistralAITogetherAIInstall dependenciespip install -qU langchain-openaiSet environment variablesimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")Install dependenciespip install -qU langchain-anthropicSet environment variablesimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")Install dependenciespip install -qU langchain-google-vertexaiSet environment variablesimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAImodel = ChatVertexAI(model=\"gemini-pro\")Install dependenciespip install -qU langchain-cohereSet environment variablesimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoheremodel = ChatCohere(model=\"command-r\")Install dependenciespip install -qU langchain-fireworksSet environment variablesimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksmodel = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")Install dependenciespip install -qU langchain-mistralaiSet environment variablesimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAImodel = ChatMistralAI(model=\"mistral-large-latest\")Install dependenciespip install -qU langchain-openaiSet environment variablesimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)# Requires:# pip install langchain docarray tiktokenfrom langchain_community.vectorstores import DocArrayInMemorySearchfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableParallel, RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsvectorstore = DocArrayInMemorySearch.from_texts(    [\"harrison worked at kensho\", \"bears like to eat honey\"],    embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)output_parser = StrOutputParser()setup_and_retrieval = RunnableParallel(    {\"context\": retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | prompt | model | output_parserchain.invoke(\"where did harrison work?\")In this case, the composed chain is: chain = setup_and_retrieval | prompt | model | output_parserTo explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context. As a preliminary step, we‚Äôve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:retriever.invoke(\"where did harrison work?\")We then use the RunnableParallel to prepare the expected inputs into the prompt by using the entries for the retrieved documents as well as the original user question, using the retriever for document search, and RunnablePassthrough to pass the user‚Äôs question:setup_and_retrieval = RunnableParallel(    {\"context\": retriever, \"question\": RunnablePassthrough()})To review, the complete chain is:setup_and_retrieval = RunnableParallel(    {\"context\": retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | prompt | model | output_parserWith the flow being:The first steps create a RunnableParallel object with two entries.  The first entry, context will include the document results fetched by the retriever. The second entry, question will contain the user‚Äôs original question. To pass on the question, we use RunnablePassthrough to copy this entry. Feed the dictionary from the step above to the prompt component. It then takes the user input which is question as well as the retrieved document which is context to construct a prompt and output a PromptValue. The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.Next steps\\u200bWe recommend reading our Advantages of LCEL section next to see a side-by-side comparison of the code needed to produce common functionality with and without LCEL.Help us out by providing feedback on this documentation page:PreviousLangChain Expression Language (LCEL)NextRunnable interfaceBasic example: prompt + model + output parser1. Prompt2. Model3. Output parser4. Entire PipelineRAG Search ExampleNext stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/get_started/', 'content_type': 'text/html; charset=utf-8', 'title': 'Get started | ü¶úÔ∏èüîó LangChain', 'description': 'LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nRunnable interface | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageRunnable interfaceOn this pageRunnable interfaceTo make it as easy as possible to create custom chains, we\\'ve implemented a \"Runnable\" protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about in this section.This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way.\\nThe standard interface includes:stream: stream back chunks of the responseinvoke: call the chain on an inputbatch: call the chain on a list of inputsThese also have corresponding async methods that should be used with asyncio await syntax for concurrency:astream: stream back chunks of the response asyncainvoke: call the chain on an input asyncabatch: call the chain on a list of inputs asyncastream_log: stream back intermediate steps as they happen, in addition to the final responseastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)The input type and output type varies by component:ComponentInput TypeOutput TypePromptDictionaryPromptValueChatModelSingle string, list of chat messages or a PromptValueChatMessageLLMSingle string, list of chat messages or a PromptValueStringOutputParserThe output of an LLM or ChatModelDepends on the parserRetrieverSingle stringList of DocumentsToolSingle string or dictionary, depending on the toolDepends on the toolAll runnables expose input and output schemas to inspect the inputs and outputs:input_schema: an input Pydantic model auto-generated from the structure of the Runnableoutput_schema: an output Pydantic model auto-generated from the structure of the RunnableLet\\'s take a look at these methods. To do so, we\\'ll create a super simple PromptTemplate + ChatModel chain.%pip install --upgrade --quiet  langchain-core langchain-community langchain-openaifrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelInput Schema\\u200bA description of the inputs accepted by a Runnable.\\nThis is a Pydantic model dynamically generated from the structure of any Runnable.\\nYou can call .schema() on it to obtain a JSONSchema representation.# The input schema of the chain is the input schema of its first part, the prompt.chain.input_schema.schema(){\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\': {\\'topic\\': {\\'title\\': \\'Topic\\', \\'type\\': \\'string\\'}}}prompt.input_schema.schema(){\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\': {\\'topic\\': {\\'title\\': \\'Topic\\', \\'type\\': \\'string\\'}}}model.input_schema.schema(){\\'title\\': \\'ChatOpenAIInput\\', \\'anyOf\\': [{\\'type\\': \\'string\\'},  {\\'$ref\\': \\'#/definitions/StringPromptValue\\'},  {\\'$ref\\': \\'#/definitions/ChatPromptValueConcrete\\'},  {\\'type\\': \\'array\\',   \\'items\\': {\\'anyOf\\': [{\\'$ref\\': \\'#/definitions/AIMessage\\'},     {\\'$ref\\': \\'#/definitions/HumanMessage\\'},     {\\'$ref\\': \\'#/definitions/ChatMessage\\'},     {\\'$ref\\': \\'#/definitions/SystemMessage\\'},     {\\'$ref\\': \\'#/definitions/FunctionMessage\\'},     {\\'$ref\\': \\'#/definitions/ToolMessage\\'}]}}], \\'definitions\\': {\\'StringPromptValue\\': {\\'title\\': \\'StringPromptValue\\',   \\'description\\': \\'String prompt value.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'text\\': {\\'title\\': \\'Text\\', \\'type\\': \\'string\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'StringPromptValue\\',     \\'enum\\': [\\'StringPromptValue\\'],     \\'type\\': \\'string\\'}},   \\'required\\': [\\'text\\']},  \\'AIMessage\\': {\\'title\\': \\'AIMessage\\',   \\'description\\': \\'A Message from an AI.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'ai\\',     \\'enum\\': [\\'ai\\'],     \\'type\\': \\'string\\'},    \\'example\\': {\\'title\\': \\'Example\\', \\'default\\': False, \\'type\\': \\'boolean\\'}},   \\'required\\': [\\'content\\']},  \\'HumanMessage\\': {\\'title\\': \\'HumanMessage\\',   \\'description\\': \\'A Message from a human.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'human\\',     \\'enum\\': [\\'human\\'],     \\'type\\': \\'string\\'},    \\'example\\': {\\'title\\': \\'Example\\', \\'default\\': False, \\'type\\': \\'boolean\\'}},   \\'required\\': [\\'content\\']},  \\'ChatMessage\\': {\\'title\\': \\'ChatMessage\\',   \\'description\\': \\'A Message that can be assigned an arbitrary speaker (i.e. role).\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'chat\\',     \\'enum\\': [\\'chat\\'],     \\'type\\': \\'string\\'},    \\'role\\': {\\'title\\': \\'Role\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'role\\']},  \\'SystemMessage\\': {\\'title\\': \\'SystemMessage\\',   \\'description\\': \\'A Message for priming AI behavior, usually passed in as the first of a sequence\\\\nof input messages.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'system\\',     \\'enum\\': [\\'system\\'],     \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\']},  \\'FunctionMessage\\': {\\'title\\': \\'FunctionMessage\\',   \\'description\\': \\'A Message for passing the result of executing a function back to a model.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'function\\',     \\'enum\\': [\\'function\\'],     \\'type\\': \\'string\\'},    \\'name\\': {\\'title\\': \\'Name\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'name\\']},  \\'ToolMessage\\': {\\'title\\': \\'ToolMessage\\',   \\'description\\': \\'A Message for passing the result of executing a tool back to a model.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'tool\\',     \\'enum\\': [\\'tool\\'],     \\'type\\': \\'string\\'},    \\'tool_call_id\\': {\\'title\\': \\'Tool Call Id\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'tool_call_id\\']},  \\'ChatPromptValueConcrete\\': {\\'title\\': \\'ChatPromptValueConcrete\\',   \\'description\\': \\'Chat prompt value which explicitly lists out the message types it accepts.\\\\nFor use in external schemas.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'messages\\': {\\'title\\': \\'Messages\\',     \\'type\\': \\'array\\',     \\'items\\': {\\'anyOf\\': [{\\'$ref\\': \\'#/definitions/AIMessage\\'},       {\\'$ref\\': \\'#/definitions/HumanMessage\\'},       {\\'$ref\\': \\'#/definitions/ChatMessage\\'},       {\\'$ref\\': \\'#/definitions/SystemMessage\\'},       {\\'$ref\\': \\'#/definitions/FunctionMessage\\'},       {\\'$ref\\': \\'#/definitions/ToolMessage\\'}]}},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'ChatPromptValueConcrete\\',     \\'enum\\': [\\'ChatPromptValueConcrete\\'],     \\'type\\': \\'string\\'}},   \\'required\\': [\\'messages\\']}}}Output Schema\\u200bA description of the outputs produced by a Runnable.\\nThis is a Pydantic model dynamically generated from the structure of any Runnable.\\nYou can call .schema() on it to obtain a JSONSchema representation.# The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessagechain.output_schema.schema(){\\'title\\': \\'ChatOpenAIOutput\\', \\'anyOf\\': [{\\'$ref\\': \\'#/definitions/AIMessage\\'},  {\\'$ref\\': \\'#/definitions/HumanMessage\\'},  {\\'$ref\\': \\'#/definitions/ChatMessage\\'},  {\\'$ref\\': \\'#/definitions/SystemMessage\\'},  {\\'$ref\\': \\'#/definitions/FunctionMessage\\'},  {\\'$ref\\': \\'#/definitions/ToolMessage\\'}], \\'definitions\\': {\\'AIMessage\\': {\\'title\\': \\'AIMessage\\',   \\'description\\': \\'A Message from an AI.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'ai\\',     \\'enum\\': [\\'ai\\'],     \\'type\\': \\'string\\'},    \\'example\\': {\\'title\\': \\'Example\\', \\'default\\': False, \\'type\\': \\'boolean\\'}},   \\'required\\': [\\'content\\']},  \\'HumanMessage\\': {\\'title\\': \\'HumanMessage\\',   \\'description\\': \\'A Message from a human.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'human\\',     \\'enum\\': [\\'human\\'],     \\'type\\': \\'string\\'},    \\'example\\': {\\'title\\': \\'Example\\', \\'default\\': False, \\'type\\': \\'boolean\\'}},   \\'required\\': [\\'content\\']},  \\'ChatMessage\\': {\\'title\\': \\'ChatMessage\\',   \\'description\\': \\'A Message that can be assigned an arbitrary speaker (i.e. role).\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'chat\\',     \\'enum\\': [\\'chat\\'],     \\'type\\': \\'string\\'},    \\'role\\': {\\'title\\': \\'Role\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'role\\']},  \\'SystemMessage\\': {\\'title\\': \\'SystemMessage\\',   \\'description\\': \\'A Message for priming AI behavior, usually passed in as the first of a sequence\\\\nof input messages.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'system\\',     \\'enum\\': [\\'system\\'],     \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\']},  \\'FunctionMessage\\': {\\'title\\': \\'FunctionMessage\\',   \\'description\\': \\'A Message for passing the result of executing a function back to a model.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'function\\',     \\'enum\\': [\\'function\\'],     \\'type\\': \\'string\\'},    \\'name\\': {\\'title\\': \\'Name\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'name\\']},  \\'ToolMessage\\': {\\'title\\': \\'ToolMessage\\',   \\'description\\': \\'A Message for passing the result of executing a tool back to a model.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'tool\\',     \\'enum\\': [\\'tool\\'],     \\'type\\': \\'string\\'},    \\'tool_call_id\\': {\\'title\\': \\'Tool Call Id\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'tool_call_id\\']}}}Stream\\u200bfor s in chain.stream({\"topic\": \"bears\"}):    print(s.content, end=\"\", flush=True)Sure, here\\'s a bear-themed joke for you:Why don\\'t bears wear shoes?Because they already have bear feet!Invoke\\u200bchain.invoke({\"topic\": \"bears\"})AIMessage(content=\"Why don\\'t bears wear shoes? \\\\n\\\\nBecause they have bear feet!\")Batch\\u200bchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])[AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\"), AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]You can set the number of concurrent requests by using the max_concurrency parameterchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why don\\'t cats play poker in the wild? Too many cheetahs!\")]Async Stream\\u200basync for s in chain.astream({\"topic\": \"bears\"}):    print(s.content, end=\"\", flush=True)Why don\\'t bears wear shoes?Because they have bear feet!Async Invoke\\u200bawait chain.ainvoke({\"topic\": \"bears\"})AIMessage(content=\"Why don\\'t bears ever wear shoes?\\\\n\\\\nBecause they already have bear feet!\")Async Batch\\u200bawait chain.abatch([{\"topic\": \"bears\"}])[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")]Async Stream Events (beta)\\u200bEvent Streaming is a beta API, and may change a bit based on feedback.Note: Introduced in langchain-core 0.2.0For now, when using the astream_events API, for everything to work properly please:Use async throughout the code (including async tools etc)Propagate callbacks if defining custom functions / runnables. Whenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens.Event Reference\\u200bHere is a reference table that shows some events that might be emitted by the various Runnable objects.\\nDefinitions for some of the Runnable are included after the table.‚ö†Ô∏è When streaming the inputs for the runnable will not be available until the input stream has been entirely consumed This means that the inputs will be available at for the corresponding end hook rather than start event.eventnamechunkinputoutputon_chat_model_start[model name]{\"messages\": [[SystemMessage, HumanMessage]]}on_chat_model_stream[model name]AIMessageChunk(content=\"hello\")on_chat_model_end[model name]{\"messages\": [[SystemMessage, HumanMessage]]}{\"generations\": [...], \"llm_output\": None, ...}on_llm_start[model name]{\\'input\\': \\'hello\\'}on_llm_stream[model name]\\'Hello\\'on_llm_end[model name]\\'Hello human!\\'on_chain_startformat_docson_chain_streamformat_docs\"hello world!, goodbye world!\"on_chain_endformat_docs[Document(...)]\"hello world!, goodbye world!\"on_tool_startsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_streamsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_endsome_tool{\"x\": 1, \"y\": \"2\"}on_retriever_start[retriever name]{\"query\": \"hello\"}on_retriever_chunk[retriever name]{documents: [...]}on_retriever_end[retriever name]{\"query\": \"hello\"}{documents: [...]}on_prompt_start[template_name]{\"question\": \"hello\"}on_prompt_end[template_name]{\"question\": \"hello\"}ChatPromptValue(messages: [SystemMessage, ...])Here are declarations associated with the events shown above:format_docs:def format_docs(docs: List[Document]) -> str:    \\'\\'\\'Format the docs.\\'\\'\\'    return \", \".join([doc.page_content for doc in docs])format_docs = RunnableLambda(format_docs)some_tool:@tooldef some_tool(x: int, y: str) -> dict:    \\'\\'\\'Some_tool.\\'\\'\\'    return {\"x\": x, \"y\": y}prompt:template = ChatPromptTemplate.from_messages(    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})Let\\'s define a new chain to make it more interesting to show off the astream_events interface (and later the astream_log interface).from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()retrieval_chain = (    {        \"context\": retriever.with_config(run_name=\"Docs\"),        \"question\": RunnablePassthrough(),    }    | prompt    | model.with_config(run_name=\"my_llm\")    | StrOutputParser())Now let\\'s use astream_events to get events from the retriever and the LLM.async for event in retrieval_chain.astream_events(    \"where did harrison work?\", version=\"v1\", include_names=[\"Docs\", \"my_llm\"]):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(event[\"data\"][\"chunk\"].content, end=\"|\")    elif kind in {\"on_chat_model_start\"}:        print()        print(\"Streaming LLM:\")    elif kind in {\"on_chat_model_end\"}:        print()        print(\"Done streaming LLM.\")    elif kind == \"on_retriever_end\":        print(\"--\")        print(\"Retrieved the following documents:\")        print(event[\"data\"][\"output\"][\"documents\"])    elif kind == \"on_tool_end\":        print(f\"Ended tool: {event[\\'name\\']}\")    else:        pass/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.  warn_beta(``````output--Retrieved the following documents:[Document(page_content=\\'harrison worked at kensho\\')]Streaming LLM:|H|arrison| worked| at| Kens|ho|.||Done streaming LLM.Async Stream Intermediate Steps\\u200bAll runnables also have a method .astream_log() which is used to stream (as they happen) all or part of the intermediate steps of your chain/sequence. This is useful to show progress to the user, to use intermediate results, or to debug your chain.You can stream all steps (default) or include/exclude steps by name, tags or metadata.This method yields JSONPatch ops that when applied in the same order as received build up the RunState.class LogEntry(TypedDict):    id: str    \"\"\"ID of the sub-run.\"\"\"    name: str    \"\"\"Name of the object being run.\"\"\"    type: str    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"    tags: List[str]    \"\"\"List of tags for the run.\"\"\"    metadata: Dict[str, Any]    \"\"\"Key-value pairs of metadata for the run.\"\"\"    start_time: str    \"\"\"ISO-8601 timestamp of when the run started.\"\"\"    streamed_output_str: List[str]    \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"    final_output: Optional[Any]    \"\"\"Final output of this run.    Only available after the run has finished successfully.\"\"\"    end_time: Optional[str]    \"\"\"ISO-8601 timestamp of when the run ended.    Only available after the run has finished.\"\"\"class RunState(TypedDict):    id: str    \"\"\"ID of the run.\"\"\"    streamed_output: List[Any]    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"    final_output: Optional[Any]    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.    Only available after the run has finished successfully.\"\"\"    logs: Dict[str, LogEntry]    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will    contain only the runs that matched the filters.\"\"\"Streaming JSONPatch chunks\\u200bThis is useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops on the client to rebuild the run state there. See LangServe for tooling to make it easier to build a webserver from any Runnable.async for chunk in retrieval_chain.astream_log(    \"where did harrison work?\", include_names=[\"Docs\"]):    print(\"-\" * 40)    print(chunk)----------------------------------------RunLogPatch({\\'op\\': \\'replace\\',  \\'path\\': \\'\\',  \\'value\\': {\\'final_output\\': None,            \\'id\\': \\'82e9b4b1-3dd6-4732-8db9-90e79c4da48c\\',            \\'logs\\': {},            \\'name\\': \\'RunnableSequence\\',            \\'streamed_output\\': [],            \\'type\\': \\'chain\\'}})----------------------------------------RunLogPatch({\\'op\\': \\'add\\',  \\'path\\': \\'/logs/Docs\\',  \\'value\\': {\\'end_time\\': None,            \\'final_output\\': None,            \\'id\\': \\'9206e94a-57bd-48ee-8c5e-fdd1c52a6da2\\',            \\'metadata\\': {},            \\'name\\': \\'Docs\\',            \\'start_time\\': \\'2024-01-19T22:33:55.902+00:00\\',            \\'streamed_output\\': [],            \\'streamed_output_str\\': [],            \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],            \\'type\\': \\'retriever\\'}})----------------------------------------RunLogPatch({\\'op\\': \\'add\\',  \\'path\\': \\'/logs/Docs/final_output\\',  \\'value\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]}}, {\\'op\\': \\'add\\',  \\'path\\': \\'/logs/Docs/end_time\\',  \\'value\\': \\'2024-01-19T22:33:56.064+00:00\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'H\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'H\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'arrison\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'Harrison\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\' worked\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'Harrison worked\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\' at\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'Harrison worked at\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\' Kens\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'Harrison worked at Kens\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'ho\\'}, {\\'op\\': \\'replace\\',  \\'path\\': \\'/final_output\\',  \\'value\\': \\'Harrison worked at Kensho\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'.\\'}, {\\'op\\': \\'replace\\',  \\'path\\': \\'/final_output\\',  \\'value\\': \\'Harrison worked at Kensho.\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'\\'})Streaming the incremental RunState\\u200bYou can simply pass diff=False to get incremental values of RunState.\\nYou get more verbose output with more repetitive parts.async for chunk in retrieval_chain.astream_log(    \"where did harrison work?\", include_names=[\"Docs\"], diff=False):    print(\"-\" * 70)    print(chunk)----------------------------------------------------------------------RunLog({\\'final_output\\': None, \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': None, \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': None,                   \\'final_output\\': None,                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': None, \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'H\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at Kens\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\', \\' Kens\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at Kensho\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\', \\' Kens\\', \\'ho\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at Kensho.\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\', \\' Kens\\', \\'ho\\', \\'.\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at Kensho.\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\',                     \\'H\\',                     \\'arrison\\',                     \\' worked\\',                     \\' at\\',                     \\' Kens\\',                     \\'ho\\',                     \\'.\\',                     \\'\\'], \\'type\\': \\'chain\\'})Parallelism\\u200bLet\\'s take a look at how LangChain Expression Language supports parallel requests.\\nFor example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel.from langchain_core.runnables import RunnableParallelchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | modelchain2 = (    ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")    | model)combined = RunnableParallel(joke=chain1, poem=chain2)%%timechain1.invoke({\"topic\": \"bears\"})CPU times: user 18 ms, sys: 1.27 ms, total: 19.3 msWall time: 692 msAIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")%%timechain2.invoke({\"topic\": \"bears\"})CPU times: user 10.5 ms, sys: 166 ¬µs, total: 10.7 msWall time: 579 msAIMessage(content=\"In forest\\'s embrace,\\\\nMajestic bears pace.\")%%timecombined.invoke({\"topic\": \"bears\"})CPU times: user 32 ms, sys: 2.59 ms, total: 34.6 msWall time: 816 ms{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear-related joke for you:\\\\n\\\\nWhy did the bear bring a ladder to the bar?\\\\n\\\\nBecause he heard the drinks were on the house!\"), \\'poem\\': AIMessage(content=\"In wilderness they roam,\\\\nMajestic strength, nature\\'s throne.\")}Parallelism on batches\\u200bParallelism can be combined with other runnables.\\nLet\\'s try to use parallelism with batches.%%timechain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU times: user 17.3 ms, sys: 4.84 ms, total: 22.2 msWall time: 628 ms[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]%%timechain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU times: user 15.8 ms, sys: 3.83 ms, total: 19.7 msWall time: 718 ms[AIMessage(content=\\'In the wild, bears roam,\\\\nMajestic guardians of ancient home.\\'), AIMessage(content=\\'Whiskers grace, eyes gleam,\\\\nCats dance through the moonbeam.\\')]%%timecombined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU times: user 44.8 ms, sys: 3.17 ms, total: 48 msWall time: 721 ms[{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"),  \\'poem\\': AIMessage(content=\"Majestic bears roam,\\\\nNature\\'s strength, beauty shown.\")}, {\\'joke\\': AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\"),  \\'poem\\': AIMessage(content=\"Whiskers dance, eyes aglow,\\\\nCats embrace the night\\'s gentle flow.\")}]Help us out by providing feedback on this documentation page:PreviousGet startedNextPrimitivesInput SchemaOutput SchemaStreamInvokeBatchAsync StreamAsync InvokeAsync BatchAsync Stream Events (beta)Event ReferenceAsync Stream Intermediate StepsStreaming JSONPatch chunksStreaming the incremental RunStateParallelismParallelism on batchesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/interface/', 'content_type': 'text/html; charset=utf-8', 'title': 'Runnable interface | ü¶úÔ∏èüîó LangChain', 'description': 'To make it as easy as possible to create custom chains, we\\'ve implemented a \"Runnable\" protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about in this section.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nAdvantages of LCEL | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageAdvantages of LCELOn this pageAdvantages of LCELtipWe recommend reading the LCEL Get started section first.LCEL is designed to streamline the process of building useful apps with LLMs and combining related components. It does this by providing:A unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, ...). This makes it possible for chains of LCEL objects to also automatically support useful operations like batching and streaming of intermediate steps, since every chain of LCEL objects is itself an LCEL object.Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internals, and more.To better understand the value of LCEL, it\\'s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough we\\'ll do just that with our basic example from the get started section. We\\'ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it.%pip install --upgrade --quiet  langchain-core langchain-openai langchain-anthropicInvoke\\u200bIn the simplest case, we just want to pass in a topic string and get back a joke string:Without LCEL\\u200bfrom typing import Listimport openaiprompt_template = \"Tell me a short joke about {topic}\"client = openai.OpenAI()def call_chat_model(messages: List[dict]) -> str:    response = client.chat.completions.create(        model=\"gpt-3.5-turbo\",         messages=messages,    )    return response.choices[0].message.contentdef invoke_chain(topic: str) -> str:    prompt_value = prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", \"content\": prompt_value}]    return call_chat_model(messages)invoke_chain(\"ice cream\")LCEL\\u200bfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughprompt = ChatPromptTemplate.from_template(    \"Tell me a short joke about {topic}\")output_parser = StrOutputParser()model = ChatOpenAI(model=\"gpt-3.5-turbo\")chain = (    {\"topic\": RunnablePassthrough()}     | prompt    | model    | output_parser)chain.invoke(\"ice cream\")Stream\\u200bIf we want to stream results instead, we\\'ll need to change our function:Without LCEL\\u200bfrom typing import Iteratordef stream_chat_model(messages: List[dict]) -> Iterator[str]:    stream = client.chat.completions.create(        model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    for response in stream:        content = response.choices[0].delta.content        if content is not None:            yield contentdef stream_chain(topic: str) -> Iterator[str]:    prompt_value = prompt.format(topic=topic)    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])for chunk in stream_chain(\"ice cream\"):    print(chunk, end=\"\", flush=True)LCEL\\u200bfor chunk in chain.stream(\"ice cream\"):    print(chunk, end=\"\", flush=True)Batch\\u200bIf we want to run on a batch of inputs in parallel, we\\'ll again need a new function:Without LCEL\\u200bfrom concurrent.futures import ThreadPoolExecutordef batch_chain(topics: list) -> list:    with ThreadPoolExecutor(max_workers=5) as executor:        return list(executor.map(invoke_chain, topics))batch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])LCEL\\u200bchain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])## AsyncIf we need an asynchronous version:Without LCEL\\u200basync_client = openai.AsyncOpenAI()async def acall_chat_model(messages: List[dict]) -> str:    response = await async_client.chat.completions.create(        model=\"gpt-3.5-turbo\",         messages=messages,    )    return response.choices[0].message.contentasync def ainvoke_chain(topic: str) -> str:    prompt_value = prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", \"content\": prompt_value}]    return await acall_chat_model(messages)await ainvoke_chain(\"ice cream\")LCEL\\u200bawait chain.ainvoke(\"ice cream\")Async Batch\\u200bWithout LCEL\\u200bimport asyncioimport openaiasync def abatch_chain(topics: list) -> list:    coros = map(ainvoke_chain, topics)    return await asyncio.gather(*coros)await abatch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])LCEL\\u200bawait chain.abatch([\"ice cream\", \"spaghetti\", \"dumplings\"])LLM instead of chat model\\u200bIf we want to use a completion endpoint instead of a chat endpoint: Without LCEL\\u200bdef call_llm(prompt_value: str) -> str:    response = client.completions.create(        model=\"gpt-3.5-turbo-instruct\",        prompt=prompt_value,    )    return response.choices[0].textdef invoke_llm_chain(topic: str) -> str:    prompt_value = prompt_template.format(topic=topic)    return call_llm(prompt_value)invoke_llm_chain(\"ice cream\")LCEL\\u200bfrom langchain_openai import OpenAIllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")llm_chain = (    {\"topic\": RunnablePassthrough()}     | prompt    | llm    | output_parser)llm_chain.invoke(\"ice cream\")Different model provider\\u200bIf we want to use Anthropic instead of OpenAI: Without LCEL\\u200bimport anthropicanthropic_template = f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"anthropic_client = anthropic.Anthropic()def call_anthropic(prompt_value: str) -> str:    response = anthropic_client.completions.create(        model=\"claude-2\",        prompt=prompt_value,        max_tokens_to_sample=256,    )    return response.completion    def invoke_anthropic_chain(topic: str) -> str:    prompt_value = anthropic_template.format(topic=topic)    return call_anthropic(prompt_value)invoke_anthropic_chain(\"ice cream\")LCEL\\u200bfrom langchain_anthropic import ChatAnthropicanthropic = ChatAnthropic(model=\"claude-2\")anthropic_chain = (    {\"topic\": RunnablePassthrough()}     | prompt     | anthropic    | output_parser)anthropic_chain.invoke(\"ice cream\")Runtime configurability\\u200bIf we wanted to make the choice of chat model or LLM configurable at runtime:Without LCEL\\u200bdef invoke_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> str:    if model == \"chat_openai\":        return invoke_chain(topic)    elif model == \"openai\":        return invoke_llm_chain(topic)    elif model == \"anthropic\":        return invoke_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def stream_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> Iterator[str]:    if model == \"chat_openai\":        return stream_chain(topic)    elif model == \"openai\":        # Note we haven\\'t implemented this yet.        return stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we haven\\'t implemented this yet        return stream_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def batch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    # You get the idea    ...async def abatch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    ...invoke_configurable_chain(\"ice cream\", model=\"openai\")stream = stream_configurable_chain(    \"ice_cream\",     model=\"anthropic\")for chunk in stream:    print(chunk, end=\"\", flush=True)# batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])# await ainvoke_configurable_chain(\"ice cream\")With LCEL\\u200bfrom langchain_core.runnables import ConfigurableFieldconfigurable_model = model.configurable_alternatives(    ConfigurableField(id=\"model\"),     default_key=\"chat_openai\",     openai=llm,    anthropic=anthropic,)configurable_chain = (    {\"topic\": RunnablePassthrough()}     | prompt     | configurable_model     | output_parser)configurable_chain.invoke(    \"ice cream\",     config={\"model\": \"openai\"})stream = configurable_chain.stream(    \"ice cream\",     config={\"model\": \"anthropic\"})for chunk in stream:    print(chunk, end=\"\", flush=True)configurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])# await configurable_chain.ainvoke(\"ice cream\")Logging\\u200bIf we want to log our intermediate results:Without LCEL\\u200bWe\\'ll print intermediate steps for illustrative purposesdef invoke_anthropic_chain_with_logging(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = anthropic_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    output = call_anthropic(prompt_value)    print(f\"Output: {output}\")    return outputinvoke_anthropic_chain_with_logging(\"ice cream\")LCEL\\u200bEvery component has built-in integrations with LangSmith. If we set the following two environment variables, all chain traces are logged to LangSmith.import osos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"anthropic_chain.invoke(\"ice cream\")Here\\'s what our LangSmith trace looks like: https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/rFallbacks\\u200bIf we wanted to add fallback logic, in case one model API is down:Without LCEL\\u200bdef invoke_chain_with_fallback(topic: str) -> str:    try:        return invoke_chain(topic)    except Exception:        return invoke_anthropic_chain(topic)async def ainvoke_chain_with_fallback(topic: str) -> str:    try:        return await ainvoke_chain(topic)    except Exception:        # Note: we haven\\'t actually implemented this.        return await ainvoke_anthropic_chain(topic)async def batch_chain_with_fallback(topics: List[str]) -> str:    try:        return batch_chain(topics)    except Exception:        # Note: we haven\\'t actually implemented this.        return batch_anthropic_chain(topics)invoke_chain_with_fallback(\"ice cream\")# await ainvoke_chain_with_fallback(\"ice cream\")batch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"]))LCEL\\u200bfallback_chain = chain.with_fallbacks([anthropic_chain])fallback_chain.invoke(\"ice cream\")# await fallback_chain.ainvoke(\"ice cream\")fallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])Full code comparison\\u200bEven in this simple case, our LCEL chain succinctly packs in a lot of functionality. As chains become more complex, this becomes especially valuable.Without LCEL\\u200bfrom concurrent.futures import ThreadPoolExecutorfrom typing import Iterator, List, Tupleimport anthropicimport openaiprompt_template = \"Tell me a short joke about {topic}\"anthropic_template = f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"client = openai.OpenAI()async_client = openai.AsyncOpenAI()anthropic_client = anthropic.Anthropic()def call_chat_model(messages: List[dict]) -> str:    response = client.chat.completions.create(        model=\"gpt-3.5-turbo\",         messages=messages,    )    return response.choices[0].message.contentdef invoke_chain(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = prompt_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    messages = [{\"role\": \"user\", \"content\": prompt_value}]    output = call_chat_model(messages)    print(f\"Output: {output}\")    return outputdef stream_chat_model(messages: List[dict]) -> Iterator[str]:    stream = client.chat.completions.create(        model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    for response in stream:        content = response.choices[0].delta.content        if content is not None:            yield contentdef stream_chain(topic: str) -> Iterator[str]:    print(f\"Input: {topic}\")    prompt_value = prompt.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])    for chunk in stream:        print(f\"Token: {chunk}\", end=\"\")        yield chunkdef batch_chain(topics: list) -> list:    with ThreadPoolExecutor(max_workers=5) as executor:        return list(executor.map(invoke_chain, topics))def call_llm(prompt_value: str) -> str:    response = client.completions.create(        model=\"gpt-3.5-turbo-instruct\",        prompt=prompt_value,    )    return response.choices[0].textdef invoke_llm_chain(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = promtp_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    output = call_llm(prompt_value)    print(f\"Output: {output}\")    return outputdef call_anthropic(prompt_value: str) -> str:    response = anthropic_client.completions.create(        model=\"claude-2\",        prompt=prompt_value,        max_tokens_to_sample=256,    )    return response.completion   def invoke_anthropic_chain(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = anthropic_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    output = call_anthropic(prompt_value)    print(f\"Output: {output}\")    return outputasync def ainvoke_anthropic_chain(topic: str) -> str:    ...def stream_anthropic_chain(topic: str) -> Iterator[str]:    ...def batch_anthropic_chain(topics: List[str]) -> List[str]:    ...def invoke_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> str:    if model == \"chat_openai\":        return invoke_chain(topic)    elif model == \"openai\":        return invoke_llm_chain(topic)    elif model == \"anthropic\":        return invoke_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def stream_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> Iterator[str]:    if model == \"chat_openai\":        return stream_chain(topic)    elif model == \"openai\":        # Note we haven\\'t implemented this yet.        return stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we haven\\'t implemented this yet        return stream_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def batch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    ...async def abatch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    ...def invoke_chain_with_fallback(topic: str) -> str:    try:        return invoke_chain(topic)    except Exception:        return invoke_anthropic_chain(topic)async def ainvoke_chain_with_fallback(topic: str) -> str:    try:        return await ainvoke_chain(topic)    except Exception:        return await ainvoke_anthropic_chain(topic)async def batch_chain_with_fallback(topics: List[str]) -> str:    try:        return batch_chain(topics)    except Exception:        return batch_anthropic_chain(topics)LCEL\\u200bimport osfrom langchain_anthropic import ChatAnthropicfrom langchain_openai import ChatOpenAIfrom langchain_openai import OpenAIfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthrough, ConfigurableFieldos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"prompt = ChatPromptTemplate.from_template(    \"Tell me a short joke about {topic}\")chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")anthropic = ChatAnthropic(model=\"claude-2\")model = (    chat_openai    .with_fallbacks([anthropic])    .configurable_alternatives(        ConfigurableField(id=\"model\"),        default_key=\"chat_openai\",        openai=openai,        anthropic=anthropic,    ))chain = (    {\"topic\": RunnablePassthrough()}     | prompt     | model     | StrOutputParser())Next steps\\u200bTo continue learning about LCEL, we recommend:Reading up on the full LCEL Interface, which we\\'ve only partially covered here.Exploring the primitives to learn more about what LCEL provides.Help us out by providing feedback on this documentation page:PreviousPrimitivesNextStreamingInvokeStreamBatchAsync BatchLLM instead of chat modelDifferent model providerRuntime configurabilityLoggingFallbacksFull code comparisonNext stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/why/', 'content_type': 'text/html; charset=utf-8', 'title': 'Advantages of LCEL | ü¶úÔ∏èüîó LangChain', 'description': 'We recommend reading the LCEL Get started section first.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nAdd message history (memory) | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageAdd message history (memory)On this pageAdd message history (memory)The RunnableWithMessageHistory lets us add message history to certain types of chains. It wraps another Runnable and manages the chat message history for it.Specifically, it can be used for any Runnable that takes as input one ofa sequence of BaseMessagea dict with a key that takes a sequence of BaseMessagea dict with a key that takes the latest message(s) as a string or sequence of BaseMessage, and a separate key that takes historical messagesAnd returns as output one ofa string that can be treated as the contents of an AIMessagea sequence of BaseMessagea dict with a key that contains a sequence of BaseMessageLet\\'s take a look at some examples to see how it works. First we construct a runnable (which here accepts a dict as input and returns a message as output):from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai.chat_models import ChatOpenAImodel = ChatOpenAI()prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You\\'re an assistant who\\'s good at {ability}. Respond in 20 words or fewer\",        ),        MessagesPlaceholder(variable_name=\"history\"),        (\"human\", \"{input}\"),    ])runnable = prompt | modelTo manage the message history, we will need:This runnable;A callable that returns an instance of BaseChatMessageHistory.Check out the memory integrations page for implementations of chat message histories using Redis and other providers. Here we demonstrate using an in-memory ChatMessageHistory as well as more persistent storage using RedisChatMessageHistory.In-memory\\u200bBelow we show a simple example in which the chat history lives in memory, in this case via a global Python dict.We construct a callable get_session_history that references this dict to return an instance of ChatMessageHistory. The arguments to the callable can be specified by passing a configuration to the RunnableWithMessageHistory at runtime. By default, the configuration parameter is expected to be a single string session_id. This can be adjusted via the history_factory_config kwarg.Using the single-parameter default:from langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {}def get_session_history(session_id: str) -> BaseChatMessageHistory:    if session_id not in store:        store[session_id] = ChatMessageHistory()    return store[session_id]with_message_history = RunnableWithMessageHistory(    runnable,    get_session_history,    input_messages_key=\"input\",    history_messages_key=\"history\",)Note that we\\'ve specified input_messages_key (the key to be treated as the latest input message) and history_messages_key (the key to add historical messages to).When invoking this new runnable, we specify the corresponding chat history via a configuration parameter:with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},    config={\"configurable\": {\"session_id\": \"abc123\"}},)AIMessage(content=\\'Cosine is a trigonometric function that calculates the ratio of the adjacent side to the hypotenuse of a right triangle.\\')# Rememberswith_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What?\"},    config={\"configurable\": {\"session_id\": \"abc123\"}},)AIMessage(content=\\'Cosine is a mathematical function used to calculate the length of a side in a right triangle.\\')# New session_id --> does not remember.with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What?\"},    config={\"configurable\": {\"session_id\": \"def234\"}},)AIMessage(content=\\'I can help with math problems. What do you need assistance with?\\')The configuration parameters by which we track message histories can be customized by passing in a list of ConfigurableFieldSpec objects to the history_factory_config parameter. Below, we use two parameters: a user_id and conversation_id.from langchain_core.runnables import ConfigurableFieldSpecstore = {}def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:    if (user_id, conversation_id) not in store:        store[(user_id, conversation_id)] = ChatMessageHistory()    return store[(user_id, conversation_id)]with_message_history = RunnableWithMessageHistory(    runnable,    get_session_history,    input_messages_key=\"input\",    history_messages_key=\"history\",    history_factory_config=[        ConfigurableFieldSpec(            id=\"user_id\",            annotation=str,            name=\"User ID\",            description=\"Unique identifier for the user.\",            default=\"\",            is_shared=True,        ),        ConfigurableFieldSpec(            id=\"conversation_id\",            annotation=str,            name=\"Conversation ID\",            description=\"Unique identifier for the conversation.\",            default=\"\",            is_shared=True,        ),    ],)with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"Hello\"},    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},)Examples with runnables of different signatures\\u200bThe above runnable takes a dict as input and returns a BaseMessage. Below we show some alternatives.Messages input, dict output\\u200bfrom langchain_core.messages import HumanMessagefrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({\"output_message\": ChatOpenAI()})def get_session_history(session_id: str) -> BaseChatMessageHistory:    if session_id not in store:        store[session_id] = ChatMessageHistory()    return store[session_id]with_message_history = RunnableWithMessageHistory(    chain,    get_session_history,    output_messages_key=\"output_message\",)with_message_history.invoke(    [HumanMessage(content=\"What did Simone de Beauvoir believe about free will\")],    config={\"configurable\": {\"session_id\": \"baz\"}},){\\'output_message\\': AIMessage(content=\"Simone de Beauvoir believed in the existence of free will. She argued that individuals have the ability to make choices and determine their own actions, even in the face of social and cultural constraints. She rejected the idea that individuals are purely products of their environment or predetermined by biology or destiny. Instead, she emphasized the importance of personal responsibility and the need for individuals to actively engage in creating their own lives and defining their own existence. De Beauvoir believed that freedom and agency come from recognizing one\\'s own freedom and actively exercising it in the pursuit of personal and collective liberation.\")}with_message_history.invoke(    [HumanMessage(content=\"How did this compare to Sartre\")],    config={\"configurable\": {\"session_id\": \"baz\"}},){\\'output_message\\': AIMessage(content=\\'Simone de Beauvoir\\\\\\'s views on free will were closely aligned with those of her contemporary and partner Jean-Paul Sartre. Both de Beauvoir and Sartre were existentialist philosophers who emphasized the importance of individual freedom and the rejection of determinism. They believed that human beings have the capacity to transcend their circumstances and create their own meaning and values.\\\\n\\\\nSartre, in his famous work \"Being and Nothingness,\" argued that human beings are condemned to be free, meaning that we are burdened with the responsibility of making choices and defining ourselves in a world that lacks inherent meaning. Like de Beauvoir, Sartre believed that individuals have the ability to exercise their freedom and make choices in the face of external and internal constraints.\\\\n\\\\nWhile there may be some nuanced differences in their philosophical writings, overall, de Beauvoir and Sartre shared a similar belief in the existence of free will and the importance of individual agency in shaping one\\\\\\'s own life.\\')}Messages input, messages output\\u200bRunnableWithMessageHistory(    ChatOpenAI(),    get_session_history,)Dict with single key for all messages input, messages output\\u200bfrom operator import itemgetterRunnableWithMessageHistory(    itemgetter(\"input_messages\") | ChatOpenAI(),    get_session_history,    input_messages_key=\"input_messages\",)Persistent storage\\u200bIn many cases it is preferable to persist conversation histories. RunnableWithMessageHistory is agnostic as to how the get_session_history callable retrieves its chat message histories. See here for an example using a local filesystem. Below we demonstrate how one could use Redis. Check out the memory integrations page for implementations of chat message histories using other providers.Setup\\u200bWe\\'ll need to install Redis if it\\'s not installed already:%pip install --upgrade --quiet redisStart a local Redis Stack server if we don\\'t have an existing Redis deployment to connect to:docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latestREDIS_URL = \"redis://localhost:6379/0\"LangSmith\\u200bLangSmith is especially useful for something like message history injection, where it can be hard to otherwise understand what the inputs are to various parts of the chain.Note that LangSmith is not needed, but it is helpful.\\nIf you do want to use LangSmith, after you sign up at the link above, make sure to uncoment the below and set your environment variables to start logging traces:# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()Updating the message history implementation just requires us to define a new callable, this time returning an instance of RedisChatMessageHistory:from langchain_community.chat_message_histories import RedisChatMessageHistorydef get_message_history(session_id: str) -> RedisChatMessageHistory:    return RedisChatMessageHistory(session_id, url=REDIS_URL)with_message_history = RunnableWithMessageHistory(    runnable,    get_message_history,    input_messages_key=\"input\",    history_messages_key=\"history\",)We can invoke as before:with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},    config={\"configurable\": {\"session_id\": \"foobar\"}},)AIMessage(content=\\'Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\')with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What\\'s its inverse\"},    config={\"configurable\": {\"session_id\": \"foobar\"}},)AIMessage(content=\\'The inverse of cosine is the arccosine function, denoted as acos or cos^-1, which gives the angle corresponding to a given cosine value.\\')tipLangsmith traceLooking at the Langsmith trace for the second call, we can see that when constructing the prompt, a \"history\" variable has been injected which is a list of two messages (our first input and first output).Help us out by providing feedback on this documentation page:PreviousStreamingNextRoute logic based on inputIn-memoryExamples with runnables of different signaturesPersistent storageSetupLangSmithCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/how_to/message_history/', 'content_type': 'text/html; charset=utf-8', 'title': 'Add message history (memory) | ü¶úÔ∏èüîó LangChain', 'description': 'The RunnableWithMessageHistory lets us add message history to certain types of chains. It wraps another Runnable and manages the chat message history for it.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nCreate a runnable with the @chain decorator | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageMoreCreate a runnable with the @chain decoratorCreate a runnable with the @chain decoratorYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping in a RunnableLambda.This will have the benefit of improved observability by tracing your chain correctly. Any calls to runnables inside this function will be traced as nested childen.It will also allow you to use this as any other runnable, compose it in chain, etc.Let\\'s take a look at this in action!%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import chainfrom langchain_openai import ChatOpenAIprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain is now a runnable, meaning you will need to use invokecustom_chain.invoke(\"bears\")\\'The subject of this joke is bears.\\'If you check out your LangSmith traces, you should see a custom_chain trace in there, with the calls to OpenAI nested underneathHelp us out by providing feedback on this documentation page:PreviousInspect your runnablesNextManaging prompt sizeCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/how_to/decorator/', 'content_type': 'text/html; charset=utf-8', 'title': 'Create a runnable with the @chain decorator | ü¶úÔ∏èüîó LangChain', 'description': 'You can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping in a RunnableLambda.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nMultiple chains | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguageMoreMultiple chainsOn this pageMultiple chainsRunnables can easily be used to string together multiple Chains%pip install --upgrade --quiet  langchain langchain-openaifrom operator import itemgetterfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIprompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")prompt2 = ChatPromptTemplate.from_template(    \"what country is the city {city} in? respond in {language}\")model = ChatOpenAI()chain1 = prompt1 | model | StrOutputParser()chain2 = (    {\"city\": chain1, \"language\": itemgetter(\"language\")}    | prompt2    | model    | StrOutputParser())chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})\\'El pa√≠s donde se encuentra la ciudad de Honolulu, donde naci√≥ Barack Obama, el 44¬∫ Presidente de los Estados Unidos, es Estados Unidos. Honolulu se encuentra en la isla de Oahu, en el estado de Haw√°i.\\'from langchain_core.runnables import RunnablePassthroughprompt1 = ChatPromptTemplate.from_template(    \"generate a {attribute} color. Return the name of the color and nothing else:\")prompt2 = ChatPromptTemplate.from_template(    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\")prompt3 = ChatPromptTemplate.from_template(    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\")prompt4 = ChatPromptTemplate.from_template(    \"What is the color of {fruit} and the flag of {country}?\")model_parser = model | StrOutputParser()color_generator = (    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser})color_to_fruit = prompt2 | model_parsercolor_to_country = prompt3 | model_parserquestion_generator = (    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4)question_generator.invoke(\"warm\")ChatPromptValue(messages=[HumanMessage(content=\\'What is the color of strawberry and the flag of China?\\', additional_kwargs={}, example=False)])prompt = question_generator.invoke(\"warm\")model.invoke(prompt)AIMessage(content=\\'The color of an apple is typically red or green. The flag of China is predominantly red with a large yellow star in the upper left corner and four smaller yellow stars surrounding it.\\', additional_kwargs={}, example=False)Branching and Merging\\u200bYou may want the output of one component to be processed by 2 or more other components. RunnableParallels let you split or fork the chain so multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:     Input      / \\\\     /   \\\\ Branch1 Branch2     \\\\   /      \\\\ /      Combineplanner = (    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")    | ChatOpenAI()    | StrOutputParser()    | {\"base_response\": RunnablePassthrough()})arguments_for = (    ChatPromptTemplate.from_template(        \"List the pros or positive aspects of {base_response}\"    )    | ChatOpenAI()    | StrOutputParser())arguments_against = (    ChatPromptTemplate.from_template(        \"List the cons or negative aspects of {base_response}\"    )    | ChatOpenAI()    | StrOutputParser())final_responder = (    ChatPromptTemplate.from_messages(        [            (\"ai\", \"{original_response}\"),            (\"human\", \"Pros:\\\\n{results_1}\\\\n\\\\nCons:\\\\n{results_2}\"),            (\"system\", \"Generate a final response given the critique\"),        ]    )    | ChatOpenAI()    | StrOutputParser())chain = (    planner    | {        \"results_1\": arguments_for,        \"results_2\": arguments_against,        \"original_response\": itemgetter(\"base_response\"),    }    | final_responder)chain.invoke({\"input\": \"scrum\"})\\'While Scrum has its potential cons and challenges, many organizations have successfully embraced and implemented this project management framework to great effect. The cons mentioned above can be mitigated or overcome with proper training, support, and a commitment to continuous improvement. It is also important to note that not all cons may be applicable to every organization or project.\\\\n\\\\nFor example, while Scrum may be complex initially, with proper training and guidance, teams can quickly grasp the concepts and practices. The lack of predictability can be mitigated by implementing techniques such as velocity tracking and release planning. The limited documentation can be addressed by maintaining a balance between lightweight documentation and clear communication among team members. The dependency on team collaboration can be improved through effective communication channels and regular team-building activities.\\\\n\\\\nScrum can be scaled and adapted to larger projects by using frameworks like Scrum of Scrums or LeSS (Large Scale Scrum). Concerns about speed versus quality can be addressed by incorporating quality assurance practices, such as continuous integration and automated testing, into the Scrum process. Scope creep can be managed by having a well-defined and prioritized product backlog, and a strong product owner can be developed through training and mentorship.\\\\n\\\\nResistance to change can be overcome by providing proper education and communication to stakeholders and involving them in the decision-making process. Ultimately, the cons of Scrum can be seen as opportunities for growth and improvement, and with the right mindset and support, they can be effectively managed.\\\\n\\\\nIn conclusion, while Scrum may have its challenges and potential cons, the benefits and advantages it offers in terms of collaboration, flexibility, adaptability, transparency, and customer satisfaction make it a widely adopted and successful project management framework. With proper implementation and continuous improvement, organizations can leverage Scrum to drive innovation, efficiency, and project success.\\'Help us out by providing feedback on this documentation page:PreviousManaging prompt sizeNextü¶úüõ†Ô∏è LangSmithBranching and MergingCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/cookbook/multiple_chains/', 'content_type': 'text/html; charset=utf-8', 'title': 'Multiple chains | ü¶úÔ∏èüîó LangChain', 'description': 'Runnables can easily be used to string together multiple Chains', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nLambda: Run custom functions | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguagePrimitivesLambda: Run custom functionsOn this pageRun custom functionsYou can use arbitrary functions in the pipeline.Note that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single input and unpacks it into multiple argument.\\n%pip install --upgrade --quiet langchain langchain-openaifrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")model = ChatOpenAI()chain1 = prompt | modelchain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})AIMessage(content=\\'3 + 9 = 12\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 7, \\'prompt_tokens\\': 14, \\'total_tokens\\': 21}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_b28b39ffa8\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-bd204541-81fd-429a-ad92-dd1913af9b1c-0\\')Accepting a Runnable Config\\u200bRunnable lambdas can optionally accept a RunnableConfig, which they can use to pass callbacks, tags, and other configuration information to nested runs.from langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnableConfigimport jsondef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n```text\\\\n{input}\\\\n```\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | ChatOpenAI()        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb){\\'foo\\': \\'bar\\'}Tokens Used: 62    Prompt Tokens: 56    Completion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05StreamingYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a LCEL pipeline.The signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].These are useful for:implementing a custom output parsermodifying the output of a previous step, while preserving streaming capabilitiesHere\\'s an example of a custom output parser for comma-separated lists:from typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")model = ChatOpenAI(temperature=0.0)str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)lion, tiger, wolf, gorilla, pandastr_chain.invoke({\"animal\": \"bear\"})\\'lion, tiger, wolf, gorilla, panda\\'# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']list_chain.invoke({\"animal\": \"bear\"})[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'elephant\\']Async version\\u200bfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']await list_chain.ainvoke({\"animal\": \"bear\"})[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']Help us out by providing feedback on this documentation page:PreviousBinding: Attach runtime argsNextPassthrough: Pass through inputsAccepting a Runnable ConfigAsync versionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/primitives/functions/', 'content_type': 'text/html; charset=utf-8', 'title': 'Lambda: Run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'You can use arbitrary functions in the pipeline.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nBinding: Attach runtime args | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguagePrimitivesBinding: Attach runtime argsOn this pageBinding: Attach runtime argsSometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to pass these arguments in.Suppose we have a simple prompt + model sequence:%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAIprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Write out the following equation using algebraic symbols then solve it. Use the format\\\\n\\\\nEQUATION:...\\\\nSOLUTION:...\\\\n\\\\n\",        ),        (\"human\", \"{equation_statement}\"),    ])model = ChatOpenAI(temperature=0)runnable = (    {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser())print(runnable.invoke(\"x raised to the third plus seven equals 12\"))EQUATION: x^3 + 7 = 12SOLUTION:Subtracting 7 from both sides of the equation, we get:x^3 = 12 - 7x^3 = 5Taking the cube root of both sides, we get:x = ‚àõ5Therefore, the solution to the equation x^3 + 7 = 12 is x = ‚àõ5.and want to call the model with certain stop words:runnable = (    {\"equation_statement\": RunnablePassthrough()}    | prompt    | model.bind(stop=\"SOLUTION\")    | StrOutputParser())print(runnable.invoke(\"x raised to the third plus seven equals 12\"))EQUATION: x^3 + 7 = 12Attaching OpenAI functions\\u200bOne particularly useful application of binding is to attach OpenAI functions to a compatible OpenAI model:function = {    \"name\": \"solver\",    \"description\": \"Formulates and solves an equation\",    \"parameters\": {        \"type\": \"object\",        \"properties\": {            \"equation\": {                \"type\": \"string\",                \"description\": \"The algebraic expression of the equation\",            },            \"solution\": {                \"type\": \"string\",                \"description\": \"The solution to the equation\",            },        },        \"required\": [\"equation\", \"solution\"],    },}# Need gpt-4 to solve this one correctlyprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Write out the following equation using algebraic symbols then solve it.\",        ),        (\"human\", \"{equation_statement}\"),    ])model = ChatOpenAI(model=\"gpt-4\", temperature=0).bind(    function_call={\"name\": \"solver\"}, functions=[function])runnable = {\"equation_statement\": RunnablePassthrough()} | prompt | modelrunnable.invoke(\"x raised to the third plus seven equals 12\")AIMessage(content=\\'\\', additional_kwargs={\\'function_call\\': {\\'name\\': \\'solver\\', \\'arguments\\': \\'{\\\\n\"equation\": \"x^3 + 7 = 12\",\\\\n\"solution\": \"x = ‚àõ5\"\\\\n}\\'}}, example=False)Attaching OpenAI tools\\u200btools = [    {        \"type\": \"function\",        \"function\": {            \"name\": \"get_current_weather\",            \"description\": \"Get the current weather in a given location\",            \"parameters\": {                \"type\": \"object\",                \"properties\": {                    \"location\": {                        \"type\": \"string\",                        \"description\": \"The city and state, e.g. San Francisco, CA\",                    },                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},                },                \"required\": [\"location\"],            },        },    }]model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools)model.invoke(\"What\\'s the weather in SF, NYC and LA?\")AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_zHN0ZHwrxM7nZDdqTp6dkPko\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, {\\'id\\': \\'call_aqdMm9HBSlFW9c9rqxTa7eQv\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"New York, NY\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, {\\'id\\': \\'call_cx8E567zcLzYV2WSWVgO63f1\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}]})Help us out by providing feedback on this documentation page:PreviousParallel: Format dataNextLambda: Run custom functionsAttaching OpenAI functionsAttaching OpenAI toolsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/primitives/binding/', 'content_type': 'text/html; charset=utf-8', 'title': 'Binding: Attach runtime args | ü¶úÔ∏èüîó LangChain', 'description': 'Sometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to pass these arguments in.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nPassthrough: Pass through inputs | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguagePrimitivesPassthrough: Pass through inputsOn this pagePassing data throughRunnablePassthrough on its own allows you to pass inputs unchanged. This typically is used in conjuction with RunnableParallel to pass data through to a new key in the map. See the example below:%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_core.runnables import RunnableParallel, RunnablePassthroughrunnable = RunnableParallel(    passed=RunnablePassthrough(),    modified=lambda x: x[\"num\"] + 1,)runnable.invoke({\"num\": 1}){\\'passed\\': {\\'num\\': 1}, \\'extra\\': {\\'num\\': 1, \\'mult\\': 3}, \\'modified\\': 2}As seen above, passed key was called with RunnablePassthrough() and so it simply passed on {\\'num\\': 1}. We also set a second key in the map with modified. This uses a lambda to set a single value adding 1 to the num, which resulted in modified key with the value of 2.Retrieval Example\\u200bIn the example below, we see a use case where we use RunnablePassthrough along with RunnableParallel. from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())retrieval_chain.invoke(\"where did harrison work?\")\\'Harrison worked at Kensho.\\'Here the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key. In this case, the RunnablePassthrough allows us to pass on the user\\'s question to the prompt and model.Help us out by providing feedback on this documentation page:PreviousLambda: Run custom functionsNextAssign: Add values to stateRetrieval ExampleCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/primitives/passthrough/', 'content_type': 'text/html; charset=utf-8', 'title': 'Passthrough: Pass through inputs | ü¶úÔ∏èüîó LangChain', 'description': 'RunnablePassthrough on its own allows you to pass inputs unchanged. This typically is used in conjuction with RunnableParallel to pass data through to a new key in the map.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nSequences: Chaining runnables | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguagePrimitivesSequences: Chaining runnablesOn this pageChaining runnablesOne key advantage of the Runnable interface is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable\\'s .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing. The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or piped just like any other runnable.The pipe operator\\u200bTo show off how this works, let\\'s go through an example. We\\'ll walk through a common pattern in LangChain: using a prompt template to format input into a chat model, and finally converting the chat message output into a string with an output parser.%pip install --upgrade --quiet langchain langchain-anthropicfrom langchain_anthropic import ChatAnthropicfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")chain = prompt | model | StrOutputParser()Prompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:chain.invoke({\"topic\": \"bears\"})\"Here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear socks? \\\\nBecause they have bear feet!\\\\n\\\\nHow\\'s that? I tried to keep it light and silly. Bears can make for some fun puns and jokes. Let me know if you\\'d like to hear another one!\"Coercion\\u200bWe can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.For example, let\\'s say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.We would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a RunnableParallel, which runs all of its values in parallel and returns a dict with the results.This happens to be the same format the next prompt template expects. Here it is in action:from langchain_core.output_parsers import StrOutputParseranalysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()composed_chain.invoke({\"topic\": \"bears\"})\"That\\'s a pretty classic and well-known bear pun joke. Whether it\\'s considered funny is quite subjective, as humor is very personal. Some people may find that type of pun-based joke amusing, while others may not find it that humorous. Ultimately, the funniness of a joke is in the eye (or ear) of the beholder. If you enjoyed the joke and got a chuckle out of it, then that\\'s what matters most.\"Functions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:composed_chain_with_lambda = (    chain    | (lambda input: {\"joke\": input})    | analysis_prompt    | model    | StrOutputParser())composed_chain_with_lambda.invoke({\"topic\": \"beets\"})\\'I appreciate the effort, but I have to be honest - I didn\\\\\\'t find that joke particularly funny. Beet-themed puns can be quite hit-or-miss, and this one falls more on the \"miss\" side for me. The premise is a bit too straightforward and predictable. While I can see the logic behind it, the punchline just doesn\\\\\\'t pack much of a comedic punch. \\\\n\\\\nThat said, I do admire your willingness to explore puns and wordplay around vegetables. Cultivating a good sense of humor takes practice, and not every joke is going to land. The important thing is to keep experimenting and finding what works. Maybe try for a more unexpected or creative twist on beet-related humor next time. But thanks for sharing - I always appreciate when humans test out jokes on me, even if they don\\\\\\'t always make me laugh out loud.\\'However, keep in mind that using functions like this may interfere with operations like streaming. See this section for more information.The .pipe() method\\u200bWe could also compose the same sequence using the .pipe() method. Here\\'s what that looks like:from langchain_core.runnables import RunnableParallelcomposed_chain_with_pipe = (    RunnableParallel({\"joke\": chain})    .pipe(analysis_prompt)    .pipe(model)    .pipe(StrOutputParser()))composed_chain_with_pipe.invoke({\"topic\": \"battlestar galactica\"})\\'That\\\\\\'s a pretty good Battlestar Galactica-themed pun! I appreciated the clever play on words with \"Centurion\" and \"center on.\" It\\\\\\'s the kind of nerdy, science fiction-inspired humor that fans of the show would likely enjoy. The joke is clever and demonstrates a good understanding of the Battlestar Galactica universe. I\\\\\\'d be curious to hear any other Battlestar-related jokes you might have up your sleeve. As long as they don\\\\\\'t reproduce copyrighted material, I\\\\\\'m happy to provide my thoughts on the humor and appeal for fans of the show.\\'Help us out by providing feedback on this documentation page:PreviousPrimitivesNextParallel: Format dataThe pipe operatorCoercionThe .pipe() methodCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/primitives/sequence/', 'content_type': 'text/html; charset=utf-8', 'title': 'Sequences: Chaining runnables | ü¶úÔ∏èüîó LangChain', 'description': 'One key advantage of the Runnable interface is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable\\'s .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing. The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or piped just like any other runnable.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nAssign: Add values to state | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression LanguagePrimitivesAssign: Add values to stateOn this pageAdding values to chain stateThe RunnablePassthrough.assign(...) static method takes an input value and adds the extra arguments passed to the assign function.This is useful when additively creating a dictionary to use as input to a later step, which is a common LCEL pattern.Here\\'s an example:%pip install --upgrade --quiet langchain langchain-openai\\x1b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.You should consider upgrading via the \\'/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip\\' command.\\x1b[0m\\x1b[33m\\x1b[0mNote: you may need to restart the kernel to use updated packages.from langchain_core.runnables import RunnableParallel, RunnablePassthroughrunnable = RunnableParallel(    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),    modified=lambda x: x[\"num\"] + 1,)runnable.invoke({\"num\": 1}){\\'extra\\': {\\'num\\': 1, \\'mult\\': 3}, \\'modified\\': 2}Let\\'s break down what\\'s happening here.The input to the chain is {\"num\": 1}. This is passed into a RunnableParallel, which invokes the runnables it is passed in parallel with that input.The value under the extra key is invoked. RunnablePassthrough.assign() keeps the original keys in the input dict ({\"num\": 1}), and assigns a new key called mult. The value is lambda x: x[\"num\"] * 3), which is 3. Thus, the result is {\"num\": 1, \"mult\": 3}.{\"num\": 1, \"mult\": 3} is returned to the RunnableParallel call, and is set as the value to the key extra.At the same time, the modified key is called. The result is 2, since the lambda extracts a key called \"num\" from its input and adds one.Thus, the result is {\\'extra\\': {\\'num\\': 1, \\'mult\\': 3}, \\'modified\\': 2}.Streaming\\u200bOne nice feature of this method is that it allows values to pass through as soon as they are available. To show this off, we\\'ll use RunnablePassthrough.assign() to immediately return source docs in a retrieval chain:from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()generation_chain = prompt | model | StrOutputParser()retrieval_chain = {    \"context\": retriever,    \"question\": RunnablePassthrough(),} | RunnablePassthrough.assign(output=generation_chain)stream = retrieval_chain.stream(\"where did harrison work?\")for chunk in stream:    print(chunk){\\'question\\': \\'where did harrison work?\\'}{\\'context\\': [Document(page_content=\\'harrison worked at kensho\\')]}{\\'output\\': \\'\\'}{\\'output\\': \\'H\\'}{\\'output\\': \\'arrison\\'}{\\'output\\': \\' worked\\'}{\\'output\\': \\' at\\'}{\\'output\\': \\' Kens\\'}{\\'output\\': \\'ho\\'}{\\'output\\': \\'.\\'}{\\'output\\': \\'\\'}We can see that the first chunk contains the original \"question\" since that is immediately available. The second chunk contains \"context\" since the retriever finishes second. Finally, the output from the generation_chain streams in chunks as soon as it is available.Help us out by providing feedback on this documentation page:PreviousPassthrough: Pass through inputsNextConfigure runtime chain internalsStreamingCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/primitives/assign/', 'content_type': 'text/html; charset=utf-8', 'title': 'Assign: Add values to state | ü¶úÔ∏èüîó LangChain', 'description': 'The RunnablePassthrough.assign(...) static method takes an input value and adds the extra arguments passed to the assign function.', 'language': 'en'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL with pydantic output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader= RecursiveUrlLoader(\n",
    "    url= url,\n",
    "    max_depth=1,\n",
    "    extractor= lambda x: Soup(x, \"html.parser\").text\n",
    "\n",
    ")\n",
    "docs_pydantic= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nQuickstart | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput ParsersCustom Output ParserstypesRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel I/OOutput parsersQuickstartOn this pageQuickstartLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.And then one optional one:\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.Get started\\u200bBelow we go over the main type of output parser, the PydanticOutputParser.from langchain.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_core.pydantic_v1 import BaseModel, Field, validatorfrom langchain_openai import OpenAImodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @validator(\"setup\")    def question_ends_with_question_mark(cls, field):        if field[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return field# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')LCEL\\u200bOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.Output parsers accept a string or BaseMessage as input and can return an arbitrary type.parser.invoke(output)Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')Instead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:chain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')While all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.The SimpleJsonOutputParser for example can stream through partial outputs:from langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]While the PydanticOutputParser cannot:list(chain.stream({\"query\": \"Tell me a joke.\"}))[Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')]Help us out by providing feedback on this documentation page:PreviousOutput ParsersNextOutput ParsersGet startedLCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start', 'content_type': 'text/html; charset=utf-8', 'title': 'Quickstart | ü¶úÔ∏èüîó LangChain', 'description': 'Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.', 'language': 'en'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LCEL with self query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader= RecursiveUrlLoader(\n",
    "    url= url,\n",
    "    max_depth=1,\n",
    "    extractor= lambda x: Soup(x, \"html.parser\").text\n",
    "\n",
    ")\n",
    "docs_sq= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nSelf-querying | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversVector store-backed retrieverRetrieversMultiQueryRetrieverContextual compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn this pageSelf-queryinginfoHead to Integrations for documentation on vector stores with built-in support for self-querying.A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.Get started\\u200bFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.Note: The self-query retriever requires you to have lark package installed.%pip install --upgrade --quiet  lark langchain-chromafrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())Creating our self-querying retriever\\u200bNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.from langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)Testing it out\\u200bAnd now we can actually try using our retriever!# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Filter k\\u200bWe can also use the self query retriever to specify k: the number of documents to fetch.We can do this by passing enable_limit=True to the constructor.retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Constructing from scratch with LCEL\\u200bTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.First, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.from langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserLet\\'s look at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:```json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparatorattr (string):  name of attribute to apply the comparison toval (string): is the comparison valueA logical operation statement takes the form op(statement1, statement2, ...):op (and | or | not): logical operatorstatement1, statement2, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format YYYY-MM-DD when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}<< Example 2. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs that were not published on SpotifyStructured Request:{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\nData Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}User Query:\\ndummy questionStructured Request:And what our full chain produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })StructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)The query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.from langchain.retrievers.self_query.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)retriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Help us out by providing feedback on this documentation page:PreviousParent Document RetrieverNextTime-weighted vector store retrieverGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/', 'content_type': 'text/html; charset=utf-8', 'title': 'Self-querying | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on vector stores with built-in support for self-querying.', 'language': 'en'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append all the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts= [d.page_content for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'LangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageLangChain Expression Language '\n",
      " '(LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily '\n",
      " 'compose chains together.\\n'\n",
      " 'LCEL was designed from day 1 to support putting prototypes in production, '\n",
      " 'with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most '\n",
      " 'complex chains (we‚Äôve seen folks successfully run LCEL chains with 100s of '\n",
      " 'steps in production). To highlight a few of the reasons you might want to '\n",
      " 'use LCEL:First-class streaming support\\n'\n",
      " 'When you build your chains with LCEL you get the best possible '\n",
      " 'time-to-first-token (time elapsed until the first chunk of output comes '\n",
      " 'out). For some chains this means eg. we stream tokens straight from an LLM '\n",
      " 'to a streaming output parser, and you get back parsed, incremental chunks of '\n",
      " 'output at the same rate as the LLM provider outputs the raw tokens.Async '\n",
      " 'support\\n'\n",
      " 'Any chain built with LCEL can be called both with the synchronous API (eg. '\n",
      " 'in your Jupyter notebook while prototyping) as well as with the asynchronous '\n",
      " 'API (eg. in a LangServe server). This enables using the same code for '\n",
      " 'prototypes and in production, with great performance, and the ability to '\n",
      " 'handle many concurrent requests in the same server.Optimized parallel '\n",
      " 'execution\\n'\n",
      " 'Whenever your LCEL chains have steps that can be executed in parallel (eg if '\n",
      " 'you fetch documents from multiple retrievers) we automatically do it, both '\n",
      " 'in the sync and the async interfaces, for the smallest possible '\n",
      " 'latency.Retries and fallbacks\\n'\n",
      " 'Configure retries and fallbacks for any part of your LCEL chain. This is a '\n",
      " 'great way to make your chains more reliable at scale. We‚Äôre currently '\n",
      " 'working on adding streaming support for retries/fallbacks, so you can get '\n",
      " 'the added reliability without any latency cost.Access intermediate results\\n'\n",
      " 'For more complex chains it‚Äôs often very useful to access the results of '\n",
      " 'intermediate steps even before the final output is produced. This can be '\n",
      " 'used to let end-users know something is happening, or even just to debug '\n",
      " 'your chain. You can stream intermediate results, and it‚Äôs available on every '\n",
      " 'LangServe server.Input and output schemas\\n'\n",
      " 'Input and output schemas give every LCEL chain Pydantic and JSONSchema '\n",
      " 'schemas inferred from the structure of your chain. This can be used for '\n",
      " 'validation of inputs and outputs, and is an integral part of '\n",
      " 'LangServe.Seamless LangSmith tracing\\n'\n",
      " 'As your chains get more and more complex, it becomes increasingly important '\n",
      " 'to understand what exactly is happening at every step.\\n'\n",
      " 'With LCEL, all steps are automatically logged to LangSmith for maximum '\n",
      " 'observability and debuggability.Seamless LangServe deployment\\n'\n",
      " 'Any chain created with LCEL can be easily deployed using LangServe.Help us '\n",
      " 'out by providing feedback on this documentation page:PreviousWeb '\n",
      " 'scrapingNextGet '\n",
      " 'startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Parallel: Format data | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesParallel: Format dataOn this '\n",
      " 'pageFormatting inputs & outputThe RunnableParallel primitive is essentially '\n",
      " 'a dict whose values are runnables (or things that can be coerced to '\n",
      " 'runnables, like functions). It runs all of its values in parallel, and each '\n",
      " 'value is called with the overall input of the RunnableParallel. The final '\n",
      " 'return value is a dict with the results of each value under its appropriate '\n",
      " 'key.It is useful for parallelizing operations, but can also be useful for '\n",
      " 'manipulating the output of one Runnable to match the input format of the '\n",
      " 'next Runnable in a sequence.Here the input to prompt is expected to be a map '\n",
      " 'with keys \"context\" and \"question\". The user input is just the question. So '\n",
      " 'we need to get the context using our retriever and passthrough the user '\n",
      " 'input under the \"question\" key.%pip install --upgrade --quiet  langchain '\n",
      " 'langchain-openaifrom langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)model = '\n",
      " 'ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": '\n",
      " 'RunnablePassthrough()}    | prompt    | model    | '\n",
      " 'StrOutputParser())retrieval_chain.invoke(\"where did harrison '\n",
      " 'work?\")\\'Harrison worked at Kensho.\\'::: {.callout-tip}\\n'\n",
      " \"Note that when composing a RunnableParallel with another Runnable we don't \"\n",
      " 'even need to wrap our dictionary in the RunnableParallel class ‚Äî\\xa0the type '\n",
      " 'conversion is handled for us. In the context of a chain, these are '\n",
      " 'equivalent:\\n'\n",
      " ':::{\"context\": retriever, \"question\": '\n",
      " 'RunnablePassthrough()}RunnableParallel({\"context\": retriever, \"question\": '\n",
      " 'RunnablePassthrough()})RunnableParallel(context=retriever, '\n",
      " 'question=RunnablePassthrough())Using itemgetter as shorthand\\u200bNote that '\n",
      " \"you can use Python's itemgetter as shorthand to extract data from the map \"\n",
      " 'when combining with RunnableParallel. You can find more information about '\n",
      " 'itemgetter in the Python Documentation. In the example below, we use '\n",
      " 'itemgetter to extract specific keys from the map:from operator import '\n",
      " 'itemgetterfrom langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}Answer in the following '\n",
      " 'language: {language}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)chain = (    {        \"context\": '\n",
      " 'itemgetter(\"question\") | retriever,        \"question\": '\n",
      " 'itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | '\n",
      " 'prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where '\n",
      " 'did harrison work\", \"language\": \"italian\"})\\'Harrison ha lavorato a '\n",
      " \"Kensho.'Parallelize steps\\u200bRunnableParallel (aka. RunnableMap) makes it \"\n",
      " 'easy to execute multiple Runnables in parallel, and to return the output of '\n",
      " 'these Runnables as a map.from langchain_core.prompts import '\n",
      " 'ChatPromptTemplatefrom langchain_core.runnables import RunnableParallelfrom '\n",
      " 'langchain_openai import ChatOpenAImodel = ChatOpenAI()joke_chain = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | '\n",
      " 'modelpoem_chain = (    ChatPromptTemplate.from_template(\"write a 2-line poem '\n",
      " 'about {topic}\") | model)map_chain = RunnableParallel(joke=joke_chain, '\n",
      " 'poem=poem_chain)map_chain.invoke({\"topic\": \"bear\"}){\\'joke\\': '\n",
      " 'AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear '\n",
      " 'feet!\"), \\'poem\\': AIMessage(content=\"In the wild\\'s embrace, bear roams '\n",
      " 'free,\\\\nStrength and grace, a majestic '\n",
      " 'decree.\")}Parallelism\\u200bRunnableParallel are also useful for running '\n",
      " 'independent processes in parallel, since each Runnable in the map is '\n",
      " 'executed in parallel. For example, we can see our earlier joke_chain, '\n",
      " 'poem_chain and map_chain all have about the same runtime, even though '\n",
      " 'map_chain executes both of the other two.%%timeitjoke_chain.invoke({\"topic\": '\n",
      " '\"bear\"})958 ms ¬± 402 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop '\n",
      " 'each)%%timeitpoem_chain.invoke({\"topic\": \"bear\"})1.22 s ¬± 508 ms per loop '\n",
      " '(mean ¬± std. dev. of 7 runs, 1 loop each)%%timeitmap_chain.invoke({\"topic\": '\n",
      " '\"bear\"})1.15 s ¬± 119 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop '\n",
      " 'each)Help us out by providing feedback on this documentation '\n",
      " 'page:PreviousSequences: Chaining runnablesNextBinding: Attach runtime '\n",
      " 'argsUsing itemgetter as shorthandParallelize '\n",
      " 'stepsParallelismCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Configure runtime chain internals | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesConfigure runtime chain '\n",
      " 'internalsOn this pageConfigure chain internals at runtimeOftentimes you may '\n",
      " 'want to experiment with, or even expose to the end user, multiple different '\n",
      " 'ways of doing things.\\n'\n",
      " 'In order to make this experience as easy as possible, we have defined two '\n",
      " 'methods.First, a configurable_fields method.\\n'\n",
      " 'This lets you configure particular fields of a runnable.Second, a '\n",
      " 'configurable_alternatives method.\\n'\n",
      " 'With this method, you can list out alternatives for any particular runnable '\n",
      " 'that can be set during runtime.Configuration Fields\\u200bWith LLMs\\u200bWith '\n",
      " 'LLMs we can configure things like temperature%pip install --upgrade --quiet  '\n",
      " 'langchain langchain-openaifrom langchain_core.prompts import '\n",
      " 'PromptTemplatefrom langchain_core.runnables import ConfigurableFieldfrom '\n",
      " 'langchain_openai import ChatOpenAImodel = '\n",
      " 'ChatOpenAI(temperature=0).configurable_fields(    '\n",
      " 'temperature=ConfigurableField(        id=\"llm_temperature\",        name=\"LLM '\n",
      " 'Temperature\",        description=\"The temperature of the LLM\",    '\n",
      " '))model.invoke(\"pick a random '\n",
      " 'number\")AIMessage(content=\\'7\\')model.with_config(configurable={\"llm_temperature\": '\n",
      " '0.9}).invoke(\"pick a random number\")AIMessage(content=\\'34\\')We can also do '\n",
      " 'this when its used as part of a chainprompt = '\n",
      " 'PromptTemplate.from_template(\"Pick a random number above {x}\")chain = prompt '\n",
      " '| modelchain.invoke({\"x\": '\n",
      " '0})AIMessage(content=\\'57\\')chain.with_config(configurable={\"llm_temperature\": '\n",
      " '0.9}).invoke({\"x\": 0})AIMessage(content=\\'6\\')With HubRunnables\\u200bThis is '\n",
      " 'useful to allow for switching of promptsfrom langchain.runnables.hub import '\n",
      " 'HubRunnableprompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(    '\n",
      " 'owner_repo_commit=ConfigurableField(        id=\"hub_commit\",        '\n",
      " 'name=\"Hub Commit\",        description=\"The Hub commit to pull from\",    '\n",
      " '))prompt.invoke({\"question\": \"foo\", \"context\": '\n",
      " '\"bar\"})ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant '\n",
      " 'for question-answering tasks. Use the following pieces of retrieved context '\n",
      " \"to answer the question. If you don't know the answer, just say that you \"\n",
      " \"don't know. Use three sentences maximum and keep the answer \"\n",
      " 'concise.\\\\nQuestion: foo \\\\nContext: bar '\n",
      " '\\\\nAnswer:\")])prompt.with_config(configurable={\"hub_commit\": '\n",
      " '\"rlm/rag-prompt-llama\"}).invoke(    {\"question\": \"foo\", \"context\": '\n",
      " '\"bar\"})ChatPromptValue(messages=[HumanMessage(content=\"[INST]<<SYS>> You are '\n",
      " 'an assistant for question-answering tasks. Use the following pieces of '\n",
      " \"retrieved context to answer the question. If you don't know the answer, just \"\n",
      " \"say that you don't know. Use three sentences maximum and keep the answer \"\n",
      " 'concise.<</SYS>> \\\\nQuestion: foo \\\\nContext: bar \\\\nAnswer: '\n",
      " '[/INST]\")])Configurable Alternatives\\u200bWith LLMs\\u200bLet\\'s take a look '\n",
      " 'at doing this with LLMsfrom langchain_community.chat_models import '\n",
      " 'ChatAnthropicfrom langchain_core.prompts import PromptTemplatefrom '\n",
      " 'langchain_core.runnables import ConfigurableFieldfrom langchain_openai '\n",
      " 'import ChatOpenAIllm = '\n",
      " 'ChatAnthropic(temperature=0).configurable_alternatives(    # This gives this '\n",
      " 'field an id    # When configuring the end runnable, we can then use this id '\n",
      " 'to configure this field    ConfigurableField(id=\"llm\"),    # This sets a '\n",
      " 'default_key.    # If we specify this key, the default LLM (ChatAnthropic '\n",
      " 'initialized above) will be used    default_key=\"anthropic\",    # This adds a '\n",
      " 'new option, with name `openai` that is equal to `ChatOpenAI()`    '\n",
      " 'openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is '\n",
      " 'equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # '\n",
      " 'You can add more configuration options here)prompt = '\n",
      " 'PromptTemplate.from_template(\"Tell me a joke about {topic}\")chain = prompt | '\n",
      " 'llm# By default it will call Anthropicchain.invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do '\n",
      " 'you call a bear with no teeth?\\\\nA gummy bear!\")# We can use '\n",
      " '`.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to '\n",
      " 'usechain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")# If we '\n",
      " 'use the `default_key` then it uses the '\n",
      " 'defaultchain.with_config(configurable={\"llm\": \"anthropic\"}).invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do '\n",
      " 'you call a bear with no teeth?\\\\nA gummy bear!\")With Prompts\\u200bWe can do '\n",
      " 'a similar thing, but alternate between promptsllm = '\n",
      " 'ChatAnthropic(temperature=0)prompt = PromptTemplate.from_template(    \"Tell '\n",
      " 'me a joke about {topic}\").configurable_alternatives(    # This gives this '\n",
      " 'field an id    # When configuring the end runnable, we can then use this id '\n",
      " 'to configure this field    ConfigurableField(id=\"prompt\"),    # This sets a '\n",
      " 'default_key.    # If we specify this key, the default LLM (ChatAnthropic '\n",
      " 'initialized above) will be used    default_key=\"joke\",    # This adds a new '\n",
      " 'option, with name `poem`    poem=PromptTemplate.from_template(\"Write a short '\n",
      " 'poem about {topic}\"),    # You can add more configuration options here)chain '\n",
      " '= prompt | llm# By default it will write a jokechain.invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do '\n",
      " 'you call a bear with no teeth?\\\\nA gummy bear!\")# We can configure it write '\n",
      " 'a poemchain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\\' Here is a short poem about bears:\\\\n\\\\nThe '\n",
      " 'bears awaken from their sleep\\\\nAnd lumber out into the deep\\\\nForests '\n",
      " 'filled with trees so tall\\\\nForaging for food before nightfall \\\\nTheir '\n",
      " 'furry coats and claws so sharp\\\\nSniffing for berries and fish to '\n",
      " 'nab\\\\nLumbering about without a care\\\\nThe mighty grizzly and black '\n",
      " 'bear\\\\nProud creatures, wild and free\\\\nRuling their domain '\n",
      " 'majestically\\\\nWandering the woods they call their own\\\\nBefore returning to '\n",
      " \"their dens alone')With Prompts and LLMs\\u200bWe can also have multiple \"\n",
      " 'things configurable!\\n'\n",
      " \"Here's an example doing that with both prompts and LLMs.llm = \"\n",
      " 'ChatAnthropic(temperature=0).configurable_alternatives(    # This gives this '\n",
      " 'field an id    # When configuring the end runnable, we can then use this id '\n",
      " 'to configure this field    ConfigurableField(id=\"llm\"),    # This sets a '\n",
      " 'default_key.    # If we specify this key, the default LLM (ChatAnthropic '\n",
      " 'initialized above) will be used    default_key=\"anthropic\",    # This adds a '\n",
      " 'new option, with name `openai` that is equal to `ChatOpenAI()`    '\n",
      " 'openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is '\n",
      " 'equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # '\n",
      " 'You can add more configuration options here)prompt = '\n",
      " 'PromptTemplate.from_template(    \"Tell me a joke about '\n",
      " '{topic}\").configurable_alternatives(    # This gives this field an id    # '\n",
      " 'When configuring the end runnable, we can then use this id to configure this '\n",
      " 'field    ConfigurableField(id=\"prompt\"),    # This sets a default_key.    # '\n",
      " 'If we specify this key, the default LLM (ChatAnthropic initialized above) '\n",
      " 'will be used    default_key=\"joke\",    # This adds a new option, with name '\n",
      " '`poem`    poem=PromptTemplate.from_template(\"Write a short poem about '\n",
      " '{topic}\"),    # You can add more configuration options here)chain = prompt | '\n",
      " 'llm# We can configure it write a poem with '\n",
      " 'OpenAIchain.with_config(configurable={\"prompt\": \"poem\", \"llm\": '\n",
      " '\"openai\"}).invoke(    {\"topic\": \"bears\"})AIMessage(content=\"In the forest, '\n",
      " 'where tall trees sway,\\\\nA creature roams, both fierce and gray.\\\\nWith '\n",
      " 'mighty paws and piercing eyes,\\\\nThe bear, a symbol of strength, '\n",
      " 'defies.\\\\n\\\\nThrough snow-kissed mountains, it does roam,\\\\nA guardian of '\n",
      " 'its woodland home.\\\\nWith fur so thick, a shield of might,\\\\nIt braves the '\n",
      " 'coldest winter night.\\\\n\\\\nA gentle giant, yet wild and free,\\\\nThe bear '\n",
      " 'commands respect, you see.\\\\nWith every step, it leaves a trace,\\\\nOf '\n",
      " \"untamed power and ancient grace.\\\\n\\\\nFrom honeyed feast to salmon's \"\n",
      " \"leap,\\\\nIt takes its place, in nature's keep.\\\\nA symbol of untamed \"\n",
      " 'delight,\\\\nThe bear, a wonder, day and night.\\\\n\\\\nSo let us honor this '\n",
      " 'noble beast,\\\\nIn forests where its soul finds peace.\\\\nFor in its presence, '\n",
      " 'we come to know,\\\\nThe untamed spirit that in us also flows.\")# We can '\n",
      " 'always just configure only one if we '\n",
      " 'wantchain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")Saving '\n",
      " 'configurations\\u200bWe can also easily save configured chains as their own '\n",
      " 'objectsopenai_joke = chain.with_config(configurable={\"llm\": '\n",
      " '\"openai\"})openai_joke.invoke({\"topic\": \"bears\"})AIMessage(content=\"Why '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")Help us out by '\n",
      " 'providing feedback on this documentation page:PreviousAssign: Add values to '\n",
      " 'stateNextPrimitivesConfiguration FieldsWith LLMsWith '\n",
      " 'HubRunnablesConfigurable AlternativesWith LLMsWith PromptsWith Prompts and '\n",
      " 'LLMsSaving '\n",
      " 'configurationsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Primitives | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesPrimitivesIn addition to '\n",
      " 'various components that are usable with LCEL, LangChain also includes '\n",
      " 'various primitives\\n'\n",
      " 'that help pass around and format data, bind arguments, invoke custom logic, '\n",
      " 'and more.This section goes into greater depth on where and how some of these '\n",
      " 'components are useful.üìÑÔ∏è Sequences: Chaining runnablesOne key advantage of '\n",
      " 'the Runnable interface is that any two runnables can be \"chained\" together '\n",
      " \"into sequences. The output of the previous runnable's .invoke() call is \"\n",
      " 'passed as input to the next runnable. This can be done using the pipe '\n",
      " 'operator (|), or the more explicit .pipe() method, which does the same '\n",
      " 'thing. The resulting RunnableSequence is itself a runnable, which means it '\n",
      " 'can be invoked, streamed, or piped just like any other runnable.üìÑÔ∏è Parallel: '\n",
      " 'Format dataThe RunnableParallel primitive is essentially a dict whose values '\n",
      " 'are runnables (or things that can be coerced to runnables, like functions). '\n",
      " 'It runs all of its values in parallel, and each value is called with the '\n",
      " 'overall input of the RunnableParallel. The final return value is a dict with '\n",
      " 'the results of each value under its appropriate key.üìÑÔ∏è Binding: Attach '\n",
      " 'runtime argsSometimes we want to invoke a Runnable within a Runnable '\n",
      " 'sequence with constant arguments that are not part of the output of the '\n",
      " 'preceding Runnable in the sequence, and which are not part of the user '\n",
      " 'input. We can use Runnable.bind() to pass these arguments in.üìÑÔ∏è Lambda: Run '\n",
      " 'custom functionsYou can use arbitrary functions in the pipeline.üìÑÔ∏è '\n",
      " 'Passthrough: Pass through inputsRunnablePassthrough on its own allows you to '\n",
      " 'pass inputs unchanged. This typically is used in conjuction with '\n",
      " 'RunnableParallel to pass data through to a new key in the map.üìÑÔ∏è Assign: Add '\n",
      " 'values to stateThe RunnablePassthrough.assign(...) static method takes an '\n",
      " 'input value and adds the extra arguments passed to the assign function.üìÑÔ∏è '\n",
      " 'Configure runtime chain internalsOftentimes you may want to experiment with, '\n",
      " 'or even expose to the end user, multiple different ways of doing things.Help '\n",
      " 'us out by providing feedback on this documentation page:PreviousRunnable '\n",
      " 'interfaceNextSequences: Chaining '\n",
      " 'runnablesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Streaming | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageStreamingOn this pageStreaming With '\n",
      " 'LangChainStreaming is critical in making applications based on LLMs feel '\n",
      " 'responsive to end-users.Important LangChain primitives like LLMs, parsers, '\n",
      " 'prompts, retrievers, and agents implement the LangChain Runnable '\n",
      " 'Interface.This interface provides two general approaches to stream '\n",
      " 'content:sync stream and async astream: a default implementation of streaming '\n",
      " 'that streams the final output from the chain.async astream_events and async '\n",
      " 'astream_log: these provide a way to stream both intermediate steps and final '\n",
      " \"output from the chain.Let's take a look at both approaches, and try to \"\n",
      " 'understand how to use them. ü•∑Using Stream\\u200bAll Runnable objects '\n",
      " 'implement a sync method called stream and an async variant called astream. '\n",
      " 'These methods are designed to stream the final output in chunks, yielding '\n",
      " 'each chunk as soon as it is available.Streaming is only possible if all '\n",
      " 'steps in the program know how to process an input stream; i.e., process an '\n",
      " 'input chunk one at a time, and yield a corresponding output chunk.The '\n",
      " 'complexity of this processing can vary, from straightforward tasks like '\n",
      " 'emitting tokens produced by an LLM, to more challenging ones like streaming '\n",
      " 'parts of JSON results before the entire JSON is complete.The best place to '\n",
      " 'start exploring streaming is with the single most important components in '\n",
      " 'LLMs apps-- the LLMs themselves!LLMs and Chat Models\\u200bLarge language '\n",
      " 'models and their chat variants are the primary bottleneck in LLM based apps. '\n",
      " 'üôäLarge language models can take several seconds to generate a complete '\n",
      " 'response to a query. This is far slower than the ~200-300 ms threshold at '\n",
      " 'which an application feels responsive to an end user.The key strategy to '\n",
      " 'make the application feel more responsive is to show intermediate progress; '\n",
      " 'viz., to stream the output from the model token by token.We will show '\n",
      " 'examples of streaming using the chat model from Anthropic. To use the model, '\n",
      " 'you will need to install the langchain-anthropic package. You can do this '\n",
      " 'with the following command:pip install -qU langchain-anthropic# Showing the '\n",
      " 'example using anthropic, but you can use# your favorite chat model!from '\n",
      " 'langchain_anthropic import ChatAnthropicmodel = ChatAnthropic()chunks = '\n",
      " '[]async for chunk in model.astream(\"hello. tell me something about '\n",
      " 'yourself\"):    chunks.append(chunk)    print(chunk.content, end=\"|\", '\n",
      " \"flush=True) Hello|!| My| name| is| Claude|.| I|'m| an| AI| assistant| \"\n",
      " 'created| by| An|throp|ic| to| be| helpful|,| harmless|,| and| '\n",
      " \"honest|.||Let's inspect one of the chunkschunks[0]AIMessageChunk(content=' \"\n",
      " \"Hello')We got back something called an AIMessageChunk. This chunk represents \"\n",
      " 'a part of an AIMessage.Message chunks are additive by design -- one can '\n",
      " 'simply add them up to get the state of the response so far!chunks[0] + '\n",
      " \"chunks[1] + chunks[2] + chunks[3] + chunks[4]AIMessageChunk(content=' Hello! \"\n",
      " \"My name is')Chains\\u200bVirtually all LLM applications involve more steps \"\n",
      " \"than just a call to a language model.Let's build a simple chain using \"\n",
      " 'LangChain Expression Language (LCEL) that combines a prompt, model and a '\n",
      " 'parser and verify that streaming works.We will use StrOutputParser to parse '\n",
      " 'the output from the model. This is a simple parser that extracts the content '\n",
      " 'field from an AIMessageChunk, giving us the token returned by the '\n",
      " 'model.tipLCEL is a declarative way to specify a \"program\" by chainining '\n",
      " 'together different LangChain primitives. Chains created using LCEL benefit '\n",
      " 'from an automatic implementation of stream and astream allowing streaming of '\n",
      " 'the final output. In fact, chains created with LCEL implement the entire '\n",
      " 'standard Runnable interface.from langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = '\n",
      " 'StrOutputParser()chain = prompt | model | parserasync for chunk in '\n",
      " 'chain.astream({\"topic\": \"parrot\"}):    print(chunk, end=\"|\", flush=True) '\n",
      " \"Here|'s| a| silly| joke| about| a| par|rot|:|What| kind| of| teacher| gives| \"\n",
      " 'good| advice|?| An| ap|-|parent| (|app|arent|)| one|!||You might notice '\n",
      " \"above that parser actually doesn't block the streaming output from the \"\n",
      " 'model, and instead processes each chunk individually. Many of the LCEL '\n",
      " 'primitives also support this kind of transform-style passthrough streaming, '\n",
      " 'which can be very convenient when constructing apps.Certain runnables, like '\n",
      " 'prompt templates and chat models, cannot process individual chunks and '\n",
      " 'instead aggregate all previous steps. This will interrupt the streaming '\n",
      " 'process. Custom functions can be designed to return generators, whichnoteIf '\n",
      " \"the above functionality is not relevant to what you're building, you do not \"\n",
      " 'have to use the LangChain Expression Language to use LangChain and can '\n",
      " 'instead rely on a standard imperative programming approach by\\n'\n",
      " 'caling invoke, batch or stream on each component individually, assigning the '\n",
      " 'results to variables and then using them downstream as you see fit.If that '\n",
      " \"works for your needs, then that's fine by us üëå!Working with Input \"\n",
      " 'Streams\\u200bWhat if you wanted to stream JSON from the output as it was '\n",
      " 'being generated?If you were to rely on json.loads to parse the partial json, '\n",
      " \"the parsing would fail as the partial json wouldn't be valid json.You'd \"\n",
      " \"likely be at a complete loss of what to do and claim that it wasn't possible \"\n",
      " 'to stream JSON.Well, turns out there is a way to do it -- the parser needs '\n",
      " 'to operate on the input stream, and attempt to \"auto-complete\" the partial '\n",
      " \"json into a valid state.Let's see such a parser in action to understand what \"\n",
      " 'this means.from langchain_core.output_parsers import JsonOutputParserchain = '\n",
      " '(    model | JsonOutputParser())  # Due to a bug in older versions of '\n",
      " 'Langchain, JsonOutputParser did not stream results from some modelsasync for '\n",
      " \"text in chain.astream(    'output a list of the countries france, spain and \"\n",
      " 'japan and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " \"key `name` and `population`'):    print(text, flush=True){}{'countries': \"\n",
      " \"[]}{'countries': [{}]}{'countries': [{'name': ''}]}{'countries': [{'name': \"\n",
      " \"'France'}]}{'countries': [{'name': 'France', 'population': \"\n",
      " \"67}]}{'countries': [{'name': 'France', 'population': 6739}]}{'countries': \"\n",
      " \"[{'name': 'France', 'population': 673915}]}{'countries': [{'name': 'France', \"\n",
      " \"'population': 67391582}]}{'countries': [{'name': 'France', 'population': \"\n",
      " \"67391582}, {}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': ''}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Sp'}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Spain'}]}{'countries': [{'name': 'France', 'population': \"\n",
      " \"67391582}, {'name': 'Spain', 'population': 46}]}{'countries': [{'name': \"\n",
      " \"'France', 'population': 67391582}, {'name': 'Spain', 'population': \"\n",
      " \"4675}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': \"\n",
      " \"'Spain', 'population': 467547}]}{'countries': [{'name': 'France', \"\n",
      " \"'population': 67391582}, {'name': 'Spain', 'population': \"\n",
      " \"46754778}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Spain', 'population': 46754778}, {}]}{'countries': [{'name': \"\n",
      " \"'France', 'population': 67391582}, {'name': 'Spain', 'population': \"\n",
      " \"46754778}, {'name': ''}]}{'countries': [{'name': 'France', 'population': \"\n",
      " \"67391582}, {'name': 'Spain', 'population': 46754778}, {'name': \"\n",
      " \"'Japan'}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': \"\n",
      " \"12}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': \"\n",
      " \"'Spain', 'population': 46754778}, {'name': 'Japan', 'population': \"\n",
      " \"12647}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': \"\n",
      " \"'Spain', 'population': 46754778}, {'name': 'Japan', 'population': \"\n",
      " \"1264764}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': \"\n",
      " \"126476461}]}Now, let's break streaming. We'll use the previous example and \"\n",
      " 'append an extraction function at the end that extracts the country names '\n",
      " 'from the finalized JSON.dangerAny steps in the chain that operate on '\n",
      " 'finalized inputs rather than on input streams can break streaming '\n",
      " 'functionality via stream or astream.tipLater, we will discuss the '\n",
      " 'astream_events API which streams results from intermediate steps. This API '\n",
      " 'will stream results from intermediate steps even if the chain contains steps '\n",
      " 'that only operate on finalized inputs.from langchain_core.output_parsers '\n",
      " 'import (    JsonOutputParser,)# A function that operates on finalized '\n",
      " 'inputs# rather than on an input_streamdef _extract_country_names(inputs):    '\n",
      " '\"\"\"A function that does not operates on input streams and breaks '\n",
      " 'streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if '\n",
      " '\"countries\" not in inputs:        return \"\"    countries = '\n",
      " 'inputs[\"countries\"]    if not isinstance(countries, list):        return '\n",
      " '\"\"    country_names = [        country.get(\"name\") for country in countries '\n",
      " 'if isinstance(country, dict)    ]    return country_nameschain = model | '\n",
      " 'JsonOutputParser() | _extract_country_namesasync for text in '\n",
      " \"chain.astream(    'output a list of the countries france, spain and japan \"\n",
      " 'and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " 'key `name` and `population`\\'):    print(text, end=\"|\", '\n",
      " \"flush=True)['France', 'Spain', 'Japan']|Generator Functions\\u200bLe'ts fix \"\n",
      " 'the streaming using a generator function that can operate on the input '\n",
      " 'stream.tipA generator function (a function that uses yield) allows writing '\n",
      " 'code that operators on input streamsfrom langchain_core.output_parsers '\n",
      " 'import JsonOutputParserasync def '\n",
      " '_extract_country_names_streaming(input_stream):    \"\"\"A function that '\n",
      " 'operates on input streams.\"\"\"    country_names_so_far = set()    async for '\n",
      " 'input in input_stream:        if not isinstance(input, dict):            '\n",
      " 'continue        if \"countries\" not in input:            continue        '\n",
      " 'countries = input[\"countries\"]        if not isinstance(countries, '\n",
      " 'list):            continue        for country in countries:            name '\n",
      " '= country.get(\"name\")            if not name:                '\n",
      " 'continue            if name not in country_names_so_far:                '\n",
      " 'yield name                country_names_so_far.add(name)chain = model | '\n",
      " 'JsonOutputParser() | _extract_country_names_streamingasync for text in '\n",
      " \"chain.astream(    'output a list of the countries france, spain and japan \"\n",
      " 'and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " 'key `name` and `population`\\'):    print(text, end=\"|\", '\n",
      " 'flush=True)France|Sp|Spain|Japan|noteBecause the code above is relying on '\n",
      " 'JSON auto-completion, you may see partial names of countries (e.g., Sp and '\n",
      " \"Spain), which is not what one would want for an extraction result!We're \"\n",
      " 'focusing on streaming concepts, not necessarily the results of the '\n",
      " 'chains.Non-streaming components\\u200bSome built-in components like '\n",
      " 'Retrievers do not offer any streaming. What happens if we try to stream '\n",
      " 'them? ü§®from langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the '\n",
      " 'following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\", \"harrison likes spicy food\"],    '\n",
      " 'embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()chunks = '\n",
      " '[chunk for chunk in retriever.stream(\"where did harrison '\n",
      " 'work?\")]chunks[[Document(page_content=\\'harrison worked at kensho\\'),  '\n",
      " \"Document(page_content='harrison likes spicy food')]]Stream just yielded the \"\n",
      " 'final result from that component.This is OK \\U0001f979! Not all components '\n",
      " 'have to implement streaming -- in some cases streaming is either '\n",
      " \"unnecessary, difficult or just doesn't make sense.tipAn LCEL chain \"\n",
      " 'constructed using non-streaming components, will still be able to stream in '\n",
      " 'a lot of cases, with streaming of partial output starting after the last '\n",
      " 'non-streaming step in the chain.retrieval_chain = (    {        \"context\": '\n",
      " 'retriever.with_config(run_name=\"Docs\"),        \"question\": '\n",
      " 'RunnablePassthrough(),    }    | prompt    | model    | '\n",
      " 'StrOutputParser())for chunk in retrieval_chain.stream(    \"Where did '\n",
      " 'harrison work? \" \"Write 3 made up sentences about this place.\"):    '\n",
      " 'print(chunk, end=\"|\", flush=True) Based| on| the| given| context|,| the| '\n",
      " 'only| information| provided| about| where| Harrison| worked| is| that| he| '\n",
      " 'worked| at| Ken|sh|o|.| Since| there| are| no| other| details| provided| '\n",
      " 'about| Ken|sh|o|,| I| do| not| have| enough| information| to| write| 3| '\n",
      " 'additional| made| up| sentences| about| this| place|.| I| can| only| state| '\n",
      " \"that| Harrison| worked| at| Ken|sh|o|.||Now that we've seen how stream and \"\n",
      " \"astream work, let's venture into the world of streaming events. üèûÔ∏èUsing \"\n",
      " 'Stream Events\\u200bEvent Streaming is a beta API. This API may change a bit '\n",
      " 'based on feedback.noteIntroduced in langchain-core 0.1.14.import '\n",
      " \"langchain_corelangchain_core.__version__'0.1.18'For the astream_events API \"\n",
      " 'to work properly:Use async throughout the code to the extent possible (e.g., '\n",
      " 'async tools etc)Propagate callbacks if defining custom functions / '\n",
      " 'runnablesWhenever using runnables without LCEL, make sure to call .astream() '\n",
      " 'on LLMs rather than .ainvoke to force the LLM to stream tokens.Let us know '\n",
      " \"if anything doesn't work as expected! :)Event Reference\\u200bBelow is a \"\n",
      " 'reference table that shows some events that might be emitted by the various '\n",
      " 'Runnable objects.noteWhen streaming is implemented properly, the inputs to a '\n",
      " 'runnable will not be known until after the input stream has been entirely '\n",
      " 'consumed. This means that inputs will often be included only for end events '\n",
      " 'and rather than for start '\n",
      " 'events.eventnamechunkinputoutputon_chat_model_start[model name]{\"messages\": '\n",
      " '[[SystemMessage, HumanMessage]]}on_chat_model_stream[model '\n",
      " 'name]AIMessageChunk(content=\"hello\")on_chat_model_end[model '\n",
      " 'name]{\"messages\": [[SystemMessage, HumanMessage]]}{\"generations\": [...], '\n",
      " '\"llm_output\": None, ...}on_llm_start[model name]{\\'input\\': '\n",
      " \"'hello'}on_llm_stream[model name]'Hello'on_llm_end[model name]'Hello \"\n",
      " 'human!\\'on_chain_startformat_docson_chain_streamformat_docs\"hello world!, '\n",
      " 'goodbye world!\"on_chain_endformat_docs[Document(...)]\"hello world!, goodbye '\n",
      " 'world!\"on_tool_startsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_streamsome_tool{\"x\": '\n",
      " '1, \"y\": \"2\"}on_tool_endsome_tool{\"x\": 1, \"y\": '\n",
      " '\"2\"}on_retriever_start[retriever name]{\"query\": '\n",
      " '\"hello\"}on_retriever_chunk[retriever name]{documents: '\n",
      " '[...]}on_retriever_end[retriever name]{\"query\": \"hello\"}{documents: '\n",
      " '[...]}on_prompt_start[template_name]{\"question\": '\n",
      " '\"hello\"}on_prompt_end[template_name]{\"question\": '\n",
      " '\"hello\"}ChatPromptValue(messages: [SystemMessage, ...])Chat '\n",
      " \"Model\\u200bLet's start off by looking at the events produced by a chat \"\n",
      " 'model.events = []async for event in model.astream_events(\"hello\", '\n",
      " 'version=\"v1\"):    '\n",
      " 'events.append(event)/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: '\n",
      " 'LangChainBetaWarning: This API is in beta and may change in the future.  '\n",
      " 'warn_beta(noteHey what\\'s that funny version=\"v1\" parameter in the API?! '\n",
      " \"üòæThis is a beta API, and we're almost certainly going to make some changes \"\n",
      " 'to it.This version parameter will allow us to minimize such breaking changes '\n",
      " \"to your code. In short, we are annoying you now, so we don't have to annoy \"\n",
      " \"you later.Let's take a look at the few of the start event and a few of the \"\n",
      " \"end events.events[:3][{'event': 'on_chat_model_start',  'run_id': \"\n",
      " \"'555843ed-3d24-4774-af25-fbf030d5e8c4',  'name': 'ChatAnthropic',  'tags': \"\n",
      " \"[],  'metadata': {},  'data': {'input': 'hello'}}, {'event': \"\n",
      " \"'on_chat_model_stream',  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  \"\n",
      " \"'tags': [],  'metadata': {},  'name': 'ChatAnthropic',  'data': {'chunk': \"\n",
      " \"AIMessageChunk(content=' Hello')}}, {'event': 'on_chat_model_stream',  \"\n",
      " \"'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': [],  'metadata': \"\n",
      " \"{},  'name': 'ChatAnthropic',  'data': {'chunk': \"\n",
      " \"AIMessageChunk(content='!')}}]events[-2:][{'event': 'on_chat_model_stream',  \"\n",
      " \"'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': [],  'metadata': \"\n",
      " \"{},  'name': 'ChatAnthropic',  'data': {'chunk': \"\n",
      " \"AIMessageChunk(content='')}}, {'event': 'on_chat_model_end',  'name': \"\n",
      " \"'ChatAnthropic',  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': \"\n",
      " \"[],  'metadata': {},  'data': {'output': AIMessageChunk(content=' \"\n",
      " \"Hello!')}}]Chain\\u200bLet's revisit the example chain that parsed streaming \"\n",
      " 'JSON to explore the streaming events API.chain = (    model | '\n",
      " 'JsonOutputParser())  # Due to a bug in older versions of Langchain, '\n",
      " 'JsonOutputParser did not stream results from some modelsevents = [    '\n",
      " \"event    async for event in chain.astream_events(        'output a list of \"\n",
      " 'the countries france, spain and japan and their populations in JSON format. '\n",
      " 'Use a dict with an outer key of \"countries\" which contains a list of '\n",
      " \"countries. Each country should have the key `name` and `population`',        \"\n",
      " 'version=\"v1\",    )]If you examine at the first few events, you\\'ll notice '\n",
      " 'that there are 3 different start events rather than 2 start events.The three '\n",
      " 'start events correspond to:The chain (model + parser)The modelThe '\n",
      " \"parserevents[:3][{'event': 'on_chain_start',  'run_id': \"\n",
      " \"'b1074bff-2a17-458b-9e7b-625211710df4',  'name': 'RunnableSequence',  \"\n",
      " \"'tags': [],  'metadata': {},  'data': {'input': 'output a list of the \"\n",
      " 'countries france, spain and japan and their populations in JSON format. Use '\n",
      " 'a dict with an outer key of \"countries\" which contains a list of countries. '\n",
      " \"Each country should have the key `name` and `population`'}}, {'event': \"\n",
      " \"'on_chat_model_start',  'name': 'ChatAnthropic',  'run_id': \"\n",
      " \"'6072be59-1f43-4f1c-9470-3b92e8406a99',  'tags': ['seq:step:1'],  \"\n",
      " \"'metadata': {},  'data': {'input': {'messages': \"\n",
      " \"[[HumanMessage(content='output a list of the countries france, spain and \"\n",
      " 'japan and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " \"key `name` and `population`')]]}}}, {'event': 'on_parser_start',  'name': \"\n",
      " \"'JsonOutputParser',  'run_id': 'bf978194-0eda-4494-ad15-3a5bfe69cd59',  \"\n",
      " \"'tags': ['seq:step:2'],  'metadata': {},  'data': {}}]What do you think \"\n",
      " \"you'd see if you looked at the last 3 events? what about the middle?Let's \"\n",
      " 'use this API to take output the stream events from the model and the parser. '\n",
      " \"We're ignoring start events, end events and events from the chain.num_events \"\n",
      " \"= 0async for event in chain.astream_events(    'output a list of the \"\n",
      " 'countries france, spain and japan and their populations in JSON format. Use '\n",
      " 'a dict with an outer key of \"countries\" which contains a list of countries. '\n",
      " \"Each country should have the key `name` and `population`',    \"\n",
      " 'version=\"v1\",):    kind = event[\"event\"]    if kind == '\n",
      " '\"on_chat_model_stream\":        print(            f\"Chat model chunk: '\n",
      " '{repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        '\n",
      " ')    if kind == \"on_parser_stream\":        print(f\"Parser chunk: '\n",
      " '{event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if '\n",
      " 'num_events > 30:        # Truncate the output        print(\"...\")        '\n",
      " \"breakChat model chunk: ' Here'Chat model chunk: ' is'Chat model chunk: ' \"\n",
      " \"the'Chat model chunk: ' JSON'Chat model chunk: ' with'Chat model chunk: ' \"\n",
      " \"the'Chat model chunk: ' requested'Chat model chunk: ' countries'Chat model \"\n",
      " \"chunk: ' and'Chat model chunk: ' their'Chat model chunk: ' populations'Chat \"\n",
      " \"model chunk: ':'Chat model chunk: '\\\\n\\\\n```'Chat model chunk: 'json'Parser \"\n",
      " \"chunk: {}Chat model chunk: '\\\\n{'Chat model chunk: '\\\\n 'Chat model chunk: ' \"\n",
      " '\"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: '\n",
      " \"{'countries': []}Chat model chunk: ' ['Chat model chunk: '\\\\n   'Parser \"\n",
      " \"chunk: {'countries': [{}]}Chat model chunk: ' {'...Because both the model \"\n",
      " 'and the parser support streaming, we see sreaming events from both '\n",
      " \"components in real time! Kind of cool isn't it? ü¶úFiltering \"\n",
      " 'Events\\u200bBecause this API produces so many events, it is useful to be '\n",
      " 'able to filter on events.You can filter by either component name, component '\n",
      " 'tags or component type.By Name\\u200bchain = model.with_config({\"run_name\": '\n",
      " '\"model\"}) | JsonOutputParser().with_config(    {\"run_name\": '\n",
      " '\"my_parser\"})max_events = 0async for event in chain.astream_events(    '\n",
      " \"'output a list of the countries france, spain and japan and their \"\n",
      " 'populations in JSON format. Use a dict with an outer key of \"countries\" '\n",
      " 'which contains a list of countries. Each country should have the key `name` '\n",
      " 'and `population`\\',    version=\"v1\",    include_names=[\"my_parser\"],):    '\n",
      " 'print(event)    max_events += 1    if max_events > 10:        # Truncate '\n",
      " 'output        print(\"...\")        break{\\'event\\': \\'on_parser_start\\', '\n",
      " \"'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', \"\n",
      " \"'tags': ['seq:step:2'], 'metadata': {}, 'data': {}}{'event': \"\n",
      " \"'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {}}}{'event': 'on_parser_stream', 'name': 'my_parser', \"\n",
      " \"'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': {'countries': []}}}{'event': \"\n",
      " \"'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{}]}}}{'event': 'on_parser_stream', \"\n",
      " \"'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', \"\n",
      " \"'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': \"\n",
      " \"[{'name': ''}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', \"\n",
      " \"'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': {'countries': [{'name': \"\n",
      " \"'France'}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': \"\n",
      " \"67}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': \"\n",
      " \"6739}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': \"\n",
      " \"673915}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': \"\n",
      " \"67391582}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': \"\n",
      " '67391582}, {}]}}}...By Type\\u200bchain = model.with_config({\"run_name\": '\n",
      " '\"model\"}) | JsonOutputParser().with_config(    {\"run_name\": '\n",
      " '\"my_parser\"})max_events = 0async for event in chain.astream_events(    '\n",
      " \"'output a list of the countries france, spain and japan and their \"\n",
      " 'populations in JSON format. Use a dict with an outer key of \"countries\" '\n",
      " 'which contains a list of countries. Each country should have the key `name` '\n",
      " 'and `population`\\',    version=\"v1\",    include_types=[\"chat_model\"],):    '\n",
      " 'print(event)    max_events += 1    if max_events > 10:        # Truncate '\n",
      " 'output        print(\"...\")        break{\\'event\\': \\'on_chat_model_start\\', '\n",
      " \"'name': 'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': \"\n",
      " \"['seq:step:1'], 'metadata': {}, 'data': {'input': {'messages': \"\n",
      " \"[[HumanMessage(content='output a list of the countries france, spain and \"\n",
      " 'japan and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " \"key `name` and `population`')]]}}}{'event': 'on_chat_model_stream', 'name': \"\n",
      " \"'model', 'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': \"\n",
      " \"['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' \"\n",
      " \"Here')}}{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' is')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' JSON')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' with')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' requested')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' countries')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' and')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' their')}}...By \"\n",
      " 'Tags\\u200bcautionTags are inherited by child components of a given runnable. '\n",
      " \"If you're using tags to filter, make sure that this is what you want.chain = \"\n",
      " '(model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})max_events = '\n",
      " \"0async for event in chain.astream_events(    'output a list of the countries \"\n",
      " 'france, spain and japan and their populations in JSON format. Use a dict '\n",
      " 'with an outer key of \"countries\" which contains a list of countries. Each '\n",
      " 'country should have the key `name` and `population`\\',    version=\"v1\",    '\n",
      " 'include_tags=[\"my_chain\"],):    print(event)    max_events += 1    if '\n",
      " 'max_events > 10:        # Truncate output        print(\"...\")        '\n",
      " \"break{'event': 'on_chain_start', 'run_id': \"\n",
      " \"'190875f3-3fb7-49ad-9b6e-f49da22f3e49', 'name': 'RunnableSequence', 'tags': \"\n",
      " \"['my_chain'], 'metadata': {}, 'data': {'input': 'output a list of the \"\n",
      " 'countries france, spain and japan and their populations in JSON format. Use '\n",
      " 'a dict with an outer key of \"countries\" which contains a list of countries. '\n",
      " \"Each country should have the key `name` and `population`'}}{'event': \"\n",
      " \"'on_chat_model_start', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'input': {'messages': \"\n",
      " \"[[HumanMessage(content='output a list of the countries france, spain and \"\n",
      " 'japan and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " \"key `name` and `population`')]]}}}{'event': 'on_parser_start', 'name': \"\n",
      " \"'JsonOutputParser', 'run_id': '3b5e4ca1-40fe-4a02-9a19-ba2a43a6115c', \"\n",
      " \"'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' Here')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' is')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' JSON')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' with')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' \"\n",
      " \"requested')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', \"\n",
      " \"'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', \"\n",
      " \"'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' \"\n",
      " \"countries')}}...Non-streaming components\\u200bRemember how some components \"\n",
      " \"don't stream well because they don't operate on input streams?While such \"\n",
      " 'components can break streaming of the final output when using astream, '\n",
      " 'astream_events will still yield streaming events from intermediate steps '\n",
      " 'that support streaming!# Function that does not support streaming.# It '\n",
      " 'operates on the finalizes inputs rather than# operating on the input '\n",
      " 'stream.def _extract_country_names(inputs):    \"\"\"A function that does not '\n",
      " 'operates on input streams and breaks streaming.\"\"\"    if not '\n",
      " 'isinstance(inputs, dict):        return \"\"    if \"countries\" not in '\n",
      " 'inputs:        return \"\"    countries = inputs[\"countries\"]    if not '\n",
      " 'isinstance(countries, list):        return \"\"    country_names = [        '\n",
      " 'country.get(\"name\") for country in countries if isinstance(country, dict)    '\n",
      " ']    return country_nameschain = (    model | JsonOutputParser() | '\n",
      " '_extract_country_names)  # This parser only works with OpenAI right nowAs '\n",
      " \"expected, the astream API doesn't work correctly because \"\n",
      " \"_extract_country_names doesn't operate on streams.async for chunk in \"\n",
      " \"chain.astream(    'output a list of the countries france, spain and japan \"\n",
      " 'and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " \"key `name` and `population`',):    print(chunk, flush=True)['France', \"\n",
      " \"'Spain', 'Japan']Now, let's confirm that with astream_events we're still \"\n",
      " 'seeing streaming output from the model and the parser.num_events = 0async '\n",
      " \"for event in chain.astream_events(    'output a list of the countries \"\n",
      " 'france, spain and japan and their populations in JSON format. Use a dict '\n",
      " 'with an outer key of \"countries\" which contains a list of countries. Each '\n",
      " 'country should have the key `name` and `population`\\',    version=\"v1\",):    '\n",
      " 'kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        '\n",
      " 'print(            f\"Chat model chunk: '\n",
      " '{repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        '\n",
      " ')    if kind == \"on_parser_stream\":        print(f\"Parser chunk: '\n",
      " '{event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if '\n",
      " 'num_events > 30:        # Truncate the output        print(\"...\")        '\n",
      " \"breakChat model chunk: ' Here'Chat model chunk: ' is'Chat model chunk: ' \"\n",
      " \"the'Chat model chunk: ' JSON'Chat model chunk: ' with'Chat model chunk: ' \"\n",
      " \"the'Chat model chunk: ' requested'Chat model chunk: ' countries'Chat model \"\n",
      " \"chunk: ' and'Chat model chunk: ' their'Chat model chunk: ' populations'Chat \"\n",
      " \"model chunk: ':'Chat model chunk: '\\\\n\\\\n```'Chat model chunk: 'json'Parser \"\n",
      " \"chunk: {}Chat model chunk: '\\\\n{'Chat model chunk: '\\\\n 'Chat model chunk: ' \"\n",
      " '\"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: '\n",
      " \"{'countries': []}Chat model chunk: ' ['Chat model chunk: '\\\\n   'Parser \"\n",
      " \"chunk: {'countries': [{}]}Chat model chunk: ' {'Chat model chunk: '\\\\n     \"\n",
      " '\\'Chat model chunk: \\' \"\\'...Propagating Callbacks\\u200bcautionIf you\\'re '\n",
      " 'using invoking runnables inside your tools, you need to propagate callbacks '\n",
      " 'to the runnable; otherwise, no stream events will be generated.noteWhen '\n",
      " 'using RunnableLambdas or @chain decorator, callbacks are propagated '\n",
      " 'automatically behind the scenes.from langchain_core.runnables import '\n",
      " 'RunnableLambdafrom langchain_core.tools import tooldef reverse_word(word: '\n",
      " 'str):    return word[::-1]reverse_word = '\n",
      " 'RunnableLambda(reverse_word)@tooldef bad_tool(word: str):    \"\"\"Custom tool '\n",
      " 'that doesn\\'t propagate callbacks.\"\"\"    return '\n",
      " 'reverse_word.invoke(word)async for event in bad_tool.astream_events(\"hello\", '\n",
      " 'version=\"v1\"):    print(event){\\'event\\': \\'on_tool_start\\', \\'run_id\\': '\n",
      " \"'ae7690f8-ebc9-4886-9bbe-cb336ff274f2', 'name': 'bad_tool', 'tags': [], \"\n",
      " \"'metadata': {}, 'data': {'input': 'hello'}}{'event': 'on_tool_stream', \"\n",
      " \"'run_id': 'ae7690f8-ebc9-4886-9bbe-cb336ff274f2', 'tags': [], 'metadata': \"\n",
      " \"{}, 'name': 'bad_tool', 'data': {'chunk': 'olleh'}}{'event': 'on_tool_end', \"\n",
      " \"'name': 'bad_tool', 'run_id': 'ae7690f8-ebc9-4886-9bbe-cb336ff274f2', \"\n",
      " \"'tags': [], 'metadata': {}, 'data': {'output': 'olleh'}}Here's a \"\n",
      " \"re-implementation that does propagate callbacks correctly. You'll notice \"\n",
      " \"that now we're getting events from the reverse_word runnable as \"\n",
      " 'well.@tooldef correct_tool(word: str, callbacks):    \"\"\"A tool that '\n",
      " 'correctly propagates callbacks.\"\"\"    return reverse_word.invoke(word, '\n",
      " '{\"callbacks\": callbacks})async for event in '\n",
      " 'correct_tool.astream_events(\"hello\", version=\"v1\"):    '\n",
      " \"print(event){'event': 'on_tool_start', 'run_id': \"\n",
      " \"'384f1710-612e-4022-a6d4-8a7bb0cc757e', 'name': 'correct_tool', 'tags': [], \"\n",
      " \"'metadata': {}, 'data': {'input': 'hello'}}{'event': 'on_chain_start', \"\n",
      " \"'name': 'reverse_word', 'run_id': 'c4882303-8867-4dff-b031-7d9499b39dda', \"\n",
      " \"'tags': [], 'metadata': {}, 'data': {'input': 'hello'}}{'event': \"\n",
      " \"'on_chain_end', 'name': 'reverse_word', 'run_id': \"\n",
      " \"'c4882303-8867-4dff-b031-7d9499b39dda', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'input': 'hello', 'output': 'olleh'}}{'event': 'on_tool_stream', 'run_id': \"\n",
      " \"'384f1710-612e-4022-a6d4-8a7bb0cc757e', 'tags': [], 'metadata': {}, 'name': \"\n",
      " \"'correct_tool', 'data': {'chunk': 'olleh'}}{'event': 'on_tool_end', 'name': \"\n",
      " \"'correct_tool', 'run_id': '384f1710-612e-4022-a6d4-8a7bb0cc757e', 'tags': \"\n",
      " \"[], 'metadata': {}, 'data': {'output': 'olleh'}}If you're invoking runnables \"\n",
      " 'from within Runnable Lambdas or @chains, then callbacks will be passed '\n",
      " 'automatically on your behalf.from langchain_core.runnables import '\n",
      " 'RunnableLambdaasync def reverse_and_double(word: str):    return await '\n",
      " 'reverse_word.ainvoke(word) * 2reverse_and_double = '\n",
      " 'RunnableLambda(reverse_and_double)await '\n",
      " 'reverse_and_double.ainvoke(\"1234\")async for event in '\n",
      " 'reverse_and_double.astream_events(\"1234\", version=\"v1\"):    '\n",
      " \"print(event){'event': 'on_chain_start', 'run_id': \"\n",
      " \"'4fe56c7b-6982-4999-a42d-79ba56151176', 'name': 'reverse_and_double', \"\n",
      " \"'tags': [], 'metadata': {}, 'data': {'input': '1234'}}{'event': \"\n",
      " \"'on_chain_start', 'name': 'reverse_word', 'run_id': \"\n",
      " \"'335fe781-8944-4464-8d2e-81f61d1f85f5', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'input': '1234'}}{'event': 'on_chain_end', 'name': 'reverse_word', \"\n",
      " \"'run_id': '335fe781-8944-4464-8d2e-81f61d1f85f5', 'tags': [], 'metadata': \"\n",
      " \"{}, 'data': {'input': '1234', 'output': '4321'}}{'event': 'on_chain_stream', \"\n",
      " \"'run_id': '4fe56c7b-6982-4999-a42d-79ba56151176', 'tags': [], 'metadata': \"\n",
      " \"{}, 'name': 'reverse_and_double', 'data': {'chunk': '43214321'}}{'event': \"\n",
      " \"'on_chain_end', 'name': 'reverse_and_double', 'run_id': \"\n",
      " \"'4fe56c7b-6982-4999-a42d-79ba56151176', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'output': '43214321'}}And with the @chain decorator:from \"\n",
      " 'langchain_core.runnables import chain@chainasync def '\n",
      " 'reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * '\n",
      " '2await reverse_and_double.ainvoke(\"1234\")async for event in '\n",
      " 'reverse_and_double.astream_events(\"1234\", version=\"v1\"):    '\n",
      " \"print(event){'event': 'on_chain_start', 'run_id': \"\n",
      " \"'7485eedb-1854-429c-a2f8-03d01452daef', 'name': 'reverse_and_double', \"\n",
      " \"'tags': [], 'metadata': {}, 'data': {'input': '1234'}}{'event': \"\n",
      " \"'on_chain_start', 'name': 'reverse_word', 'run_id': \"\n",
      " \"'e7cddab2-9b95-4e80-abaf-4b2429117835', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'input': '1234'}}{'event': 'on_chain_end', 'name': 'reverse_word', \"\n",
      " \"'run_id': 'e7cddab2-9b95-4e80-abaf-4b2429117835', 'tags': [], 'metadata': \"\n",
      " \"{}, 'data': {'input': '1234', 'output': '4321'}}{'event': 'on_chain_stream', \"\n",
      " \"'run_id': '7485eedb-1854-429c-a2f8-03d01452daef', 'tags': [], 'metadata': \"\n",
      " \"{}, 'name': 'reverse_and_double', 'data': {'chunk': '43214321'}}{'event': \"\n",
      " \"'on_chain_end', 'name': 'reverse_and_double', 'run_id': \"\n",
      " \"'7485eedb-1854-429c-a2f8-03d01452daef', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'output': '43214321'}}Help us out by providing feedback on this \"\n",
      " 'documentation page:PreviousAdvantages of LCELNextAdd message history '\n",
      " '(memory)Using StreamLLMs and Chat ModelsChainsWorking with Input '\n",
      " 'StreamsNon-streaming componentsUsing Stream EventsEvent ReferenceChat '\n",
      " 'ModelChainFiltering EventsNon-streaming componentsPropagating '\n",
      " 'CallbacksCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Route logic based on input | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreRoute logic based on inputOn this pageDynamically route logic '\n",
      " 'based on inputThis notebook covers how to do routing in the LangChain '\n",
      " 'Expression Language.Routing allows you to create non-deterministic chains '\n",
      " 'where the output of a previous step defines the next step. Routing helps '\n",
      " 'provide structure and consistency around interactions with LLMs.There are '\n",
      " 'two ways to perform routing:Conditionally return runnables from a '\n",
      " \"RunnableLambda (recommended)Using a RunnableBranch.We'll illustrate both \"\n",
      " 'methods using a two step sequence where the first step classifies an input '\n",
      " 'question as being about LangChain, Anthropic, or Other, then routes to a '\n",
      " \"corresponding prompt chain.Example Setup\\u200bFirst, let's create a chain \"\n",
      " 'that will identify incoming questions as being about LangChain, Anthropic, '\n",
      " 'or Other:from langchain_anthropic import ChatAnthropicfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import PromptTemplatechain = (    '\n",
      " 'PromptTemplate.from_template(        \"\"\"Given the user question below, '\n",
      " 'classify it as either being about `LangChain`, `Anthropic`, or `Other`.Do '\n",
      " 'not respond with more than one '\n",
      " 'word.<question>{question}</question>Classification:\"\"\"    )    | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")    | '\n",
      " 'StrOutputParser())chain.invoke({\"question\": \"how do I call '\n",
      " 'Anthropic?\"})\\'Anthropic\\'Now, let\\'s create three sub '\n",
      " 'chains:langchain_chain = PromptTemplate.from_template(    \"\"\"You are an '\n",
      " 'expert in langchain. \\\\Always answer questions starting with \"As Harrison '\n",
      " 'Chase told me\". \\\\Respond to the following question:Question: '\n",
      " '{question}Answer:\"\"\") | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")anthropic_chain = '\n",
      " 'PromptTemplate.from_template(    \"\"\"You are an expert in anthropic. \\\\Always '\n",
      " 'answer questions starting with \"As Dario Amodei told me\". \\\\Respond to the '\n",
      " 'following question:Question: {question}Answer:\"\"\") | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")general_chain = '\n",
      " 'PromptTemplate.from_template(    \"\"\"Respond to the following '\n",
      " 'question:Question: {question}Answer:\"\"\") | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")Using a custom function '\n",
      " '(Recommended)\\u200bYou can also use a custom function to route between '\n",
      " 'different outputs. Here\\'s an example:def route(info):    if \"anthropic\" in '\n",
      " 'info[\"topic\"].lower():        return anthropic_chain    elif \"langchain\" in '\n",
      " 'info[\"topic\"].lower():        return langchain_chain    else:        return '\n",
      " 'general_chainfrom langchain_core.runnables import RunnableLambdafull_chain = '\n",
      " '{\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(    '\n",
      " 'route)full_chain.invoke({\"question\": \"how do I use '\n",
      " 'Anthropic?\"})AIMessage(content=\"As Dario Amodei told me, to use Anthropic, '\n",
      " \"you can start by exploring the company's website and learning about their \"\n",
      " 'mission, values, and the different services and products they offer. '\n",
      " 'Anthropic is focused on developing safe and ethical AI systems, so they have '\n",
      " 'a strong emphasis on transparency and responsible AI development. '\n",
      " \"\\\\n\\\\nDepending on your specific needs, you can look into Anthropic's AI \"\n",
      " 'research and development services, which cover areas like natural language '\n",
      " 'processing, computer vision, and reinforcement learning. They also offer '\n",
      " 'consulting and advisory services to help organizations navigate the '\n",
      " 'challenges and opportunities of AI integration.\\\\n\\\\nAdditionally, Anthropic '\n",
      " 'has released some open-source AI models and tools that you can explore and '\n",
      " 'experiment with. These can be a great way to get hands-on experience with '\n",
      " \"Anthropic's approach to AI development.\\\\n\\\\nOverall, Anthropic aims to be a \"\n",
      " \"reliable and trustworthy partner in the AI space, so I'd encourage you to \"\n",
      " 'reach out to them directly to discuss how they can best support your '\n",
      " 'specific requirements.\", response_metadata={\\'id\\': '\n",
      " '\\'msg_01CtLFgFSwvTaJomrihE87Ra\\', \\'content\\': [ContentBlock(text=\"As Dario '\n",
      " \"Amodei told me, to use Anthropic, you can start by exploring the company's \"\n",
      " 'website and learning about their mission, values, and the different services '\n",
      " 'and products they offer. Anthropic is focused on developing safe and ethical '\n",
      " 'AI systems, so they have a strong emphasis on transparency and responsible '\n",
      " 'AI development. \\\\n\\\\nDepending on your specific needs, you can look into '\n",
      " \"Anthropic's AI research and development services, which cover areas like \"\n",
      " 'natural language processing, computer vision, and reinforcement learning. '\n",
      " 'They also offer consulting and advisory services to help organizations '\n",
      " 'navigate the challenges and opportunities of AI '\n",
      " 'integration.\\\\n\\\\nAdditionally, Anthropic has released some open-source AI '\n",
      " 'models and tools that you can explore and experiment with. These can be a '\n",
      " \"great way to get hands-on experience with Anthropic's approach to AI \"\n",
      " 'development.\\\\n\\\\nOverall, Anthropic aims to be a reliable and trustworthy '\n",
      " \"partner in the AI space, so I'd encourage you to reach out to them directly \"\n",
      " 'to discuss how they can best support your specific requirements.\", '\n",
      " \"type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', \"\n",
      " \"'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', \"\n",
      " \"'usage': Usage(input_tokens=53, \"\n",
      " 'output_tokens=219)})full_chain.invoke({\"question\": \"how do I use '\n",
      " 'LangChain?\"})AIMessage(content=\"As Harrison Chase told me, using LangChain '\n",
      " 'involves a few key steps:\\\\n\\\\n1. **Set up your environment**: Install the '\n",
      " 'necessary Python packages, including the LangChain library itself, as well '\n",
      " 'as any other dependencies your application might require, such as language '\n",
      " 'models or other integrations.\\\\n\\\\n2. **Understand the core concepts**: '\n",
      " 'LangChain revolves around a few core concepts, like Agents, Chains, and '\n",
      " 'Tools. Familiarize yourself with these concepts and how they work together '\n",
      " 'to build powerful language-based applications.\\\\n\\\\n3. **Identify your use '\n",
      " 'case**: Determine what kind of task or application you want to build using '\n",
      " 'LangChain, such as a chatbot, a question-answering system, or a document '\n",
      " 'summarization tool.\\\\n\\\\n4. **Choose the appropriate components**: Based on '\n",
      " 'your use case, select the right LangChain components, such as agents, '\n",
      " 'chains, and tools, to build your application.\\\\n\\\\n5. **Integrate with '\n",
      " 'language models**: LangChain is designed to work seamlessly with various '\n",
      " \"language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your \"\n",
      " 'chosen language model to your LangChain application.\\\\n\\\\n6. **Implement '\n",
      " \"your application logic**: Use LangChain's building blocks to implement the \"\n",
      " 'specific functionality of your application, such as prompting the language '\n",
      " 'model, processing the response, and integrating with other services or data '\n",
      " 'sources.\\\\n\\\\n7. **Test and iterate**: Thoroughly test your application, '\n",
      " 'gather feedback, and iterate on your design and implementation to improve '\n",
      " 'its performance and user experience.\\\\n\\\\nAs Harrison Chase emphasized, '\n",
      " 'LangChain provides a flexible and powerful framework for building '\n",
      " 'language-based applications, making it easier to leverage the capabilities '\n",
      " 'of modern language models. By following these steps, you can get started '\n",
      " 'with LangChain and create innovative solutions tailored to your specific '\n",
      " 'needs.\", response_metadata={\\'id\\': \\'msg_01H3UXAAHG4TwxJLpxwuuVU7\\', '\n",
      " '\\'content\\': [ContentBlock(text=\"As Harrison Chase told me, using LangChain '\n",
      " 'involves a few key steps:\\\\n\\\\n1. **Set up your environment**: Install the '\n",
      " 'necessary Python packages, including the LangChain library itself, as well '\n",
      " 'as any other dependencies your application might require, such as language '\n",
      " 'models or other integrations.\\\\n\\\\n2. **Understand the core concepts**: '\n",
      " 'LangChain revolves around a few core concepts, like Agents, Chains, and '\n",
      " 'Tools. Familiarize yourself with these concepts and how they work together '\n",
      " 'to build powerful language-based applications.\\\\n\\\\n3. **Identify your use '\n",
      " 'case**: Determine what kind of task or application you want to build using '\n",
      " 'LangChain, such as a chatbot, a question-answering system, or a document '\n",
      " 'summarization tool.\\\\n\\\\n4. **Choose the appropriate components**: Based on '\n",
      " 'your use case, select the right LangChain components, such as agents, '\n",
      " 'chains, and tools, to build your application.\\\\n\\\\n5. **Integrate with '\n",
      " 'language models**: LangChain is designed to work seamlessly with various '\n",
      " \"language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your \"\n",
      " 'chosen language model to your LangChain application.\\\\n\\\\n6. **Implement '\n",
      " \"your application logic**: Use LangChain's building blocks to implement the \"\n",
      " 'specific functionality of your application, such as prompting the language '\n",
      " 'model, processing the response, and integrating with other services or data '\n",
      " 'sources.\\\\n\\\\n7. **Test and iterate**: Thoroughly test your application, '\n",
      " 'gather feedback, and iterate on your design and implementation to improve '\n",
      " 'its performance and user experience.\\\\n\\\\nAs Harrison Chase emphasized, '\n",
      " 'LangChain provides a flexible and powerful framework for building '\n",
      " 'language-based applications, making it easier to leverage the capabilities '\n",
      " 'of modern language models. By following these steps, you can get started '\n",
      " 'with LangChain and create innovative solutions tailored to your specific '\n",
      " 'needs.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': '\n",
      " \"'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': \"\n",
      " \"'message', 'usage': Usage(input_tokens=50, \"\n",
      " 'output_tokens=400)})full_chain.invoke({\"question\": \"whats 2 + '\n",
      " '2\"})AIMessage(content=\\'4\\', response_metadata={\\'id\\': '\n",
      " \"'msg_01UAKP81jTZu9fyiyFYhsbHc', 'content': [ContentBlock(text='4', \"\n",
      " \"type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', \"\n",
      " \"'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', \"\n",
      " \"'usage': Usage(input_tokens=28, output_tokens=5)})Using a \"\n",
      " 'RunnableBranch\\u200bA RunnableBranch is a special type of runnable that '\n",
      " 'allows you to define a set of conditions and runnables to execute based on '\n",
      " \"the input. It does not offer anything that you can't achieve in a custom \"\n",
      " 'function as described above, so we recommend using a custom function '\n",
      " 'instead.A RunnableBranch is initialized with a list of (condition, runnable) '\n",
      " 'pairs and a default runnable. It selects which branch by passing each '\n",
      " \"condition the input it's invoked with. It selects the first condition to \"\n",
      " 'evaluate to True, and runs the corresponding runnable to that condition with '\n",
      " 'the input. If no provided conditions match, it runs the default '\n",
      " \"runnable.Here's an example of what it looks like in action:from \"\n",
      " 'langchain_core.runnables import RunnableBranchbranch = RunnableBranch(    '\n",
      " '(lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),    (lambda '\n",
      " 'x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),    '\n",
      " 'general_chain,)full_chain = {\"topic\": chain, \"question\": lambda x: '\n",
      " 'x[\"question\"]} | branchfull_chain.invoke({\"question\": \"how do I use '\n",
      " 'Anthropic?\"})AIMessage(content=\"As Dario Amodei told me, to use Anthropic, '\n",
      " 'you should first familiarize yourself with our mission and principles. '\n",
      " 'Anthropic is committed to developing safe and beneficial artificial '\n",
      " 'intelligence that can help solve important problems facing humanity. '\n",
      " '\\\\n\\\\nTo get started, I recommend exploring the resources on our website, '\n",
      " 'which cover our research, products, and approach to AI development. You can '\n",
      " \"also reach out to our team to learn more about how Anthropic's technology \"\n",
      " 'and services can support your specific needs.\\\\n\\\\nThe key is to engage with '\n",
      " 'us in a way that aligns with our values of transparency, ethical AI, and a '\n",
      " \"commitment to the wellbeing of humanity. We're here to collaborate and help \"\n",
      " 'you harness the power of advanced AI responsibly.\", '\n",
      " \"response_metadata={'id': 'msg_0187BVnpniPDJnVvwf3M1LdY', 'content': \"\n",
      " '[ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you should '\n",
      " 'first familiarize yourself with our mission and principles. Anthropic is '\n",
      " 'committed to developing safe and beneficial artificial intelligence that can '\n",
      " 'help solve important problems facing humanity. \\\\n\\\\nTo get started, I '\n",
      " 'recommend exploring the resources on our website, which cover our research, '\n",
      " 'products, and approach to AI development. You can also reach out to our team '\n",
      " \"to learn more about how Anthropic's technology and services can support your \"\n",
      " 'specific needs.\\\\n\\\\nThe key is to engage with us in a way that aligns with '\n",
      " 'our values of transparency, ethical AI, and a commitment to the wellbeing of '\n",
      " \"humanity. We're here to collaborate and help you harness the power of \"\n",
      " 'advanced AI responsibly.\", type=\\'text\\')], \\'model\\': '\n",
      " \"'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', \"\n",
      " \"'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, \"\n",
      " 'output_tokens=160)})full_chain.invoke({\"question\": \"how do I use '\n",
      " 'LangChain?\"})AIMessage(content=\"As Harrison Chase told me, using LangChain '\n",
      " \"involves several key steps. First, you'll need to install the LangChain \"\n",
      " \"library and import the necessary modules. Then, you'll want to define your \"\n",
      " 'language model, any data sources you plan to use, and the specific tasks you '\n",
      " 'want to accomplish, such as question answering, text generation, or '\n",
      " 'agent-based reasoning. \\\\n\\\\nLangChain provides a flexible framework for '\n",
      " 'building applications that leverage large language models. It includes '\n",
      " 'abstractions for things like retrievers, prompts, and chains, which allow '\n",
      " 'you to compose different components together to create powerful workflows. '\n",
      " '\\\\n\\\\nThe documentation on the LangChain website is excellent and covers '\n",
      " \"many common use cases in detail. I'd recommend starting there to get a solid \"\n",
      " 'understanding of the core concepts and how to apply them to your specific '\n",
      " 'needs. And of course, feel free to reach out if you have any other questions '\n",
      " \"- I'm always happy to share more insights from my conversations with \"\n",
      " 'Harrison.\", response_metadata={\\'id\\': \\'msg_01T1naS99wGPkEAP4LME8iAv\\', '\n",
      " '\\'content\\': [ContentBlock(text=\"As Harrison Chase told me, using LangChain '\n",
      " \"involves several key steps. First, you'll need to install the LangChain \"\n",
      " \"library and import the necessary modules. Then, you'll want to define your \"\n",
      " 'language model, any data sources you plan to use, and the specific tasks you '\n",
      " 'want to accomplish, such as question answering, text generation, or '\n",
      " 'agent-based reasoning. \\\\n\\\\nLangChain provides a flexible framework for '\n",
      " 'building applications that leverage large language models. It includes '\n",
      " 'abstractions for things like retrievers, prompts, and chains, which allow '\n",
      " 'you to compose different components together to create powerful workflows. '\n",
      " '\\\\n\\\\nThe documentation on the LangChain website is excellent and covers '\n",
      " \"many common use cases in detail. I'd recommend starting there to get a solid \"\n",
      " 'understanding of the core concepts and how to apply them to your specific '\n",
      " 'needs. And of course, feel free to reach out if you have any other questions '\n",
      " \"- I'm always happy to share more insights from my conversations with \"\n",
      " 'Harrison.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', '\n",
      " \"'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, \"\n",
      " \"'type': 'message', 'usage': Usage(input_tokens=50, \"\n",
      " 'output_tokens=205)})full_chain.invoke({\"question\": \"whats 2 + '\n",
      " '2\"})AIMessage(content=\\'4\\', response_metadata={\\'id\\': '\n",
      " \"'msg_01T6T3TS6hRCtU8JayN93QEi', 'content': [ContentBlock(text='4', \"\n",
      " \"type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', \"\n",
      " \"'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', \"\n",
      " \"'usage': Usage(input_tokens=28, output_tokens=5)})Routing by semantic \"\n",
      " 'similarityOne especially useful technique is to use embeddings to route a '\n",
      " \"query to the most relevant prompt. Here's an example.from \"\n",
      " 'langchain.utils.math import cosine_similarityfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import PromptTemplatefrom langchain_core.runnables '\n",
      " 'import RunnableLambda, RunnablePassthroughfrom langchain_openai import '\n",
      " 'OpenAIEmbeddingsphysics_template = \"\"\"You are a very smart physics '\n",
      " 'professor. \\\\You are great at answering questions about physics in a concise '\n",
      " \"and easy to understand manner. \\\\When you don't know the answer to a \"\n",
      " \"question you admit that you don't know.Here is a \"\n",
      " 'question:{query}\"\"\"math_template = \"\"\"You are a very good mathematician. You '\n",
      " 'are great at answering math questions. \\\\You are so good because you are '\n",
      " 'able to break down hard problems into their component parts, \\\\answer the '\n",
      " 'component parts, and then put them together to answer the broader '\n",
      " 'question.Here is a question:{query}\"\"\"embeddings = '\n",
      " 'OpenAIEmbeddings()prompt_templates = [physics_template, '\n",
      " 'math_template]prompt_embeddings = '\n",
      " 'embeddings.embed_documents(prompt_templates)def prompt_router(input):    '\n",
      " 'query_embedding = embeddings.embed_query(input[\"query\"])    similarity = '\n",
      " 'cosine_similarity([query_embedding], prompt_embeddings)[0]    most_similar = '\n",
      " 'prompt_templates[similarity.argmax()]    print(\"Using MATH\" if most_similar '\n",
      " '== math_template else \"Using PHYSICS\")    return '\n",
      " 'PromptTemplate.from_template(most_similar)chain = (    {\"query\": '\n",
      " 'RunnablePassthrough()}    | RunnableLambda(prompt_router)    | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")    | '\n",
      " 'StrOutputParser())print(chain.invoke(\"What\\'s a black hole\"))Using PHYSICSAs '\n",
      " 'a physics professor, I would be happy to provide a concise and '\n",
      " 'easy-to-understand explanation of what a black hole is.A black hole is an '\n",
      " 'incredibly dense region of space-time where the gravitational pull is so '\n",
      " 'strong that nothing, not even light, can escape from it. This means that if '\n",
      " 'you were to get too close to a black hole, you would be pulled in and '\n",
      " 'crushed by the intense gravitational forces.The formation of a black hole '\n",
      " 'occurs when a massive star, much larger than our Sun, reaches the end of its '\n",
      " 'life and collapses in on itself. This collapse causes the matter to become '\n",
      " 'extremely dense, and the gravitational force becomes so strong that it '\n",
      " 'creates a point of no return, known as the event horizon.Beyond the event '\n",
      " 'horizon, the laws of physics as we know them break down, and the intense '\n",
      " 'gravitational forces create a singularity, which is a point of infinite '\n",
      " 'density and curvature in space-time.Black holes are fascinating and '\n",
      " 'mysterious objects, and there is still much to be learned about their '\n",
      " 'properties and behavior. If I were unsure about any specific details or '\n",
      " 'aspects of black holes, I would readily admit that I do not have a complete '\n",
      " 'understanding and would encourage further research and '\n",
      " 'investigation.print(chain.invoke(\"What\\'s a path integral\"))Using MATHA path '\n",
      " 'integral is a powerful mathematical concept in physics, particularly in the '\n",
      " 'field of quantum mechanics. It was developed by the renowned physicist '\n",
      " 'Richard Feynman as an alternative formulation of quantum mechanics.In a path '\n",
      " 'integral, instead of considering a single, definite path that a particle '\n",
      " 'might take from one point to another, as in classical mechanics, the '\n",
      " 'particle is considered to take all possible paths simultaneously. Each path '\n",
      " 'is assigned a complex-valued weight, and the total probability amplitude for '\n",
      " 'the particle to go from one point to another is calculated by summing '\n",
      " '(integrating) over all possible paths.The key ideas behind the path integral '\n",
      " 'formulation are:1. Superposition principle: In quantum mechanics, particles '\n",
      " 'can exist in a superposition of multiple states or paths simultaneously.2. '\n",
      " 'Probability amplitude: The probability amplitude for a particle to go from '\n",
      " 'one point to another is calculated by summing the complex-valued weights of '\n",
      " 'all possible paths.3. Weighting of paths: Each path is assigned a weight '\n",
      " 'based on the action (the time integral of the Lagrangian) along that path. '\n",
      " \"Paths with lower action have a greater weight.4. Feynman's approach: Feynman \"\n",
      " 'developed the path integral formulation as an alternative to the traditional '\n",
      " 'wave function approach in quantum mechanics, providing a more intuitive and '\n",
      " 'conceptual understanding of quantum phenomena.The path integral approach is '\n",
      " 'particularly useful in quantum field theory, where it provides a powerful '\n",
      " 'framework for calculating transition probabilities and understanding the '\n",
      " 'behavior of quantum systems. It has also found applications in various areas '\n",
      " 'of physics, such as condensed matter, statistical mechanics, and even in '\n",
      " 'finance (the path integral approach to option pricing).The mathematical '\n",
      " 'construction of the path integral involves the use of advanced concepts from '\n",
      " 'functional analysis and measure theory, making it a powerful and '\n",
      " \"sophisticated tool in the physicist's arsenal.Help us out by providing \"\n",
      " 'feedback on this documentation page:PreviousAdd message history '\n",
      " '(memory)NextInspect your runnablesExample SetupUsing a custom function '\n",
      " '(Recommended)Using a '\n",
      " 'RunnableBranchCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Managing prompt size | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreManaging prompt sizeManaging prompt sizeAgents dynamically call '\n",
      " 'tools. The results of those tool calls are added back to the prompt, so that '\n",
      " 'the agent can plan the next action. Depending on what tools are being used '\n",
      " \"and how they're being called, the agent prompt can easily grow larger than \"\n",
      " \"the model context window.With LCEL, it's easy to add custom functionality \"\n",
      " \"for managing the size of prompts within your chain or agent. Let's look at \"\n",
      " 'simple agent example that can search Wikipedia for information.%pip install '\n",
      " '--upgrade --quiet  langchain langchain-openai wikipediafrom operator import '\n",
      " 'itemgetterfrom langchain.agents import AgentExecutor, load_toolsfrom '\n",
      " 'langchain.agents.format_scratchpad import '\n",
      " 'format_to_openai_function_messagesfrom langchain.agents.output_parsers '\n",
      " 'import OpenAIFunctionsAgentOutputParserfrom langchain_community.tools import '\n",
      " 'WikipediaQueryRunfrom langchain_community.utilities import '\n",
      " 'WikipediaAPIWrapperfrom langchain_core.prompt_values import '\n",
      " 'ChatPromptValuefrom langchain_core.prompts import ChatPromptTemplate, '\n",
      " 'MessagesPlaceholderfrom langchain_openai import ChatOpenAIwiki = '\n",
      " 'WikipediaQueryRun(    api_wrapper=WikipediaAPIWrapper(top_k_results=5, '\n",
      " 'doc_content_chars_max=10_000))tools = [wiki]prompt = '\n",
      " 'ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful '\n",
      " 'assistant\"),        (\"user\", \"{input}\"),        '\n",
      " 'MessagesPlaceholder(variable_name=\"agent_scratchpad\"),    ])llm = '\n",
      " 'ChatOpenAI(model=\"gpt-3.5-turbo\")Let\\'s try a many-step question without any '\n",
      " 'prompt size handling:agent = (    {        \"input\": '\n",
      " 'itemgetter(\"input\"),        \"agent_scratchpad\": lambda x: '\n",
      " 'format_to_openai_function_messages(            '\n",
      " 'x[\"intermediate_steps\"]        ),    }    | prompt    | '\n",
      " 'llm.bind_functions(tools)    | '\n",
      " 'OpenAIFunctionsAgentOutputParser())agent_executor = '\n",
      " 'AgentExecutor(agent=agent, tools=tools, '\n",
      " 'verbose=True)agent_executor.invoke(    {        \"input\": \"Who is the current '\n",
      " \"US president? What's their home state? What's their home state's bird? \"\n",
      " 'What\\'s that bird\\'s scientific name?\"    })\\x1b[1m> Entering new '\n",
      " 'AgentExecutor chain...\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with '\n",
      " '`List of presidents of the United States`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: '\n",
      " 'List of presidents of the United StatesSummary: The president of the United '\n",
      " 'States is the head of state and head of government of the United States, '\n",
      " 'indirectly elected to a four-year term via the Electoral College. The '\n",
      " 'officeholder leads the executive branch of the federal government and is the '\n",
      " 'commander-in-chief of the United States Armed Forces. Since the office was '\n",
      " 'established in 1789, 45 men have served in 46 presidencies. The first '\n",
      " 'president, George Washington, won a unanimous vote of the Electoral College. '\n",
      " 'Grover Cleveland served two non-consecutive terms and is therefore counted '\n",
      " 'as the 22nd and 24th president of the United States, giving rise to the '\n",
      " 'discrepancy between the number of presidencies and the number of individuals '\n",
      " 'who have served as president. The incumbent president is Joe Biden.The '\n",
      " 'presidency of William Henry Harrison, who died 31 days after taking office '\n",
      " 'in 1841, was the shortest in American history. Franklin D. Roosevelt served '\n",
      " 'the longest, over twelve years, before dying early in his fourth term in '\n",
      " '1945. He is the only U.S. president to have served more than two terms. '\n",
      " 'Since the ratification of the Twenty-second Amendment to the United States '\n",
      " 'Constitution in 1951, no person may be elected president more than twice, '\n",
      " 'and no one who has served more than two years of a term to which someone '\n",
      " 'else was elected may be elected more than once.Four presidents died in '\n",
      " 'office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. '\n",
      " 'Harding, and Franklin D. Roosevelt), four were assassinated (Abraham '\n",
      " 'Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one '\n",
      " 'resigned (Richard Nixon, facing impeachment and removal from office). John '\n",
      " 'Tyler was the first vice president to assume the presidency during a '\n",
      " 'presidential term, and set the precedent that a vice president who does so '\n",
      " 'becomes the fully functioning president with his presidency.Throughout most '\n",
      " 'of its history, American politics has been dominated by political parties. '\n",
      " 'The Constitution is silent on the issue of political parties, and at the '\n",
      " 'time it came into force in 1789, no organized parties existed. Soon after '\n",
      " 'the 1st Congress convened, political factions began rallying around dominant '\n",
      " 'Washington administration officials, such as Alexander Hamilton and Thomas '\n",
      " 'Jefferson. Concerned about the capacity of political parties to destroy the '\n",
      " 'fragile unity holding the nation together, Washington remained unaffiliated '\n",
      " 'with any political faction or party throughout his eight-year presidency. He '\n",
      " 'was, and remains, the only U.S. president never affiliated with a political '\n",
      " 'party.Page: List of presidents of the United States by ageSummary: In this '\n",
      " 'list of presidents of the United States by age, the first table charts the '\n",
      " 'age of each president of the United States at the time of presidential '\n",
      " 'inauguration (first inauguration if elected to multiple and consecutive '\n",
      " 'terms), upon leaving office, and at the time of death. Where the president '\n",
      " 'is still living, their lifespan and post-presidency timespan are calculated '\n",
      " 'up to January 25, 2024.Page: List of vice presidents of the United '\n",
      " 'StatesSummary: There have been 49 vice presidents of the United States since '\n",
      " 'the office was created in 1789. Originally, the vice president was the '\n",
      " 'person who received the second-most votes for president in the Electoral '\n",
      " 'College. But after the election of 1800 produced a tie between Thomas '\n",
      " 'Jefferson and Aaron Burr, requiring the House of Representatives to choose '\n",
      " 'between them, lawmakers acted to prevent such a situation from recurring. '\n",
      " 'The Twelfth Amendment was added to the Constitution in 1804, creating the '\n",
      " 'current system where electors cast a separate ballot for the vice '\n",
      " 'presidency.The vice president is the first person in the presidential line '\n",
      " 'of succession‚Äîthat is, they assume the presidency if the president dies, '\n",
      " 'resigns, or is impeached and removed from office. Nine vice presidents have '\n",
      " 'ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, '\n",
      " 'Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, '\n",
      " \"Harry S. Truman, and Lyndon B. Johnson) through the president's death and \"\n",
      " \"one (Gerald Ford) through the president's resignation. The vice president \"\n",
      " 'also serves as the president of the Senate and may choose to cast a '\n",
      " 'tie-breaking vote on decisions made by the Senate. Vice presidents have '\n",
      " 'exercised this latter power to varying extents over the years.Before '\n",
      " 'adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the '\n",
      " 'office of the vice president could not be filled until the next '\n",
      " 'post-election inauguration. Several such vacancies occurred: seven vice '\n",
      " 'presidents died, one resigned and eight succeeded to the presidency. This '\n",
      " 'amendment allowed for a vacancy to be filled through appointment by the '\n",
      " 'president and confirmation by both chambers of the Congress. Since its '\n",
      " 'ratification, the vice presidency has been vacant twice (both in the context '\n",
      " 'of scandals surrounding the Nixon administration) and was filled both times '\n",
      " \"through this process, namely in 1973 following Spiro Agnew's resignation, \"\n",
      " 'and again in 1974 after Gerald Ford succeeded to the presidency. The '\n",
      " 'amendment also established a procedure whereby a vice president may, if the '\n",
      " 'president is unable to discharge the powers and duties of the office, '\n",
      " 'temporarily assume the powers and duties of the office as acting president. '\n",
      " 'Three vice presidents have briefly acted as president under the 25th '\n",
      " 'Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, '\n",
      " 'and on July 21, 2007; and Kamala Harris on November 19, 2021.The persons who '\n",
      " 'have served as vice president were born in or primarily affiliated with 27 '\n",
      " 'states plus the District of Columbia. New York has produced the most of any '\n",
      " 'state as eight have been born there and three others considered it their '\n",
      " 'home state. Most vice presidents have been in their 50s or 60s and had '\n",
      " 'political experience before assuming the office. Two vice presidents‚ÄîGeorge '\n",
      " 'Clinton and John C. Calhoun‚Äîserved under more than one president. Ill with '\n",
      " 'tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. '\n",
      " 'King, by an Act of Congress, was allowed to take the oath outside the United '\n",
      " 'States. He is the only vice president to take his oath of office in a '\n",
      " 'foreign country.Page: List of presidents of the United States by net '\n",
      " 'worthSummary: The list of presidents of the United States by net worth at '\n",
      " \"peak varies greatly. Debt and depreciation often means that presidents' net \"\n",
      " 'worth is less than $0 at the time of death. Most presidents before 1845 were '\n",
      " 'extremely wealthy, especially Andrew Jackson and George Washington.    '\n",
      " 'Presidents since 1929, when Herbert Hoover took office, have generally been '\n",
      " 'wealthier than presidents of the late nineteenth and early twentieth '\n",
      " 'centuries; with the exception of Harry S. Truman, all presidents since this '\n",
      " 'time have been millionaires. These presidents have often received income '\n",
      " 'from autobiographies and other writing. Except for Franklin D. Roosevelt and '\n",
      " 'John F. Kennedy (both of whom died while in office), all presidents '\n",
      " 'beginning with Calvin Coolidge have written autobiographies. In addition, '\n",
      " 'many presidents‚Äîincluding Bill Clinton‚Äîhave earned considerable income from '\n",
      " 'public speaking after leaving office.The richest president in history may be '\n",
      " 'Donald Trump. However, his net worth is not precisely known because the '\n",
      " 'Trump Organization is privately held.Truman was among the poorest U.S. '\n",
      " 'presidents, with a net worth considerably less than $1 million. His '\n",
      " 'financial situation contributed to the doubling of the presidential salary '\n",
      " 'to $100,000 in 1949. In addition, the presidential pension was created in '\n",
      " '1958 when Truman was again experiencing financial difficulties. Harry and '\n",
      " 'Bess Truman received the first Medicare cards in 1966 via the Social '\n",
      " 'Security Act of 1965.Page: List of presidents of the United States by home '\n",
      " 'stateSummary: These lists give the states of primary affiliation and of '\n",
      " 'birth for each president of the United '\n",
      " 'States.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Joe '\n",
      " 'Biden`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Joe BidenSummary: Joseph Robinette '\n",
      " 'Biden Jr. (  BY-d…ôn; born November 20, 1942) is an American politician who '\n",
      " 'is the 46th and current president of the United States. A member of the '\n",
      " 'Democratic Party, he previously served as the 47th vice president from 2009 '\n",
      " 'to 2017 under President Barack Obama and represented Delaware in the United '\n",
      " 'States Senate from 1973 to 2009.Born in Scranton, Pennsylvania, Biden moved '\n",
      " 'with his family to Delaware in 1953. He graduated from the University of '\n",
      " 'Delaware before earning his law degree from Syracuse University. He was '\n",
      " 'elected to the New Castle County Council in 1970 and to the U.S. Senate in '\n",
      " '1972. As a senator, Biden drafted and led the effort to pass the Violent '\n",
      " 'Crime Control and Law Enforcement Act and the Violence Against Women Act. He '\n",
      " 'also oversaw six U.S. Supreme Court confirmation hearings, including the '\n",
      " 'contentious hearings for Robert Bork and Clarence Thomas. Biden ran '\n",
      " 'unsuccessfully for the Democratic presidential nomination in 1988 and 2008. '\n",
      " 'In 2008, Obama chose Biden as his running mate, and he was a close counselor '\n",
      " 'to Obama during his two terms as vice president. In the 2020 presidential '\n",
      " 'election, Biden and his running mate, Kamala Harris, defeated incumbents '\n",
      " 'Donald Trump and Mike Pence. He became the oldest president in U.S. history, '\n",
      " 'and the first to have a female vice president.As president, Biden signed the '\n",
      " 'American Rescue Plan Act in response to the COVID-19 pandemic and subsequent '\n",
      " 'recession. He signed bipartisan bills on infrastructure and manufacturing. '\n",
      " 'He proposed the Build Back Better Act, which failed in Congress, but aspects '\n",
      " 'of which were incorporated into the Inflation Reduction Act that he signed '\n",
      " 'into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme '\n",
      " 'Court. He worked with congressional Republicans to resolve the 2023 United '\n",
      " 'States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. '\n",
      " \"In foreign policy, Biden restored America's membership in the Paris \"\n",
      " 'Agreement. He oversaw the complete withdrawal of U.S. troops from '\n",
      " 'Afghanistan that ended the war in Afghanistan, during which the Afghan '\n",
      " 'government collapsed and the Taliban seized control. He responded to the '\n",
      " 'Russian invasion of Ukraine by imposing sanctions on Russia and authorizing '\n",
      " 'civilian and military aid to Ukraine. During the Israel‚ÄìHamas war, Biden '\n",
      " 'announced military support for Israel, and condemned the actions of Hamas '\n",
      " 'and other Palestinian militants as terrorism. In April 2023, Biden announced '\n",
      " 'his candidacy for the Democratic nomination in the 2024 presidential '\n",
      " \"election.Page: Presidency of Joe BidenSummary: Joe Biden's tenure as the \"\n",
      " '46th president of the United States began with his inauguration on January '\n",
      " '20, 2021. Biden, a Democrat from Delaware who previously served as vice '\n",
      " 'president for two terms under president Barack Obama, took office following '\n",
      " 'his victory in the 2020 presidential election over Republican incumbent '\n",
      " 'president Donald Trump. Biden won the presidency with a popular vote of over '\n",
      " '81 million, the highest number of votes cast for a single United States '\n",
      " 'presidential candidate. Upon his inauguration, he became the oldest '\n",
      " 'president in American history, breaking the record set by his predecessor '\n",
      " 'Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, '\n",
      " 'and increased political polarization.On the first day of his presidency, '\n",
      " \"Biden made an effort to revert President Trump's energy policy by restoring \"\n",
      " 'U.S. participation in the Paris Agreement and revoking the permit for the '\n",
      " \"Keystone XL pipeline. He also halted funding for Trump's border wall, an \"\n",
      " 'expansion of the Mexican border wall. On his second day, he issued a series '\n",
      " 'of executive orders to reduce the impact of COVID-19, including invoking the '\n",
      " 'Defense Production Act of 1950, and set an early goal of achieving one '\n",
      " 'hundred million COVID-19 vaccinations in the United States in his first 100 '\n",
      " 'days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 '\n",
      " 'trillion stimulus bill that temporarily established expanded unemployment '\n",
      " 'insurance and sent $1,400 stimulus checks to most Americans in response to '\n",
      " 'continued economic pressure from COVID-19. He signed the bipartisan '\n",
      " 'Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden '\n",
      " 'alongside Democrats and Republicans in Congress, to invest in American '\n",
      " 'roads, bridges, public transit, ports and broadband access. Biden signed the '\n",
      " 'Juneteenth National Independence Day Act, making Juneteenth a federal '\n",
      " 'holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. '\n",
      " 'Supreme Court‚Äîthe first Black woman to serve on the court. After The Supreme '\n",
      " 'Court overturned Roe v. Wade, Biden took executive actions, such as the '\n",
      " \"signing of Executive Order 14076, to preserve and protect women's health \"\n",
      " 'rights nationwide, against abortion bans in Republican led states. Biden '\n",
      " 'proposed a significant expansion of the U.S. social safety net through the '\n",
      " 'Build Back Better Act, but those efforts, along with voting rights '\n",
      " 'legislation, failed in Congress. However, in August 2022, Biden signed the '\n",
      " 'Inflation Reduction Act of 2022, a domestic appropriations bill that '\n",
      " 'included some of the provisions of the Build Back Better Act after the '\n",
      " 'entire bill failed to pass. It included significant federal investment in '\n",
      " 'climate and domestic clean energy production, tax credits for solar panels, '\n",
      " 'electric cars and other home energy programs as well as a three-year '\n",
      " \"extension of Affordable Care Act subsidies. The administration's economic \"\n",
      " 'policies, known as \"Bidenomics\", were inspired and designed by Trickle-up '\n",
      " 'economics. Described as growing the economy from the middle out and bottom '\n",
      " 'up and growing the middle class. Biden signed the CHIPS and Science Act, '\n",
      " 'bolstering the semiconductor and manufacturing industry, the Honoring our '\n",
      " 'PACT Act, expanding health care for US veterans, the Bipartisan Safer '\n",
      " 'Communities Act and the Electoral Count Reform and Presidential Transition '\n",
      " 'Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, '\n",
      " 'which repealed the Defense of Marriage Act and codified same-sex and '\n",
      " 'interracial marriage in the United States. In response to the debt-ceiling '\n",
      " 'crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of '\n",
      " '2023, which restrains federal spending for fiscal years 2024 and 2025, '\n",
      " 'implements minor changes to SNAP and TANF, includes energy permitting '\n",
      " 'reform, claws back some IRS funding and unspent money for COVID-19, and '\n",
      " 'suspends the debt ceiling to January 1, 2025. Biden established the American '\n",
      " 'Climate Corps and created the first ever White House Office of Gun Violence '\n",
      " 'Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers '\n",
      " 'picket line during the 2023 United Auto Workers strike, making him the first '\n",
      " 'US president to visit one.The foreign policy goal of the Biden '\n",
      " 'administration is to restore the US to a \"position of trusted leadership\" '\n",
      " 'among global democracies in order to address the challenges posed by Russia '\n",
      " 'and China. In foreign policy, Biden completed the withdrawal of U.S. '\n",
      " 'military forces from Afghanistan, declaring an end to nation-building '\n",
      " 'efforts and shifting U.S. foreign policy toward strategic competition with '\n",
      " 'China and, to a lesser extent, Russia. However, during the withdrawal, the '\n",
      " 'Afghan government collapsed and the Taliban seized control, leading to Biden '\n",
      " 'receiving bipartisan criticism. He responded to the Russian invasion of '\n",
      " 'Ukraine by imposing sanctions on Russia as well as providing Ukraine with '\n",
      " 'over $100 billion in combined military, economic, and humanitarian aid. '\n",
      " 'Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi '\n",
      " 'al-Qurashi, the leader of the Islamic State, and approved a drone strike '\n",
      " 'which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed and created '\n",
      " 'AUKUS, an international security alliance, together with Australia and the '\n",
      " 'United Kingdom. Biden called for the expansion of NATO with the addition of '\n",
      " 'Finland and Sweden, and rallied NATO allies in support of Ukraine. During '\n",
      " 'the 2023 Israel‚ÄìHamas war, Biden condemned Hamas and other Palestinian '\n",
      " 'militants as terrorism and announced American military support for Israel; '\n",
      " 'Biden also showed his support and sympathy towards Palestinians affected by '\n",
      " 'the war, sent humanitarian aid, and brokered a four-day temporary pause and '\n",
      " 'hostage exchange.Page: Family of Joe BidenSummary: Joe Biden, the 46th and '\n",
      " 'current president of the United States, has family members who are prominent '\n",
      " \"in law, education, activism and politics. Biden's immediate family became \"\n",
      " 'the first family of the United States on his inauguration on January 20, '\n",
      " '2021. His immediate family circle was also the second family of the United '\n",
      " \"States from 2009 to 2017, when Biden was vice president. Biden's family is \"\n",
      " 'mostly descended from the British Isles, with most of their ancestors coming '\n",
      " 'from Ireland and England, and a smaller number descending from the French.Of '\n",
      " \"Joe Biden's sixteen great-great-grandparents, ten were born in Ireland. He \"\n",
      " 'is descended from the Blewitts of County Mayo and the Finnegans of County '\n",
      " \"Louth. One of Biden's great-great-great-grandfathers was born in Sussex, \"\n",
      " 'England, and emigrated to Maryland in the United States by 1820.Page: '\n",
      " 'Inauguration of Joe BidenSummary: The inauguration of Joe Biden as the 46th '\n",
      " 'president of the United States took place on Wednesday, January 20, 2021, '\n",
      " 'marking the start of the four-year term of Joe Biden as president and Kamala '\n",
      " 'Harris as vice president. The 59th presidential inauguration took place on '\n",
      " 'the West Front of the United States Capitol in Washington, D.C. Biden took '\n",
      " 'the presidential oath of office, before which Harris took the vice '\n",
      " 'presidential oath of office.The inauguration took place amidst extraordinary '\n",
      " 'political, public health, economic, and national security crises, including '\n",
      " \"the ongoing COVID-19 pandemic; outgoing President Donald Trump's attempts to \"\n",
      " 'overturn the 2020 United States presidential election, which provoked an '\n",
      " 'attack on the United States Capitol on January 6; '\n",
      " \"Trump'\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with \"\n",
      " '`Delaware`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: DelawareSummary: Delaware (  '\n",
      " 'DEL-…ô-wair) is a state in the northeast and Mid-Atlantic regions of the '\n",
      " 'United States. It borders Maryland to its south and west, Pennsylvania to '\n",
      " 'its north, New Jersey to its northeast, and the Atlantic Ocean to its east. '\n",
      " \"The state's name derives from the adjacent Delaware Bay, which in turn was \"\n",
      " 'named after Thomas West, 3rd Baron De La Warr, an English nobleman and the '\n",
      " \"Colony of Virginia's first colonial-era governor.Delaware occupies the \"\n",
      " 'northeastern portion of the Delmarva Peninsula, and some islands and '\n",
      " 'territory within the Delaware River. It is the 2nd smallest and 6th least '\n",
      " \"populous state, but also the 6th most densely populated. Delaware's most \"\n",
      " \"populous city is Wilmington, and the state's capital is Dover, the 2nd most \"\n",
      " 'populous city in Delaware. The state is divided into three counties, the '\n",
      " 'fewest number of counties of any of the 50 U.S. states; from north to south, '\n",
      " 'the three counties are: New Castle County, Kent County, and Sussex '\n",
      " 'County.The southern two counties, Kent and Sussex counties, historically '\n",
      " 'have been predominantly agrarian economies. New Castle is more urbanized and '\n",
      " 'is considered part of the Delaware Valley metropolitan statistical area that '\n",
      " \"surrounds and includes Philadelphia, the nation's 6th most populous city. \"\n",
      " 'Delaware is considered part of the Southern United States by the U.S. Census '\n",
      " \"Bureau, but the state's geography, culture, and history are a hybrid of the \"\n",
      " 'Mid-Atlantic and Northeastern regions of the country.Before Delaware '\n",
      " 'coastline was explored and developed by Europeans in the 16th century, the '\n",
      " 'state was inhabited by several Native Americans tribes, including the Lenape '\n",
      " 'in the north and Nanticoke in the south. The state was first colonized by '\n",
      " 'Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in '\n",
      " '1631.Delaware was one of the Thirteen Colonies that participated in the '\n",
      " 'American Revolution and American Revolutionary War, in which the American '\n",
      " 'Continental Army, led by George Washington, defeated the British, ended '\n",
      " 'British colonization and establishing the United States as a sovereign and '\n",
      " 'independent nation.On December 7, 1787, Delaware was the first state to '\n",
      " 'ratify the Constitution of the United States, earning it the nickname \"The '\n",
      " 'First State\".Since the turn of the 20th century, Delaware has become an '\n",
      " 'onshore corporate haven whose corporate laws are deemed appealing to '\n",
      " 'corporations; over half of all New York Stock Exchange-listed corporations '\n",
      " 'and over three-fifths of the Fortune 500 is legally incorporated in the '\n",
      " 'state.Page: Delaware City, DelawareSummary: Delaware City is a city in New '\n",
      " 'Castle County, Delaware, United States. The population was 1,885 as of 2020. '\n",
      " 'It is a small port town on the eastern terminus of the Chesapeake and '\n",
      " 'Delaware Canal and is the location of the Forts Ferry Crossing to Fort '\n",
      " 'Delaware on Pea Patch Island.Page: Delaware RiverSummary: The Delaware River '\n",
      " 'is a major river in the Mid-Atlantic region of the United States and is the '\n",
      " 'longest free-flowing (undammed) river in the Eastern United States. From the '\n",
      " 'meeting of its branches in Hancock, New York, the river flows for 282 miles '\n",
      " '(454 km) along the borders of New York, Pennsylvania, New Jersey, and '\n",
      " 'Delaware, before emptying into Delaware Bay.The river has been recognized by '\n",
      " \"the National Wildlife Federation as one of the country's Great Waters and \"\n",
      " 'has been called the \"Lifeblood of the Northeast\" by American Rivers. Its '\n",
      " 'watershed drains an area of 13,539 square miles (35,070 km2) and provides '\n",
      " 'drinking water for 17 million people, including half of New York City via '\n",
      " 'the Delaware Aqueduct.The Delaware River has two branches that rise in the '\n",
      " 'Catskill Mountains of New York: the West Branch at Mount Jefferson in '\n",
      " 'Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware '\n",
      " 'County. The branches merge to form the main Delaware River at Hancock, New '\n",
      " 'York. Flowing south, the river remains relatively undeveloped, with 152 '\n",
      " 'miles (245 km) protected as the Upper, Middle, and Lower Delaware National '\n",
      " 'Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, '\n",
      " 'navigable, and significantly more industrial. This section forms the '\n",
      " 'backbone of the Delaware Valley metropolitan area, serving the port cities '\n",
      " 'of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river '\n",
      " 'flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the '\n",
      " \"bay's outlet to the Atlantic Ocean between Cape May and Cape Henlopen.Before \"\n",
      " 'the arrival of European settlers, the river was the homeland of the Lenape '\n",
      " 'native people. They called the river Lenapewihittuk, or Lenape River, and '\n",
      " 'Kithanne, meaning the largest river in this part of the country.In 1609, the '\n",
      " 'river was visited by a Dutch East India Company expedition led by Henry '\n",
      " 'Hudson. Hudson, an English navigator, was hired to find a western route to '\n",
      " 'Cathay (China), but his encounters set the stage for Dutch colonization of '\n",
      " 'North America in the 17th century. Early Dutch and Swedish settlements were '\n",
      " 'established along the lower section of the river and Delaware Bay. Both '\n",
      " 'colonial powers called the river the South River (Zuidrivier), compared to '\n",
      " 'the Hudson River, which was known as the North River. After the English '\n",
      " 'expelled the Dutch and took control of the New Netherland colony in 1664, '\n",
      " 'the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, '\n",
      " \"an English nobleman and the Virginia colony's first royal governor, who \"\n",
      " 'defended the colony during the First Anglo-Powhatan War.Page: University of '\n",
      " 'DelawareSummary: The University of Delaware (colloquially known as UD or '\n",
      " 'Delaware) is a privately governed, state-assisted land-grant research '\n",
      " 'university located in Newark, Delaware. UD is the largest university in '\n",
      " \"Delaware. It offers three associate's programs, 148 bachelor's programs, 121 \"\n",
      " \"master's programs (with 13 joint degrees), and 55 doctoral programs across \"\n",
      " 'its eight colleges. The main campus is in Newark, with satellite campuses in '\n",
      " 'Dover, Wilmington, Lewes, and Georgetown. It is considered a large '\n",
      " 'institution with approximately 18,200 undergraduate and 4,200 graduate '\n",
      " 'students. It is a privately governed university which receives public '\n",
      " 'funding for being a land-grant, sea-grant, and space-grant state-supported '\n",
      " 'research institution.UD is classified among \"R1: Doctoral Universities ‚Äì '\n",
      " 'Very high research activity\". According to the National Science Foundation, '\n",
      " 'UD spent $186 million on research and development in 2018, ranking it 119th '\n",
      " 'in the nation.  It is recognized with the Community Engagement '\n",
      " 'Classification by the Carnegie Foundation for the Advancement of Teaching.UD '\n",
      " 'students, alumni, and sports teams are known as the \"Fightin\\' Blue Hens\", '\n",
      " 'more commonly shortened to \"Blue Hens\", and the school colors are Delaware '\n",
      " \"blue and gold. UD sponsors 21 men's and women's NCAA Division-I sports teams \"\n",
      " 'and have competed in the Colonial Athletic Association (CAA) since '\n",
      " '2001.Page: LenapeSummary: The Lenape (English: , , ; Lenape languages: '\n",
      " '[l…ônaÀêpe]), also called the Lenni Lenape and Delaware people, are an '\n",
      " 'Indigenous people of the Northeastern Woodlands, who live in the United '\n",
      " \"States and Canada.The Lenape's historical territory includes present-day \"\n",
      " 'northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions '\n",
      " 'of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, '\n",
      " 'western Long Island, and the lower Hudson Valley in New York state. Today '\n",
      " 'they are based in Oklahoma, Wisconsin, and Ontario.During the last decades '\n",
      " 'of the 18th century, European settlers and the effects of the American '\n",
      " 'Revolutionary War displaced most Lenape from their homelands and pushed them '\n",
      " 'north and west. In the 1860s, under the Indian removal policy, the U.S. '\n",
      " 'federal government relocated most Lenape remaining in the Eastern United '\n",
      " 'States to the Indian Territory and surrounding regions. Lenape people '\n",
      " 'currently belong to the Delaware Nation and Delaware Tribe of Indians in '\n",
      " 'Oklahoma, the Stockbridge‚ÄìMunsee Community in Wisconsin, and the '\n",
      " 'Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of '\n",
      " 'Six Nations in '\n",
      " 'Ontario.\\x1b[0m---------------------------------------------------------------------------``````outputBadRequestError                           '\n",
      " 'Traceback (most recent call last)``````outputCell In[11], line 14      1 '\n",
      " 'agent = (      2     {      3         \"input\": itemgetter(\"input\"),   '\n",
      " '(...)     10     | OpenAIFunctionsAgentOutputParser()     11 )     13 '\n",
      " 'agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)---> '\n",
      " '14 agent_executor.invoke(     15     {     16         \"input\": \"Who is the '\n",
      " \"current US president? What's their home state? What's their home state's \"\n",
      " 'bird? What\\'s that bird\\'s scientific name?\"     17     }     18 '\n",
      " ')``````outputFile ~/langchain/libs/langchain/langchain/chains/base.py:162, '\n",
      " 'in Chain.invoke(self, input, config, **kwargs)    160 except BaseException '\n",
      " 'as e:    161     run_manager.on_chain_error(e)--> 162     raise e    163 '\n",
      " 'run_manager.on_chain_end(outputs)    164 final_outputs: Dict[str, Any] = '\n",
      " 'self.prep_outputs(    165     inputs, outputs, return_only_outputs    166 '\n",
      " ')``````outputFile ~/langchain/libs/langchain/langchain/chains/base.py:156, '\n",
      " 'in Chain.invoke(self, input, config, **kwargs)    149 run_manager = '\n",
      " 'callback_manager.on_chain_start(    150     dumpd(self),    151     '\n",
      " 'inputs,    152     name=run_name,    153 )    154 try:    155     outputs = '\n",
      " '(--> 156         self._call(inputs, run_manager=run_manager)    157         '\n",
      " 'if new_arg_supported    158         else self._call(inputs)    159     )    '\n",
      " '160 except BaseException as e:    161     '\n",
      " 'run_manager.on_chain_error(e)``````outputFile '\n",
      " '~/langchain/libs/langchain/langchain/agents/agent.py:1391, in '\n",
      " 'AgentExecutor._call(self, inputs, run_manager)   1389 # We now enter the '\n",
      " 'agent loop (until it returns something).   1390 while '\n",
      " 'self._should_continue(iterations, time_elapsed):-> 1391     next_step_output '\n",
      " '= self._take_next_step(   1392         name_to_tool_map,   1393         '\n",
      " 'color_mapping,   1394         inputs,   1395         intermediate_steps,   '\n",
      " '1396         run_manager=run_manager,   1397     )   1398     if '\n",
      " 'isinstance(next_step_output, AgentFinish):   1399         return '\n",
      " 'self._return(   1400             next_step_output, intermediate_steps, '\n",
      " 'run_manager=run_manager   1401         )``````outputFile '\n",
      " '~/langchain/libs/langchain/langchain/agents/agent.py:1097, in '\n",
      " 'AgentExecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, '\n",
      " 'intermediate_steps, run_manager)   1088 def _take_next_step(   1089     '\n",
      " 'self,   1090     name_to_tool_map: Dict[str, BaseTool],   (...)   1094     '\n",
      " 'run_manager: Optional[CallbackManagerForChainRun] = None,   1095 ) -> '\n",
      " 'Union[AgentFinish, List[Tuple[AgentAction, str]]]:   1096     return '\n",
      " 'self._consume_next_step(-> 1097         [   1098             a   '\n",
      " '1099             for a in self._iter_next_step(   1100                 '\n",
      " 'name_to_tool_map,   1101                 color_mapping,   '\n",
      " '1102                 inputs,   1103                 intermediate_steps,   '\n",
      " '1104                 run_manager,   1105             )   1106         ]   '\n",
      " '1107     )``````outputFile '\n",
      " '~/langchain/libs/langchain/langchain/agents/agent.py:1097, in '\n",
      " '<listcomp>(.0)   1088 def _take_next_step(   1089     self,   1090     '\n",
      " 'name_to_tool_map: Dict[str, BaseTool],   (...)   1094     run_manager: '\n",
      " 'Optional[CallbackManagerForChainRun] = None,   1095 ) -> Union[AgentFinish, '\n",
      " 'List[Tuple[AgentAction, str]]]:   1096     return self._consume_next_step(-> '\n",
      " '1097         [   1098             a   1099             for a in '\n",
      " 'self._iter_next_step(   1100                 name_to_tool_map,   '\n",
      " '1101                 color_mapping,   1102                 inputs,   '\n",
      " '1103                 intermediate_steps,   1104                 '\n",
      " 'run_manager,   1105             )   1106         ]   1107     '\n",
      " ')``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1125, '\n",
      " 'in AgentExecutor._iter_next_step(self, name_to_tool_map, color_mapping, '\n",
      " 'inputs, intermediate_steps, run_manager)   1122     intermediate_steps = '\n",
      " 'self._prepare_intermediate_steps(intermediate_steps)   1124     # Call the '\n",
      " 'LLM to see what to do.-> 1125     output = self.agent.plan(   1126         '\n",
      " 'intermediate_steps,   1127         callbacks=run_manager.get_child() if '\n",
      " 'run_manager else None,   1128         **inputs,   1129     )   1130 except '\n",
      " 'OutputParserException as e:   1131     if '\n",
      " 'isinstance(self.handle_parsing_errors, bool):``````outputFile '\n",
      " '~/langchain/libs/langchain/langchain/agents/agent.py:387, in '\n",
      " 'RunnableAgent.plan(self, intermediate_steps, callbacks, **kwargs)    381 # '\n",
      " 'Use streaming to make sure that the underlying LLM is invoked in a '\n",
      " 'streaming    382 # fashion to make it possible to get access to the '\n",
      " 'individual LLM tokens    383 # when using stream_log with the Agent '\n",
      " 'Executor.    384 # Because the response from the plan is not a generator, we '\n",
      " 'need to    385 # accumulate the output into final output and return that.    '\n",
      " '386 final_output: Any = None--> 387 for chunk in '\n",
      " 'self.runnable.stream(inputs, config={\"callbacks\": callbacks}):    388     if '\n",
      " 'final_output is None:    389         final_output = chunk``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:2424, in '\n",
      " 'RunnableSequence.stream(self, input, config, **kwargs)   2418 def stream(   '\n",
      " '2419     self,   2420     input: Input,   2421     config: '\n",
      " 'Optional[RunnableConfig] = None,   2422     **kwargs: Optional[Any],   2423 '\n",
      " ') -> Iterator[Output]:-> 2424     yield from self.transform(iter([input]), '\n",
      " 'config, **kwargs)``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:2411, in '\n",
      " 'RunnableSequence.transform(self, input, config, **kwargs)   2405 def '\n",
      " 'transform(   2406     self,   2407     input: Iterator[Input],   2408     '\n",
      " 'config: Optional[RunnableConfig] = None,   2409     **kwargs: '\n",
      " 'Optional[Any],   2410 ) -> Iterator[Output]:-> 2411     yield from '\n",
      " 'self._transform_stream_with_config(   2412         input,   2413         '\n",
      " 'self._transform,   2414         patch_config(config, run_name=(config or '\n",
      " '{}).get(\"run_name\") or self.name),   2415         **kwargs,   2416     '\n",
      " ')``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:1497, in '\n",
      " 'Runnable._transform_stream_with_config(self, input, transformer, config, '\n",
      " 'run_type, **kwargs)   1495 try:   1496     while True:-> 1497         chunk: '\n",
      " 'Output = context.run(next, iterator)  # type: ignore   1498         yield '\n",
      " 'chunk   1499         if final_output_supported:``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:2375, in '\n",
      " 'RunnableSequence._transform(self, input, run_manager, config)   2366 for '\n",
      " 'step in steps:   2367     final_pipeline = step.transform(   2368         '\n",
      " 'final_pipeline,   2369         patch_config(   (...)   2372         ),   '\n",
      " '2373     )-> 2375 for output in final_pipeline:   2376     yield '\n",
      " 'output``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:1035, in '\n",
      " 'Runnable.transform(self, input, config, **kwargs)   1032 final: Input   1033 '\n",
      " 'got_first_val = False-> 1035 for chunk in input:   1036     if not '\n",
      " 'got_first_val:   1037         final = chunk``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:3991, in '\n",
      " 'RunnableBindingBase.transform(self, input, config, **kwargs)   3985 def '\n",
      " 'transform(   3986     self,   3987     input: Iterator[Input],   3988     '\n",
      " 'config: Optional[RunnableConfig] = None,   3989     **kwargs: Any,   3990 ) '\n",
      " '-> Iterator[Output]:-> 3991     yield from self.bound.transform(   '\n",
      " '3992         input,   3993         self._merge_configs(config),   '\n",
      " '3994         **{**self.kwargs, **kwargs},   3995     )``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:1045, in '\n",
      " 'Runnable.transform(self, input, config, **kwargs)   1042         final = '\n",
      " 'final + chunk  # type: ignore[operator]   1044 if got_first_val:-> 1045     '\n",
      " 'yield from self.stream(final, config, **kwargs)``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/language_models/chat_models.py:249, in '\n",
      " 'BaseChatModel.stream(self, input, config, stop, **kwargs)    242 except '\n",
      " 'BaseException as e:    243     run_manager.on_llm_error(    244         '\n",
      " 'e,    245         response=LLMResult(    246             '\n",
      " 'generations=[[generation]] if generation else []    247         ),    '\n",
      " '248     )--> 249     raise e    250 else:    251     '\n",
      " 'run_manager.on_llm_end(LLMResult(generations=[[generation]]))``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/language_models/chat_models.py:233, in '\n",
      " 'BaseChatModel.stream(self, input, config, stop, **kwargs)    231 generation: '\n",
      " 'Optional[ChatGenerationChunk] = None    232 try:--> 233     for chunk in '\n",
      " 'self._stream(    234         messages, stop=stop, run_manager=run_manager, '\n",
      " '**kwargs    235     ):    236         yield chunk.message    237         if '\n",
      " 'generation is None:``````outputFile '\n",
      " '~/langchain/libs/partners/openai/langchain_openai/chat_models/base.py:403, '\n",
      " 'in ChatOpenAI._stream(self, messages, stop, run_manager, **kwargs)    400 '\n",
      " 'params = {**params, **kwargs, \"stream\": True}    402 default_chunk_class = '\n",
      " 'AIMessageChunk--> 403 for chunk in '\n",
      " 'self.client.create(messages=message_dicts, **params):    404     if not '\n",
      " 'isinstance(chunk, dict):    405         chunk = chunk.dict()``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/_utils/_utils.py:271, '\n",
      " 'in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)    '\n",
      " '269             msg = f\"Missing required argument: {quote(missing[0])}\"    '\n",
      " '270     raise TypeError(msg)--> 271 return func(*args, '\n",
      " '**kwargs)``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/resources/chat/completions.py:648, '\n",
      " 'in Completions.create(self, messages, model, frequency_penalty, '\n",
      " 'function_call, functions, logit_bias, logprobs, max_tokens, n, '\n",
      " 'presence_penalty, response_format, seed, stop, stream, temperature, '\n",
      " 'tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, '\n",
      " 'extra_body, timeout)    599 @required_args([\"messages\", \"model\"], '\n",
      " '[\"messages\", \"model\", \"stream\"])    600 def create(    601     self,   '\n",
      " '(...)    646     timeout: float | httpx.Timeout | None | NotGiven = '\n",
      " 'NOT_GIVEN,    647 ) -> ChatCompletion | Stream[ChatCompletionChunk]:--> '\n",
      " '648     return self._post(    649         \"/chat/completions\",    '\n",
      " '650         body=maybe_transform(    651             {    '\n",
      " '652                 \"messages\": messages,    653                 \"model\": '\n",
      " 'model,    654                 \"frequency_penalty\": frequency_penalty,    '\n",
      " '655                 \"function_call\": function_call,    656                 '\n",
      " '\"functions\": functions,    657                 \"logit_bias\": logit_bias,    '\n",
      " '658                 \"logprobs\": logprobs,    659                 '\n",
      " '\"max_tokens\": max_tokens,    660                 \"n\": n,    '\n",
      " '661                 \"presence_penalty\": presence_penalty,    '\n",
      " '662                 \"response_format\": response_format,    '\n",
      " '663                 \"seed\": seed,    664                 \"stop\": stop,    '\n",
      " '665                 \"stream\": stream,    666                 \"temperature\": '\n",
      " 'temperature,    667                 \"tool_choice\": tool_choice,    '\n",
      " '668                 \"tools\": tools,    669                 \"top_logprobs\": '\n",
      " 'top_logprobs,    670                 \"top_p\": top_p,    671                 '\n",
      " '\"user\": user,    672             },    673             '\n",
      " 'completion_create_params.CompletionCreateParams,    674         ),    '\n",
      " '675         options=make_request_options(    676             '\n",
      " 'extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, '\n",
      " 'timeout=timeout    677         ),    678         cast_to=ChatCompletion,    '\n",
      " '679         stream=stream or False,    680         '\n",
      " 'stream_cls=Stream[ChatCompletionChunk],    681     )``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:1179, '\n",
      " 'in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, '\n",
      " 'stream_cls)   1165 def post(   1166     self,   1167     path: str,   '\n",
      " '(...)   1174     stream_cls: type[_StreamT] | None = None,   1175 ) -> '\n",
      " 'ResponseT | _StreamT:   1176     opts = FinalRequestOptions.construct(   '\n",
      " '1177         method=\"post\", url=path, json_data=body, '\n",
      " 'files=to_httpx_files(files), **options   1178     )-> 1179     return '\n",
      " 'cast(ResponseT, self.request(cast_to, opts, stream=stream, '\n",
      " 'stream_cls=stream_cls))``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:868, in '\n",
      " 'SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, '\n",
      " 'stream_cls)    859 def request(    860     self,    861     cast_to: '\n",
      " 'Type[ResponseT],   (...)    866     stream_cls: type[_StreamT] | None = '\n",
      " 'None,    867 ) -> ResponseT | _StreamT:--> 868     return self._request(    '\n",
      " '869         cast_to=cast_to,    870         options=options,    871         '\n",
      " 'stream=stream,    872         stream_cls=stream_cls,    873         '\n",
      " 'remaining_retries=remaining_retries,    874     )``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:959, in '\n",
      " 'SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, '\n",
      " 'stream_cls)    956         err.response.read()    958     '\n",
      " 'log.debug(\"Re-raising status error\")--> 959     raise '\n",
      " 'self._make_status_error_from_response(err.response) from None    961 return '\n",
      " 'self._process_response(    962     cast_to=cast_to,    963     '\n",
      " 'options=options,   (...)    966     stream_cls=stream_cls,    967 '\n",
      " \")``````outputBadRequestError: Error code: 400 - {'error': {'message': \"\n",
      " '\"This model\\'s maximum context length is 4097 tokens. However, your messages '\n",
      " 'resulted in 5487 tokens (5419 in the messages, 68 in the functions). Please '\n",
      " 'reduce the length of the messages or functions.\", \\'type\\': '\n",
      " \"'invalid_request_error', 'param': 'messages', 'code': \"\n",
      " \"'context_length_exceeded'}}tipLangSmith traceUnfortunately we run out of \"\n",
      " \"space in our model's context window before we the agent can get to the final \"\n",
      " \"answer. Now let's add some prompt handling logic. To keep things simple, if \"\n",
      " \"our messages have too many tokens we'll start dropping the earliest AI, \"\n",
      " 'Function message pairs (this is the model tool invocation message and the '\n",
      " 'subsequent tool output message) in the chat history.def '\n",
      " 'condense_prompt(prompt: ChatPromptValue) -> ChatPromptValue:    messages = '\n",
      " 'prompt.to_messages()    num_tokens = '\n",
      " 'llm.get_num_tokens_from_messages(messages)    ai_function_messages = '\n",
      " 'messages[2:]    while num_tokens > 4_000:        ai_function_messages = '\n",
      " 'ai_function_messages[2:]        num_tokens = '\n",
      " 'llm.get_num_tokens_from_messages(            messages[:2] + '\n",
      " 'ai_function_messages        )    messages = messages[:2] + '\n",
      " 'ai_function_messages    return ChatPromptValue(messages=messages)agent = '\n",
      " '(    {        \"input\": itemgetter(\"input\"),        \"agent_scratchpad\": '\n",
      " 'lambda x: format_to_openai_function_messages(            '\n",
      " 'x[\"intermediate_steps\"]        ),    }    | prompt    | condense_prompt    | '\n",
      " 'llm.bind_functions(tools)    | '\n",
      " 'OpenAIFunctionsAgentOutputParser())agent_executor = '\n",
      " 'AgentExecutor(agent=agent, tools=tools, '\n",
      " 'verbose=True)agent_executor.invoke(    {        \"input\": \"Who is the current '\n",
      " \"US president? What's their home state? What's their home state's bird? \"\n",
      " 'What\\'s that bird\\'s scientific name?\"    })\\x1b[1m> Entering new '\n",
      " 'AgentExecutor chain...\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with '\n",
      " '`List of presidents of the United States`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: '\n",
      " 'List of presidents of the United StatesSummary: The president of the United '\n",
      " 'States is the head of state and head of government of the United States, '\n",
      " 'indirectly elected to a four-year term via the Electoral College. The '\n",
      " 'officeholder leads the executive branch of the federal government and is the '\n",
      " 'commander-in-chief of the United States Armed Forces. Since the office was '\n",
      " 'established in 1789, 45 men have served in 46 presidencies. The first '\n",
      " 'president, George Washington, won a unanimous vote of the Electoral College. '\n",
      " 'Grover Cleveland served two non-consecutive terms and is therefore counted '\n",
      " 'as the 22nd and 24th president of the United States, giving rise to the '\n",
      " 'discrepancy between the number of presidencies and the number of individuals '\n",
      " 'who have served as president. The incumbent president is Joe Biden.The '\n",
      " 'presidency of William Henry Harrison, who died 31 days after taking office '\n",
      " 'in 1841, was the shortest in American history. Franklin D. Roosevelt served '\n",
      " 'the longest, over twelve years, before dying early in his fourth term in '\n",
      " '1945. He is the only U.S. president to have served more than two terms. '\n",
      " 'Since the ratification of the Twenty-second Amendment to the United States '\n",
      " 'Constitution in 1951, no person may be elected president more than twice, '\n",
      " 'and no one who has served more than two years of a term to which someone '\n",
      " 'else was elected may be elected more than once.Four presidents died in '\n",
      " 'office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. '\n",
      " 'Harding, and Franklin D. Roosevelt), four were assassinated (Abraham '\n",
      " 'Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one '\n",
      " 'resigned (Richard Nixon, facing impeachment and removal from office). John '\n",
      " 'Tyler was the first vice president to assume the presidency during a '\n",
      " 'presidential term, and set the precedent that a vice president who does so '\n",
      " 'becomes the fully functioning president with his presidency.Throughout most '\n",
      " 'of its history, American politics has been dominated by political parties. '\n",
      " 'The Constitution is silent on the issue of political parties, and at the '\n",
      " 'time it came into force in 1789, no organized parties existed. Soon after '\n",
      " 'the 1st Congress convened, political factions began rallying around dominant '\n",
      " 'Washington administration officials, such as Alexander Hamilton and Thomas '\n",
      " 'Jefferson. Concerned about the capacity of political parties to destroy the '\n",
      " 'fragile unity holding the nation together, Washington remained unaffiliated '\n",
      " 'with any political faction or party throughout his eight-year presidency. He '\n",
      " 'was, and remains, the only U.S. president never affiliated with a political '\n",
      " 'party.Page: List of presidents of the United States by ageSummary: In this '\n",
      " 'list of presidents of the United States by age, the first table charts the '\n",
      " 'age of each president of the United States at the time of presidential '\n",
      " 'inauguration (first inauguration if elected to multiple and consecutive '\n",
      " 'terms), upon leaving office, and at the time of death. Where the president '\n",
      " 'is still living, their lifespan and post-presidency timespan are calculated '\n",
      " 'up to January 25, 2024.Page: List of vice presidents of the United '\n",
      " 'StatesSummary: There have been 49 vice presidents of the United States since '\n",
      " 'the office was created in 1789. Originally, the vice president was the '\n",
      " 'person who received the second-most votes for president in the Electoral '\n",
      " 'College. But after the election of 1800 produced a tie between Thomas '\n",
      " 'Jefferson and Aaron Burr, requiring the House of Representatives to choose '\n",
      " 'between them, lawmakers acted to prevent such a situation from recurring. '\n",
      " 'The Twelfth Amendment was added to the Constitution in 1804, creating the '\n",
      " 'current system where electors cast a separate ballot for the vice '\n",
      " 'presidency.The vice president is the first person in the presidential line '\n",
      " 'of succession‚Äîthat is, they assume the presidency if the president dies, '\n",
      " 'resigns, or is impeached and removed from office. Nine vice presidents have '\n",
      " 'ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, '\n",
      " 'Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, '\n",
      " \"Harry S. Truman, and Lyndon B. Johnson) through the president's death and \"\n",
      " \"one (Gerald Ford) through the president's resignation. The vice president \"\n",
      " 'also serves as the president of the Senate and may choose to cast a '\n",
      " 'tie-breaking vote on decisions made by the Senate. Vice presidents have '\n",
      " 'exercised this latter power to varying extents over the years.Before '\n",
      " 'adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the '\n",
      " 'office of the vice president could not be filled until the next '\n",
      " 'post-election inauguration. Several such vacancies occurred: seven vice '\n",
      " 'presidents died, one resigned and eight succeeded to the presidency. This '\n",
      " 'amendment allowed for a vacancy to be filled through appointment by the '\n",
      " 'president and confirmation by both chambers of the Congress. Since its '\n",
      " 'ratification, the vice presidency has been vacant twice (both in the context '\n",
      " 'of scandals surrounding the Nixon administration) and was filled both times '\n",
      " \"through this process, namely in 1973 following Spiro Agnew's resignation, \"\n",
      " 'and again in 1974 after Gerald Ford succeeded to the presidency. The '\n",
      " 'amendment also established a procedure whereby a vice president may, if the '\n",
      " 'president is unable to discharge the powers and duties of the office, '\n",
      " 'temporarily assume the powers and duties of the office as acting president. '\n",
      " 'Three vice presidents have briefly acted as president under the 25th '\n",
      " 'Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, '\n",
      " 'and on July 21, 2007; and Kamala Harris on November 19, 2021.The persons who '\n",
      " 'have served as vice president were born in or primarily affiliated with 27 '\n",
      " 'states plus the District of Columbia. New York has produced the most of any '\n",
      " 'state as eight have been born there and three others considered it their '\n",
      " 'home state. Most vice presidents have been in their 50s or 60s and had '\n",
      " 'political experience before assuming the office. Two vice presidents‚ÄîGeorge '\n",
      " 'Clinton and John C. Calhoun‚Äîserved under more than one president. Ill with '\n",
      " 'tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. '\n",
      " 'King, by an Act of Congress, was allowed to take the oath outside the United '\n",
      " 'States. He is the only vice president to take his oath of office in a '\n",
      " 'foreign country.Page: List of presidents of the United States by net '\n",
      " 'worthSummary: The list of presidents of the United States by net worth at '\n",
      " \"peak varies greatly. Debt and depreciation often means that presidents' net \"\n",
      " 'worth is less than $0 at the time of death. Most presidents before 1845 were '\n",
      " 'extremely wealthy, especially Andrew Jackson and George Washington.    '\n",
      " 'Presidents since 1929, when Herbert Hoover took office, have generally been '\n",
      " 'wealthier than presidents of the late nineteenth and early twentieth '\n",
      " 'centuries; with the exception of Harry S. Truman, all presidents since this '\n",
      " 'time have been millionaires. These presidents have often received income '\n",
      " 'from autobiographies and other writing. Except for Franklin D. Roosevelt and '\n",
      " 'John F. Kennedy (both of whom died while in office), all presidents '\n",
      " 'beginning with Calvin Coolidge have written autobiographies. In addition, '\n",
      " 'many presidents‚Äîincluding Bill Clinton‚Äîhave earned considerable income from '\n",
      " 'public speaking after leaving office.The richest president in history may be '\n",
      " 'Donald Trump. However, his net worth is not precisely known because the '\n",
      " 'Trump Organization is privately held.Truman was among the poorest U.S. '\n",
      " 'presidents, with a net worth considerably less than $1 million. His '\n",
      " 'financial situation contributed to the doubling of the presidential salary '\n",
      " 'to $100,000 in 1949. In addition, the presidential pension was created in '\n",
      " '1958 when Truman was again experiencing financial difficulties. Harry and '\n",
      " 'Bess Truman received the first Medicare cards in 1966 via the Social '\n",
      " 'Security Act of 1965.Page: List of presidents of the United States by home '\n",
      " 'stateSummary: These lists give the states of primary affiliation and of '\n",
      " 'birth for each president of the United '\n",
      " 'States.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Joe '\n",
      " 'Biden`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Joe BidenSummary: Joseph Robinette '\n",
      " 'Biden Jr. (  BY-d…ôn; born November 20, 1942) is an American politician who '\n",
      " 'is the 46th and current president of the United States. A member of the '\n",
      " 'Democratic Party, he previously served as the 47th vice president from 2009 '\n",
      " 'to 2017 under President Barack Obama and represented Delaware in the United '\n",
      " 'States Senate from 1973 to 2009.Born in Scranton, Pennsylvania, Biden moved '\n",
      " 'with his family to Delaware in 1953. He graduated from the University of '\n",
      " 'Delaware before earning his law degree from Syracuse University. He was '\n",
      " 'elected to the New Castle County Council in 1970 and to the U.S. Senate in '\n",
      " '1972. As a senator, Biden drafted and led the effort to pass the Violent '\n",
      " 'Crime Control and Law Enforcement Act and the Violence Against Women Act. He '\n",
      " 'also oversaw six U.S. Supreme Court confirmation hearings, including the '\n",
      " 'contentious hearings for Robert Bork and Clarence Thomas. Biden ran '\n",
      " 'unsuccessfully for the Democratic presidential nomination in 1988 and 2008. '\n",
      " 'In 2008, Obama chose Biden as his running mate, and he was a close counselor '\n",
      " 'to Obama during his two terms as vice president. In the 2020 presidential '\n",
      " 'election, Biden and his running mate, Kamala Harris, defeated incumbents '\n",
      " 'Donald Trump and Mike Pence. He became the oldest president in U.S. history, '\n",
      " 'and the first to have a female vice president.As president, Biden signed the '\n",
      " 'American Rescue Plan Act in response to the COVID-19 pandemic and subsequent '\n",
      " 'recession. He signed bipartisan bills on infrastructure and manufacturing. '\n",
      " 'He proposed the Build Back Better Act, which failed in Congress, but aspects '\n",
      " 'of which were incorporated into the Inflation Reduction Act that he signed '\n",
      " 'into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme '\n",
      " 'Court. He worked with congressional Republicans to resolve the 2023 United '\n",
      " 'States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. '\n",
      " \"In foreign policy, Biden restored America's membership in the Paris \"\n",
      " 'Agreement. He oversaw the complete withdrawal of U.S. troops from '\n",
      " 'Afghanistan that ended the war in Afghanistan, during which the Afghan '\n",
      " 'government collapsed and the Taliban seized control. He responded to the '\n",
      " 'Russian invasion of Ukraine by imposing sanctions on Russia and authorizing '\n",
      " 'civilian and military aid to Ukraine. During the Israel‚ÄìHamas war, Biden '\n",
      " 'announced military support for Israel, and condemned the actions of Hamas '\n",
      " 'and other Palestinian militants as terrorism. In April 2023, Biden announced '\n",
      " 'his candidacy for the Democratic nomination in the 2024 presidential '\n",
      " \"election.Page: Presidency of Joe BidenSummary: Joe Biden's tenure as the \"\n",
      " '46th president of the United States began with his inauguration on January '\n",
      " '20, 2021. Biden, a Democrat from Delaware who previously served as vice '\n",
      " 'president for two terms under president Barack Obama, took office following '\n",
      " 'his victory in the 2020 presidential election over Republican incumbent '\n",
      " 'president Donald Trump. Biden won the presidency with a popular vote of over '\n",
      " '81 million, the highest number of votes cast for a single United States '\n",
      " 'presidential candidate. Upon his inauguration, he became the oldest '\n",
      " 'president in American history, breaking the record set by his predecessor '\n",
      " 'Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, '\n",
      " 'and increased political polarization.On the first day of his presidency, '\n",
      " \"Biden made an effort to revert President Trump's energy policy by restoring \"\n",
      " 'U.S. participation in the Paris Agreement and revoking the permit for the '\n",
      " \"Keystone XL pipeline. He also halted funding for Trump's border wall, an \"\n",
      " 'expansion of the Mexican border wall. On his second day, he issued a series '\n",
      " 'of executive orders to reduce the impact of COVID-19, including invoking the '\n",
      " 'Defense Production Act of 1950, and set an early goal of achieving one '\n",
      " 'hundred million COVID-19 vaccinations in the United States in his first 100 '\n",
      " 'days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 '\n",
      " 'trillion stimulus bill that temporarily established expanded unemployment '\n",
      " 'insurance and sent $1,400 stimulus checks to most Americans in response to '\n",
      " 'continued economic pressure from COVID-19. He signed the bipartisan '\n",
      " 'Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden '\n",
      " 'alongside Democrats and Republicans in Congress, to invest in American '\n",
      " 'roads, bridges, public transit, ports and broadband access. Biden signed the '\n",
      " 'Juneteenth National Independence Day Act, making Juneteenth a federal '\n",
      " 'holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. '\n",
      " 'Supreme Court‚Äîthe first Black woman to serve on the court. After The Supreme '\n",
      " 'Court overturned Roe v. Wade, Biden took executive actions, such as the '\n",
      " \"signing of Executive Order 14076, to preserve and protect women's health \"\n",
      " 'rights nationwide, against abortion bans in Republican led states. Biden '\n",
      " 'proposed a significant expansion of the U.S. social safety net through the '\n",
      " 'Build Back Better Act, but those efforts, along with voting rights '\n",
      " 'legislation, failed in Congress. However, in August 2022, Biden signed the '\n",
      " 'Inflation Reduction Act of 2022, a domestic appropriations bill that '\n",
      " 'included some of the provisions of the Build Back Better Act after the '\n",
      " 'entire bill failed to pass. It included significant federal investment in '\n",
      " 'climate and domestic clean energy production, tax credits for solar panels, '\n",
      " 'electric cars and other home energy programs as well as a three-year '\n",
      " \"extension of Affordable Care Act subsidies. The administration's economic \"\n",
      " 'policies, known as \"Bidenomics\", were inspired and designed by Trickle-up '\n",
      " 'economics. Described as growing the economy from the middle out and bottom '\n",
      " 'up and growing the middle class. Biden signed the CHIPS and Science Act, '\n",
      " 'bolstering the semiconductor and manufacturing industry, the Honoring our '\n",
      " 'PACT Act, expanding health care for US veterans, the Bipartisan Safer '\n",
      " 'Communities Act and the Electoral Count Reform and Presidential Transition '\n",
      " 'Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, '\n",
      " 'which repealed the Defense of Marriage Act and codified same-sex and '\n",
      " 'interracial marriage in the United States. In response to the debt-ceiling '\n",
      " 'crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of '\n",
      " '2023, which restrains federal spending for fiscal years 2024 and 2025, '\n",
      " 'implements minor changes to SNAP and TANF, includes energy permitting '\n",
      " 'reform, claws back some IRS funding and unspent money for COVID-19, and '\n",
      " 'suspends the debt ceiling to January 1, 2025. Biden established the American '\n",
      " 'Climate Corps and created the first ever White House Office of Gun Violence '\n",
      " 'Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers '\n",
      " 'picket line during the 2023 United Auto Workers strike, making him the first '\n",
      " 'US president to visit one.The foreign policy goal of the Biden '\n",
      " 'administration is to restore the US to a \"position of trusted leadership\" '\n",
      " 'among global democracies in order to address the challenges posed by Russia '\n",
      " 'and China. In foreign policy, Biden completed the withdrawal of U.S. '\n",
      " 'military forces from Afghanistan, declaring an end to nation-building '\n",
      " 'efforts and shifting U.S. foreign policy toward strategic competition with '\n",
      " 'China and, to a lesser extent, Russia. However, during the withdrawal, the '\n",
      " 'Afghan government collapsed and the Taliban seized control, leading to Biden '\n",
      " 'receiving bipartisan criticism. He responded to the Russian invasion of '\n",
      " 'Ukraine by imposing sanctions on Russia as well as providing Ukraine with '\n",
      " 'over $100 billion in combined military, economic, and humanitarian aid. '\n",
      " 'Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi '\n",
      " 'al-Qurashi, the leader of the Islamic State, and approved a drone strike '\n",
      " 'which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed and created '\n",
      " 'AUKUS, an international security alliance, together with Australia and the '\n",
      " 'United Kingdom. Biden called for the expansion of NATO with the addition of '\n",
      " 'Finland and Sweden, and rallied NATO allies in support of Ukraine. During '\n",
      " 'the 2023 Israel‚ÄìHamas war, Biden condemned Hamas and other Palestinian '\n",
      " 'militants as terrorism and announced American military support for Israel; '\n",
      " 'Biden also showed his support and sympathy towards Palestinians affected by '\n",
      " 'the war, sent humanitarian aid, and brokered a four-day temporary pause and '\n",
      " 'hostage exchange.Page: Family of Joe BidenSummary: Joe Biden, the 46th and '\n",
      " 'current president of the United States, has family members who are prominent '\n",
      " \"in law, education, activism and politics. Biden's immediate family became \"\n",
      " 'the first family of the United States on his inauguration on January 20, '\n",
      " '2021. His immediate family circle was also the second family of the United '\n",
      " \"States from 2009 to 2017, when Biden was vice president. Biden's family is \"\n",
      " 'mostly descended from the British Isles, with most of their ancestors coming '\n",
      " 'from Ireland and England, and a smaller number descending from the French.Of '\n",
      " \"Joe Biden's sixteen great-great-grandparents, ten were born in Ireland. He \"\n",
      " 'is descended from the Blewitts of County Mayo and the Finnegans of County '\n",
      " \"Louth. One of Biden's great-great-great-grandfathers was born in Sussex, \"\n",
      " 'England, and emigrated to Maryland in the United States by 1820.Page: '\n",
      " 'Inauguration of Joe BidenSummary: The inauguration of Joe Biden as the 46th '\n",
      " 'president of the United States took place on Wednesday, January 20, 2021, '\n",
      " 'marking the start of the four-year term of Joe Biden as president and Kamala '\n",
      " 'Harris as vice president. The 59th presidential inauguration took place on '\n",
      " 'the West Front of the United States Capitol in Washington, D.C. Biden took '\n",
      " 'the presidential oath of office, before which Harris took the vice '\n",
      " 'presidential oath of office.The inauguration took place amidst extraordinary '\n",
      " 'political, public health, economic, and national security crises, including '\n",
      " \"the ongoing COVID-19 pandemic; outgoing President Donald Trump's attempts to \"\n",
      " 'overturn the 2020 United States presidential election, which provoked an '\n",
      " 'attack on the United States Capitol on January 6; '\n",
      " \"Trump'\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with \"\n",
      " '`Delaware`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: DelawareSummary: Delaware (  '\n",
      " 'DEL-…ô-wair) is a state in the northeast and Mid-Atlantic regions of the '\n",
      " 'United States. It borders Maryland to its south and west, Pennsylvania to '\n",
      " 'its north, New Jersey to its northeast, and the Atlantic Ocean to its east. '\n",
      " \"The state's name derives from the adjacent Delaware Bay, which in turn was \"\n",
      " 'named after Thomas West, 3rd Baron De La Warr, an English nobleman and the '\n",
      " \"Colony of Virginia's first colonial-era governor.Delaware occupies the \"\n",
      " 'northeastern portion of the Delmarva Peninsula, and some islands and '\n",
      " 'territory within the Delaware River. It is the 2nd smallest and 6th least '\n",
      " \"populous state, but also the 6th most densely populated. Delaware's most \"\n",
      " \"populous city is Wilmington, and the state's capital is Dover, the 2nd most \"\n",
      " 'populous city in Delaware. The state is divided into three counties, the '\n",
      " 'fewest number of counties of any of the 50 U.S. states; from north to south, '\n",
      " 'the three counties are: New Castle County, Kent County, and Sussex '\n",
      " 'County.The southern two counties, Kent and Sussex counties, historically '\n",
      " 'have been predominantly agrarian economies. New Castle is more urbanized and '\n",
      " 'is considered part of the Delaware Valley metropolitan statistical area that '\n",
      " \"surrounds and includes Philadelphia, the nation's 6th most populous city. \"\n",
      " 'Delaware is considered part of the Southern United States by the U.S. Census '\n",
      " \"Bureau, but the state's geography, culture, and history are a hybrid of the \"\n",
      " 'Mid-Atlantic and Northeastern regions of the country.Before Delaware '\n",
      " 'coastline was explored and developed by Europeans in the 16th century, the '\n",
      " 'state was inhabited by several Native Americans tribes, including the Lenape '\n",
      " 'in the north and Nanticoke in the south. The state was first colonized by '\n",
      " 'Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in '\n",
      " '1631.Delaware was one of the Thirteen Colonies that participated in the '\n",
      " 'American Revolution and American Revolutionary War, in which the American '\n",
      " 'Continental Army, led by George Washington, defeated the British, ended '\n",
      " 'British colonization and establishing the United States as a sovereign and '\n",
      " 'independent nation.On December 7, 1787, Delaware was the first state to '\n",
      " 'ratify the Constitution of the United States, earning it the nickname \"The '\n",
      " 'First State\".Since the turn of the 20th century, Delaware has become an '\n",
      " 'onshore corporate haven whose corporate laws are deemed appealing to '\n",
      " 'corporations; over half of all New York Stock Exchange-listed corporations '\n",
      " 'and over three-fifths of the Fortune 500 is legally incorporated in the '\n",
      " 'state.Page: Delaware City, DelawareSummary: Delaware City is a city in New '\n",
      " 'Castle County, Delaware, United States. The population was 1,885 as of 2020. '\n",
      " 'It is a small port town on the eastern terminus of the Chesapeake and '\n",
      " 'Delaware Canal and is the location of the Forts Ferry Crossing to Fort '\n",
      " 'Delaware on Pea Patch Island.Page: Delaware RiverSummary: The Delaware River '\n",
      " 'is a major river in the Mid-Atlantic region of the United States and is the '\n",
      " 'longest free-flowing (undammed) river in the Eastern United States. From the '\n",
      " 'meeting of its branches in Hancock, New York, the river flows for 282 miles '\n",
      " '(454 km) along the borders of New York, Pennsylvania, New Jersey, and '\n",
      " 'Delaware, before emptying into Delaware Bay.The river has been recognized by '\n",
      " \"the National Wildlife Federation as one of the country's Great Waters and \"\n",
      " 'has been called the \"Lifeblood of the Northeast\" by American Rivers. Its '\n",
      " 'watershed drains an area of 13,539 square miles (35,070 km2) and provides '\n",
      " 'drinking water for 17 million people, including half of New York City via '\n",
      " 'the Delaware Aqueduct.The Delaware River has two branches that rise in the '\n",
      " 'Catskill Mountains of New York: the West Branch at Mount Jefferson in '\n",
      " 'Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware '\n",
      " 'County. The branches merge to form the main Delaware River at Hancock, New '\n",
      " 'York. Flowing south, the river remains relatively undeveloped, with 152 '\n",
      " 'miles (245 km) protected as the Upper, Middle, and Lower Delaware National '\n",
      " 'Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, '\n",
      " 'navigable, and significantly more industrial. This section forms the '\n",
      " 'backbone of the Delaware Valley metropolitan area, serving the port cities '\n",
      " 'of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river '\n",
      " 'flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the '\n",
      " \"bay's outlet to the Atlantic Ocean between Cape May and Cape Henlopen.Before \"\n",
      " 'the arrival of European settlers, the river was the homeland of the Lenape '\n",
      " 'native people. They called the river Lenapewihittuk, or Lenape River, and '\n",
      " 'Kithanne, meaning the largest river in this part of the country.In 1609, the '\n",
      " 'river was visited by a Dutch East India Company expedition led by Henry '\n",
      " 'Hudson. Hudson, an English navigator, was hired to find a western route to '\n",
      " 'Cathay (China), but his encounters set the stage for Dutch colonization of '\n",
      " 'North America in the 17th century. Early Dutch and Swedish settlements were '\n",
      " 'established along the lower section of the river and Delaware Bay. Both '\n",
      " 'colonial powers called the river the South River (Zuidrivier), compared to '\n",
      " 'the Hudson River, which was known as the North River. After the English '\n",
      " 'expelled the Dutch and took control of the New Netherland colony in 1664, '\n",
      " 'the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, '\n",
      " \"an English nobleman and the Virginia colony's first royal governor, who \"\n",
      " 'defended the colony during the First Anglo-Powhatan War.Page: University of '\n",
      " 'DelawareSummary: The University of Delaware (colloquially known as UD or '\n",
      " 'Delaware) is a privately governed, state-assisted land-grant research '\n",
      " 'university located in Newark, Delaware. UD is the largest university in '\n",
      " \"Delaware. It offers three associate's programs, 148 bachelor's programs, 121 \"\n",
      " \"master's programs (with 13 joint degrees), and 55 doctoral programs across \"\n",
      " 'its eight colleges. The main campus is in Newark, with satellite campuses in '\n",
      " 'Dover, Wilmington, Lewes, and Georgetown. It is considered a large '\n",
      " 'institution with approximately 18,200 undergraduate and 4,200 graduate '\n",
      " 'students. It is a privately governed university which receives public '\n",
      " 'funding for being a land-grant, sea-grant, and space-grant state-supported '\n",
      " 'research institution.UD is classified among \"R1: Doctoral Universities ‚Äì '\n",
      " 'Very high research activity\". According to the National Science Foundation, '\n",
      " 'UD spent $186 million on research and development in 2018, ranking it 119th '\n",
      " 'in the nation.  It is recognized with the Community Engagement '\n",
      " 'Classification by the Carnegie Foundation for the Advancement of Teaching.UD '\n",
      " 'students, alumni, and sports teams are known as the \"Fightin\\' Blue Hens\", '\n",
      " 'more commonly shortened to \"Blue Hens\", and the school colors are Delaware '\n",
      " \"blue and gold. UD sponsors 21 men's and women's NCAA Division-I sports teams \"\n",
      " 'and have competed in the Colonial Athletic Association (CAA) since '\n",
      " '2001.Page: LenapeSummary: The Lenape (English: , , ; Lenape languages: '\n",
      " '[l…ônaÀêpe]), also called the Lenni Lenape and Delaware people, are an '\n",
      " 'Indigenous people of the Northeastern Woodlands, who live in the United '\n",
      " \"States and Canada.The Lenape's historical territory includes present-day \"\n",
      " 'northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions '\n",
      " 'of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, '\n",
      " 'western Long Island, and the lower Hudson Valley in New York state. Today '\n",
      " 'they are based in Oklahoma, Wisconsin, and Ontario.During the last decades '\n",
      " 'of the 18th century, European settlers and the effects of the American '\n",
      " 'Revolutionary War displaced most Lenape from their homelands and pushed them '\n",
      " 'north and west. In the 1860s, under the Indian removal policy, the U.S. '\n",
      " 'federal government relocated most Lenape remaining in the Eastern United '\n",
      " 'States to the Indian Territory and surrounding regions. Lenape people '\n",
      " 'currently belong to the Delaware Nation and Delaware Tribe of Indians in '\n",
      " 'Oklahoma, the Stockbridge‚ÄìMunsee Community in Wisconsin, and the '\n",
      " 'Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of '\n",
      " 'Six Nations in Ontario.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with '\n",
      " '`Blue hen chicken`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Delaware Blue HenSummary: '\n",
      " 'The Delaware Blue Hen or Blue Hen of Delaware is a blue strain of American '\n",
      " 'gamecock. Under the name Blue Hen Chicken it is the official bird of the '\n",
      " 'State of Delaware. It is the emblem or mascot of several institutions in the '\n",
      " 'state, among them the sports teams of the University of Delaware.Page: '\n",
      " \"Delaware Fightin' Blue HensSummary: The Delaware Fightin' Blue Hens are the \"\n",
      " 'athletic teams of the University of Delaware (UD) of Newark, Delaware, in '\n",
      " 'the United States. The Blue Hens compete in the Football Championship '\n",
      " 'Subdivision (FCS) of Division I of the National Collegiate Athletic '\n",
      " 'Association (NCAA) as members of the Coastal Athletic Association and its '\n",
      " 'technically separate football league, CAA Football.On November 28, 2023, UD '\n",
      " 'and Conference USA (CUSA) jointly announced that UD would start a transition '\n",
      " 'to the Division I Football Bowl Subdivision (FBS) in 2024 and join CUSA in '\n",
      " '2025. UD will continue to compete in both sides of the CAA in 2024‚Äì25; it '\n",
      " 'will be ineligible for the FCS playoffs due to NCAA rules for transitioning '\n",
      " 'programs, but will be eligible for all non-football CAA championships. Upon '\n",
      " 'joining CUSA, UD will be eligible for all conference championship events '\n",
      " 'except the football championship game; it will become eligible for that '\n",
      " 'event upon completing the FBS transition in 2026. At the same time, UD also '\n",
      " \"announced it would add one women's sport due to Title IX considerations, and \"\n",
      " 'would also be seeking conference homes for the seven sports that UD sponsors '\n",
      " \"but CUSA does not. The new women's sport would later be announced as ice \"\n",
      " 'hockey; UD will join College Hockey America for its first season of varsity '\n",
      " 'play in 2025‚Äì26.Page: Brahma chickenSummary: The Brahma is an American breed '\n",
      " 'of chicken. It was bred in the United States from birds imported from the '\n",
      " 'Chinese port of Shanghai,:\\u200a78\\u200a and was the principal American meat '\n",
      " 'breed from the 1850s until about 1930.Page: SilkieSummary: The Silkie (also '\n",
      " 'known as the Silky or Chinese silk chicken) is a breed of chicken named for '\n",
      " 'its atypically fluffy plumage, which is said to feel like silk and satin. '\n",
      " 'The breed has several other unusual qualities, such as black skin and bones, '\n",
      " 'blue earlobes, and five toes on each foot, whereas most chickens have only '\n",
      " 'four. They are often exhibited in poultry shows, and also appear in various '\n",
      " 'colors. In addition to their distinctive physical characteristics, Silkies '\n",
      " 'are well known for their calm and friendly temperament. It is among the most '\n",
      " 'docile of poultry. Hens are also exceptionally broody, and care for young '\n",
      " 'well. Although they are fair layers themselves, laying only about three eggs '\n",
      " 'a week, they are commonly used to hatch eggs from other breeds and bird '\n",
      " 'species due to their broody nature. Silkie chickens have been bred to have a '\n",
      " 'wide variety of colors which include but are not limited to: Black, Blue, '\n",
      " 'Buff, Partridge, Splash, White, Lavender, Paint and Porcelain.Page: '\n",
      " 'Silverudd BlueSummary: The Silverudd Blue, Swedish: Silverudds Bl√•, is a '\n",
      " 'Swedish breed of chicken. It was developed by Martin Silverudd in Sm√•land, '\n",
      " 'in southern Sweden. Hens lay blue/green eggs, weighing 50‚Äì65 grams. The '\n",
      " 'flock-book for the breed is kept by the Svenska Kulturh√∂nsf√∂reningen ‚Äì the '\n",
      " 'Swedish Cultural Hen Association. It was initially known by various names '\n",
      " 'including Isbar, Blue Isbar and Svensk Gr√∂nv√§rpare, or \"Swedish green egg '\n",
      " 'layer\"; in 2016 it was renamed to \\'Silverudd Blue\\' after its '\n",
      " 'creator.\\x1b[0m\\x1b[32;1m\\x1b[1;3mThe current US president is Joe Biden. His '\n",
      " 'home state is Delaware. The home state bird of Delaware is the Delaware Blue '\n",
      " 'Hen. The scientific name of the Delaware Blue Hen is Gallus gallus '\n",
      " 'domesticus.\\x1b[0m\\x1b[1m> Finished chain.\\x1b[0m{\\'input\\': \"Who is the '\n",
      " \"current US president? What's their home state? What's their home state's \"\n",
      " 'bird? What\\'s that bird\\'s scientific name?\", \\'output\\': \\'The current US '\n",
      " 'president is Joe Biden. His home state is Delaware. The home state bird of '\n",
      " 'Delaware is the Delaware Blue Hen. The scientific name of the Delaware Blue '\n",
      " \"Hen is Gallus gallus domesticus.'}tipLangSmith traceHelp us out by providing \"\n",
      " 'feedback on this documentation page:PreviousCreate a runnable with the '\n",
      " '@chain decoratorNextMultiple '\n",
      " 'chainsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Inspect your runnables | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreInspect your runnablesOn this pageInspect your runnablesOnce you '\n",
      " 'create a runnable with LCEL, you may often want to inspect it to get a '\n",
      " 'better sense for what is going on. This notebook covers some methods for '\n",
      " \"doing so.First, let's create an example LCEL. We will create one that does \"\n",
      " 'retrieval%pip install --upgrade --quiet  langchain langchain-openai '\n",
      " 'faiss-cpu tiktokenfrom langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)model = ChatOpenAI()chain = (    '\n",
      " '{\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | '\n",
      " 'model    | StrOutputParser())Get a graph\\u200bYou can get a graph of the '\n",
      " 'runnablechain.get_graph()Print a graph\\u200bWhile that is not super legible, '\n",
      " \"you can print it to get a display that's easier to \"\n",
      " 'understandchain.get_graph().print_ascii()           '\n",
      " '+---------------------------------+                    | '\n",
      " 'Parallel<context,question>Input |                    '\n",
      " '+---------------------------------+                             '\n",
      " '**               **                                 ***                   '\n",
      " '***                            **                         **           '\n",
      " '+----------------------+              +-------------+  | '\n",
      " 'VectorStoreRetriever |              | Passthrough |  '\n",
      " '+----------------------+              +-------------+                      '\n",
      " '**               **                                      ***         '\n",
      " '***                                           **     '\n",
      " '**                                '\n",
      " '+----------------------------------+                   | '\n",
      " 'Parallel<context,question>Output |                   '\n",
      " '+----------------------------------+                                     '\n",
      " '*                                                      '\n",
      " '*                                                      '\n",
      " '*                                           '\n",
      " '+--------------------+                                 | ChatPromptTemplate '\n",
      " '|                                 '\n",
      " '+--------------------+                                            '\n",
      " '*                                                      '\n",
      " '*                                                      '\n",
      " '*                                               '\n",
      " '+------------+                                         | ChatOpenAI '\n",
      " '|                                         '\n",
      " '+------------+                                                '\n",
      " '*                                                      '\n",
      " '*                                                      '\n",
      " '*                                            '\n",
      " '+-----------------+                                    | StrOutputParser '\n",
      " '|                                    '\n",
      " '+-----------------+                                              '\n",
      " '*                                                      '\n",
      " '*                                                      '\n",
      " '*                                         '\n",
      " '+-----------------------+                              | '\n",
      " 'StrOutputParserOutput |                              '\n",
      " '+-----------------------+Get the prompts\\u200bAn important part of every '\n",
      " 'chain is the prompts that are used. You can get the prompts present in the '\n",
      " \"chain:chain.get_prompts()[ChatPromptTemplate(input_variables=['context', \"\n",
      " \"'question'], \"\n",
      " \"messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', \"\n",
      " \"'question'], template='Answer the question based only on the following \"\n",
      " \"context:\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\n'))])]Help us out by \"\n",
      " 'providing feedback on this documentation page:PreviousRoute logic based on '\n",
      " 'inputNextCreate a runnable with the @chain decoratorGet a graphPrint a '\n",
      " 'graphGet the '\n",
      " 'promptsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Get started | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageGet startedOn this pageGet startedLCEL '\n",
      " 'makes it easy to build complex chains from basic components, and supports '\n",
      " 'out of the box functionality such as streaming, parallelism, and '\n",
      " 'logging.Basic example: prompt + model + output parser\\u200bThe most basic '\n",
      " 'and common use case is chaining a prompt template and a model together. To '\n",
      " \"see how this works, let's create a chain that takes a topic and generates a \"\n",
      " 'joke:%pip install --upgrade --quiet  langchain-core langchain-community '\n",
      " 'langchain-openaiOpenAIAnthropicGoogleCohereFireworksAIMistralAITogetherAIInstall '\n",
      " 'dependenciespip install -qU langchain-openaiSet environment variablesimport '\n",
      " 'getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4\")Install '\n",
      " 'dependenciespip install -qU langchain-anthropicSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_anthropic import ChatAnthropicmodel = '\n",
      " 'ChatAnthropic(model=\"claude-3-sonnet-20240229\")Install dependenciespip '\n",
      " 'install -qU langchain-google-vertexaiSet environment variablesimport '\n",
      " 'getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_google_vertexai import ChatVertexAImodel = '\n",
      " 'ChatVertexAI(model=\"gemini-pro\")Install dependenciespip install -qU '\n",
      " 'langchain-cohereSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere '\n",
      " 'import ChatCoheremodel = ChatCohere(model=\"command-r\")Install '\n",
      " 'dependenciespip install -qU langchain-fireworksSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_fireworks import ChatFireworksmodel = '\n",
      " 'ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")Install '\n",
      " 'dependenciespip install -qU langchain-mistralaiSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_mistralai import ChatMistralAImodel = '\n",
      " 'ChatMistralAI(model=\"mistral-large-latest\")Install dependenciespip install '\n",
      " '-qU langchain-openaiSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai '\n",
      " 'import ChatOpenAImodel = ChatOpenAI(    '\n",
      " 'base_url=\"https://api.together.xyz/v1\",    '\n",
      " 'api_key=os.environ[\"TOGETHER_API_KEY\"],    '\n",
      " 'model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)# | output: false# | echo: '\n",
      " 'falsefrom langchain_openai import ChatOpenAImodel = '\n",
      " 'ChatOpenAI(model=\"gpt-4\")from langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a short joke about '\n",
      " '{topic}\")output_parser = StrOutputParser()chain = prompt | model | '\n",
      " 'output_parserchain.invoke({\"topic\": \"ice cream\"})\"Why don\\'t ice creams ever '\n",
      " 'get invited to parties?\\\\n\\\\nBecause they always drip when things heat '\n",
      " 'up!\"Notice this line of the code, where we piece together these different '\n",
      " 'components into a single chain using LCEL:chain = prompt | model | '\n",
      " 'output_parserThe | symbol is similar to a unix pipe operator, which chains '\n",
      " 'together the different components, feeding the output from one component as '\n",
      " 'input into the next component. In this chain the user input is passed to the '\n",
      " 'prompt template, then the prompt template output is passed to the model, '\n",
      " \"then the model output is passed to the output parser. Let's take a look at \"\n",
      " \"each component individually to really understand what's going on.1. \"\n",
      " 'Prompt\\u200bprompt is a BasePromptTemplate, which means it takes in a '\n",
      " 'dictionary of template variables and produces a PromptValue. A PromptValue '\n",
      " 'is a wrapper around a completed prompt that can be passed to either an LLM '\n",
      " '(which takes a string as input) or ChatModel (which takes a sequence of '\n",
      " 'messages as input). It can work with either language model type because it '\n",
      " 'defines logic both for producing BaseMessages and for producing a '\n",
      " 'string.prompt_value = prompt.invoke({\"topic\": \"ice '\n",
      " 'cream\"})prompt_valueChatPromptValue(messages=[HumanMessage(content=\\'tell me '\n",
      " 'a short joke about ice '\n",
      " \"cream')])prompt_value.to_messages()[HumanMessage(content='tell me a short \"\n",
      " \"joke about ice cream')]prompt_value.to_string()'Human: tell me a short joke \"\n",
      " \"about ice cream'2. Model\\u200bThe PromptValue is then passed to model. In \"\n",
      " 'this case our model is a ChatModel, meaning it will output a '\n",
      " 'BaseMessage.message = '\n",
      " 'model.invoke(prompt_value)messageAIMessage(content=\"Why don\\'t ice creams '\n",
      " 'ever get invited to parties?\\\\n\\\\nBecause they always bring a melt down!\")If '\n",
      " 'our model was an LLM, it would output a string.from langchain_openai import '\n",
      " 'OpenAIllm = '\n",
      " 'OpenAI(model=\"gpt-3.5-turbo-instruct\")llm.invoke(prompt_value)\\'\\\\n\\\\nRobot: '\n",
      " \"Why did the ice cream truck break down? Because it had a meltdown!'3. Output \"\n",
      " 'parser\\u200bAnd lastly we pass our model output to the output_parser, which '\n",
      " 'is a BaseOutputParser meaning it takes either a string or a\\n'\n",
      " 'BaseMessage as input. The specific StrOutputParser simply converts any input '\n",
      " 'into a string.output_parser.invoke(message)\"Why did the ice cream go to '\n",
      " \"therapy? \\\\n\\\\nBecause it had too many toppings and couldn't find its \"\n",
      " 'cone-fidence!\"4. Entire Pipeline\\u200bTo follow the steps along:We pass in '\n",
      " 'user input on the desired topic as {\"topic\": \"ice cream\"}The prompt '\n",
      " 'component takes the user input, which is then used to construct a '\n",
      " 'PromptValue after using the topic to construct the prompt. The model '\n",
      " 'component takes the generated prompt, and passes into the OpenAI LLM model '\n",
      " 'for evaluation. The generated output from the model is a ChatMessage object. '\n",
      " 'Finally, the output_parser component takes in a ChatMessage, and transforms '\n",
      " 'this into a Python string, which is returned from the invoke method. '\n",
      " 'infoNote that if you‚Äôre curious about the output of any components, you can '\n",
      " 'always test out a smaller version of the chain such as prompt or prompt | '\n",
      " 'model to see the intermediate results:input = {\"topic\": \"ice '\n",
      " 'cream\"}prompt.invoke(input)# > '\n",
      " \"ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about \"\n",
      " 'ice cream\\')])(prompt | model).invoke(input)# > AIMessage(content=\"Why did '\n",
      " \"the ice cream go to therapy?\\\\nBecause it had too many toppings and couldn't \"\n",
      " 'cone-trol itself!\")RAG Search Example\\u200bFor our next example, we want to '\n",
      " 'run a retrieval-augmented generation chain to add some context when '\n",
      " 'responding to '\n",
      " 'questions.OpenAIAnthropicGoogleCohereFireworksAIMistralAITogetherAIInstall '\n",
      " 'dependenciespip install -qU langchain-openaiSet environment variablesimport '\n",
      " 'getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_openai import ChatOpenAImodel = '\n",
      " 'ChatOpenAI(model=\"gpt-3.5-turbo-0125\")Install dependenciespip install -qU '\n",
      " 'langchain-anthropicSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_anthropic import ChatAnthropicmodel = '\n",
      " 'ChatAnthropic(model=\"claude-3-sonnet-20240229\")Install dependenciespip '\n",
      " 'install -qU langchain-google-vertexaiSet environment variablesimport '\n",
      " 'getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_google_vertexai import ChatVertexAImodel = '\n",
      " 'ChatVertexAI(model=\"gemini-pro\")Install dependenciespip install -qU '\n",
      " 'langchain-cohereSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere '\n",
      " 'import ChatCoheremodel = ChatCohere(model=\"command-r\")Install '\n",
      " 'dependenciespip install -qU langchain-fireworksSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_fireworks import ChatFireworksmodel = '\n",
      " 'ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")Install '\n",
      " 'dependenciespip install -qU langchain-mistralaiSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_mistralai import ChatMistralAImodel = '\n",
      " 'ChatMistralAI(model=\"mistral-large-latest\")Install dependenciespip install '\n",
      " '-qU langchain-openaiSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai '\n",
      " 'import ChatOpenAImodel = ChatOpenAI(    '\n",
      " 'base_url=\"https://api.together.xyz/v1\",    '\n",
      " 'api_key=os.environ[\"TOGETHER_API_KEY\"],    '\n",
      " 'model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)# Requires:# pip install '\n",
      " 'langchain docarray tiktokenfrom langchain_community.vectorstores import '\n",
      " 'DocArrayInMemorySearchfrom langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnableParallel, RunnablePassthroughfrom '\n",
      " 'langchain_openai import OpenAIEmbeddingsvectorstore = '\n",
      " 'DocArrayInMemorySearch.from_texts(    [\"harrison worked at kensho\", \"bears '\n",
      " 'like to eat honey\"],    embedding=OpenAIEmbeddings(),)retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)output_parser = '\n",
      " 'StrOutputParser()setup_and_retrieval = RunnableParallel(    {\"context\": '\n",
      " 'retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | '\n",
      " 'prompt | model | output_parserchain.invoke(\"where did harrison work?\")In '\n",
      " 'this case, the composed chain is: chain = setup_and_retrieval | prompt | '\n",
      " 'model | output_parserTo explain this, we first can see that the prompt '\n",
      " 'template above takes in context and question as values to be substituted in '\n",
      " 'the prompt. Before building the prompt template, we want to retrieve '\n",
      " 'relevant documents to the search and include them as part of the context. As '\n",
      " 'a preliminary step, we‚Äôve setup the retriever using an in memory store, '\n",
      " 'which can retrieve documents based on a query. This is a runnable component '\n",
      " 'as well that can be chained together with other components, but you can also '\n",
      " 'try to run it separately:retriever.invoke(\"where did harrison work?\")We then '\n",
      " 'use the RunnableParallel to prepare the expected inputs into the prompt by '\n",
      " 'using the entries for the retrieved documents as well as the original user '\n",
      " 'question, using the retriever for document search, and RunnablePassthrough '\n",
      " 'to pass the user‚Äôs question:setup_and_retrieval = RunnableParallel(    '\n",
      " '{\"context\": retriever, \"question\": RunnablePassthrough()})To review, the '\n",
      " 'complete chain is:setup_and_retrieval = RunnableParallel(    {\"context\": '\n",
      " 'retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | '\n",
      " 'prompt | model | output_parserWith the flow being:The first steps create a '\n",
      " 'RunnableParallel object with two entries.  The first entry, context will '\n",
      " 'include the document results fetched by the retriever. The second entry, '\n",
      " 'question will contain the user‚Äôs original question. To pass on the question, '\n",
      " 'we use RunnablePassthrough to copy this entry. Feed the dictionary from the '\n",
      " 'step above to the prompt component. It then takes the user input which is '\n",
      " 'question as well as the retrieved document which is context to construct a '\n",
      " 'prompt and output a PromptValue. The model component takes the generated '\n",
      " 'prompt, and passes into the OpenAI LLM model for evaluation. The generated '\n",
      " 'output from the model is a ChatMessage object. Finally, the output_parser '\n",
      " 'component takes in a ChatMessage, and transforms this into a Python string, '\n",
      " 'which is returned from the invoke method.Next steps\\u200bWe recommend '\n",
      " 'reading our Advantages of LCEL section next to see a side-by-side comparison '\n",
      " 'of the code needed to produce common functionality with and without '\n",
      " 'LCEL.Help us out by providing feedback on this documentation '\n",
      " 'page:PreviousLangChain Expression Language (LCEL)NextRunnable interfaceBasic '\n",
      " 'example: prompt + model + output parser1. Prompt2. Model3. Output parser4. '\n",
      " 'Entire PipelineRAG Search ExampleNext '\n",
      " 'stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Runnable interface | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageRunnable interfaceOn this pageRunnable '\n",
      " \"interfaceTo make it as easy as possible to create custom chains, we've \"\n",
      " 'implemented a \"Runnable\" protocol. Many LangChain components implement the '\n",
      " 'Runnable protocol, including chat models, LLMs, output parsers, retrievers, '\n",
      " 'prompt templates, and more. There are also several useful primitives for '\n",
      " 'working with runnables, which you can read about in this section.This is a '\n",
      " 'standard interface, which makes it easy to define custom chains as well as '\n",
      " 'invoke them in a standard way.\\n'\n",
      " 'The standard interface includes:stream: stream back chunks of the '\n",
      " 'responseinvoke: call the chain on an inputbatch: call the chain on a list of '\n",
      " 'inputsThese also have corresponding async methods that should be used with '\n",
      " 'asyncio await syntax for concurrency:astream: stream back chunks of the '\n",
      " 'response asyncainvoke: call the chain on an input asyncabatch: call the '\n",
      " 'chain on a list of inputs asyncastream_log: stream back intermediate steps '\n",
      " 'as they happen, in addition to the final responseastream_events: beta stream '\n",
      " 'events as they happen in the chain (introduced in langchain-core 0.1.14)The '\n",
      " 'input type and output type varies by component:ComponentInput TypeOutput '\n",
      " 'TypePromptDictionaryPromptValueChatModelSingle string, list of chat messages '\n",
      " 'or a PromptValueChatMessageLLMSingle string, list of chat messages or a '\n",
      " 'PromptValueStringOutputParserThe output of an LLM or ChatModelDepends on the '\n",
      " 'parserRetrieverSingle stringList of DocumentsToolSingle string or '\n",
      " 'dictionary, depending on the toolDepends on the toolAll runnables expose '\n",
      " 'input and output schemas to inspect the inputs and outputs:input_schema: an '\n",
      " 'input Pydantic model auto-generated from the structure of the '\n",
      " 'Runnableoutput_schema: an output Pydantic model auto-generated from the '\n",
      " \"structure of the RunnableLet's take a look at these methods. To do so, we'll \"\n",
      " 'create a super simple PromptTemplate + ChatModel chain.%pip install '\n",
      " '--upgrade --quiet  langchain-core langchain-community langchain-openaifrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import '\n",
      " 'ChatOpenAImodel = ChatOpenAI()prompt = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = '\n",
      " 'prompt | modelInput Schema\\u200bA description of the inputs accepted by a '\n",
      " 'Runnable.\\n'\n",
      " 'This is a Pydantic model dynamically generated from the structure of any '\n",
      " 'Runnable.\\n'\n",
      " 'You can call .schema() on it to obtain a JSONSchema representation.# The '\n",
      " 'input schema of the chain is the input schema of its first part, the '\n",
      " \"prompt.chain.input_schema.schema(){'title': 'PromptInput', 'type': 'object', \"\n",
      " \"'properties': {'topic': {'title': 'Topic', 'type': \"\n",
      " \"'string'}}}prompt.input_schema.schema(){'title': 'PromptInput', 'type': \"\n",
      " \"'object', 'properties': {'topic': {'title': 'Topic', 'type': \"\n",
      " \"'string'}}}model.input_schema.schema(){'title': 'ChatOpenAIInput', 'anyOf': \"\n",
      " \"[{'type': 'string'},  {'$ref': '#/definitions/StringPromptValue'},  {'$ref': \"\n",
      " \"'#/definitions/ChatPromptValueConcrete'},  {'type': 'array',   'items': \"\n",
      " \"{'anyOf': [{'$ref': '#/definitions/AIMessage'},     {'$ref': \"\n",
      " \"'#/definitions/HumanMessage'},     {'$ref': \"\n",
      " \"'#/definitions/ChatMessage'},     {'$ref': \"\n",
      " \"'#/definitions/SystemMessage'},     {'$ref': \"\n",
      " \"'#/definitions/FunctionMessage'},     {'$ref': \"\n",
      " \"'#/definitions/ToolMessage'}]}}], 'definitions': {'StringPromptValue': \"\n",
      " \"{'title': 'StringPromptValue',   'description': 'String prompt value.',   \"\n",
      " \"'type': 'object',   'properties': {'text': {'title': 'Text', 'type': \"\n",
      " \"'string'},    'type': {'title': 'Type',     'default': \"\n",
      " \"'StringPromptValue',     'enum': ['StringPromptValue'],     'type': \"\n",
      " \"'string'}},   'required': ['text']},  'AIMessage': {'title': 'AIMessage',   \"\n",
      " \"'description': 'A Message from an AI.',   'type': 'object',   'properties': \"\n",
      " \"{'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      \"\n",
      " \"{'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': \"\n",
      " \"'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', \"\n",
      " \"'type': 'object'},    'type': {'title': 'Type',     'default': 'ai',     \"\n",
      " \"'enum': ['ai'],     'type': 'string'},    'example': {'title': 'Example', \"\n",
      " \"'default': False, 'type': 'boolean'}},   'required': ['content']},  \"\n",
      " \"'HumanMessage': {'title': 'HumanMessage',   'description': 'A Message from a \"\n",
      " \"human.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'human',     'enum': ['human'],     \"\n",
      " \"'type': 'string'},    'example': {'title': 'Example', 'default': False, \"\n",
      " \"'type': 'boolean'}},   'required': ['content']},  'ChatMessage': {'title': \"\n",
      " \"'ChatMessage',   'description': 'A Message that can be assigned an arbitrary \"\n",
      " \"speaker (i.e. role).',   'type': 'object',   'properties': {'content': \"\n",
      " \"{'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': \"\n",
      " \"'array',       'items': {'anyOf': [{'type': 'string'}, {'type': \"\n",
      " \"'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', \"\n",
      " \"'type': 'object'},    'type': {'title': 'Type',     'default': 'chat',     \"\n",
      " \"'enum': ['chat'],     'type': 'string'},    'role': {'title': 'Role', \"\n",
      " \"'type': 'string'}},   'required': ['content', 'role']},  'SystemMessage': \"\n",
      " \"{'title': 'SystemMessage',   'description': 'A Message for priming AI \"\n",
      " 'behavior, usually passed in as the first of a sequence\\\\nof input '\n",
      " \"messages.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'system',     'enum': \"\n",
      " \"['system'],     'type': 'string'}},   'required': ['content']},  \"\n",
      " \"'FunctionMessage': {'title': 'FunctionMessage',   'description': 'A Message \"\n",
      " \"for passing the result of executing a function back to a model.',   'type': \"\n",
      " \"'object',   'properties': {'content': {'title': 'Content',     'anyOf': \"\n",
      " \"[{'type': 'string'},      {'type': 'array',       'items': {'anyOf': \"\n",
      " \"[{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': \"\n",
      " \"{'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': \"\n",
      " \"'Type',     'default': 'function',     'enum': ['function'],     'type': \"\n",
      " \"'string'},    'name': {'title': 'Name', 'type': 'string'}},   'required': \"\n",
      " \"['content', 'name']},  'ToolMessage': {'title': 'ToolMessage',   \"\n",
      " \"'description': 'A Message for passing the result of executing a tool back to \"\n",
      " \"a model.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'tool',     'enum': ['tool'],     \"\n",
      " \"'type': 'string'},    'tool_call_id': {'title': 'Tool Call Id', 'type': \"\n",
      " \"'string'}},   'required': ['content', 'tool_call_id']},  \"\n",
      " \"'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',   \"\n",
      " \"'description': 'Chat prompt value which explicitly lists out the message \"\n",
      " \"types it accepts.\\\\nFor use in external schemas.',   'type': 'object',   \"\n",
      " \"'properties': {'messages': {'title': 'Messages',     'type': 'array',     \"\n",
      " \"'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},       {'$ref': \"\n",
      " \"'#/definitions/HumanMessage'},       {'$ref': \"\n",
      " \"'#/definitions/ChatMessage'},       {'$ref': \"\n",
      " \"'#/definitions/SystemMessage'},       {'$ref': \"\n",
      " \"'#/definitions/FunctionMessage'},       {'$ref': \"\n",
      " \"'#/definitions/ToolMessage'}]}},    'type': {'title': 'Type',     'default': \"\n",
      " \"'ChatPromptValueConcrete',     'enum': ['ChatPromptValueConcrete'],     \"\n",
      " \"'type': 'string'}},   'required': ['messages']}}}Output Schema\\u200bA \"\n",
      " 'description of the outputs produced by a Runnable.\\n'\n",
      " 'This is a Pydantic model dynamically generated from the structure of any '\n",
      " 'Runnable.\\n'\n",
      " 'You can call .schema() on it to obtain a JSONSchema representation.# The '\n",
      " 'output schema of the chain is the output schema of its last part, in this '\n",
      " 'case a ChatModel, which outputs a '\n",
      " \"ChatMessagechain.output_schema.schema(){'title': 'ChatOpenAIOutput', \"\n",
      " \"'anyOf': [{'$ref': '#/definitions/AIMessage'},  {'$ref': \"\n",
      " \"'#/definitions/HumanMessage'},  {'$ref': '#/definitions/ChatMessage'},  \"\n",
      " \"{'$ref': '#/definitions/SystemMessage'},  {'$ref': \"\n",
      " \"'#/definitions/FunctionMessage'},  {'$ref': '#/definitions/ToolMessage'}], \"\n",
      " \"'definitions': {'AIMessage': {'title': 'AIMessage',   'description': 'A \"\n",
      " \"Message from an AI.',   'type': 'object',   'properties': {'content': \"\n",
      " \"{'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': \"\n",
      " \"'array',       'items': {'anyOf': [{'type': 'string'}, {'type': \"\n",
      " \"'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', \"\n",
      " \"'type': 'object'},    'type': {'title': 'Type',     'default': 'ai',     \"\n",
      " \"'enum': ['ai'],     'type': 'string'},    'example': {'title': 'Example', \"\n",
      " \"'default': False, 'type': 'boolean'}},   'required': ['content']},  \"\n",
      " \"'HumanMessage': {'title': 'HumanMessage',   'description': 'A Message from a \"\n",
      " \"human.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'human',     'enum': ['human'],     \"\n",
      " \"'type': 'string'},    'example': {'title': 'Example', 'default': False, \"\n",
      " \"'type': 'boolean'}},   'required': ['content']},  'ChatMessage': {'title': \"\n",
      " \"'ChatMessage',   'description': 'A Message that can be assigned an arbitrary \"\n",
      " \"speaker (i.e. role).',   'type': 'object',   'properties': {'content': \"\n",
      " \"{'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': \"\n",
      " \"'array',       'items': {'anyOf': [{'type': 'string'}, {'type': \"\n",
      " \"'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', \"\n",
      " \"'type': 'object'},    'type': {'title': 'Type',     'default': 'chat',     \"\n",
      " \"'enum': ['chat'],     'type': 'string'},    'role': {'title': 'Role', \"\n",
      " \"'type': 'string'}},   'required': ['content', 'role']},  'SystemMessage': \"\n",
      " \"{'title': 'SystemMessage',   'description': 'A Message for priming AI \"\n",
      " 'behavior, usually passed in as the first of a sequence\\\\nof input '\n",
      " \"messages.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'system',     'enum': \"\n",
      " \"['system'],     'type': 'string'}},   'required': ['content']},  \"\n",
      " \"'FunctionMessage': {'title': 'FunctionMessage',   'description': 'A Message \"\n",
      " \"for passing the result of executing a function back to a model.',   'type': \"\n",
      " \"'object',   'properties': {'content': {'title': 'Content',     'anyOf': \"\n",
      " \"[{'type': 'string'},      {'type': 'array',       'items': {'anyOf': \"\n",
      " \"[{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': \"\n",
      " \"{'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': \"\n",
      " \"'Type',     'default': 'function',     'enum': ['function'],     'type': \"\n",
      " \"'string'},    'name': {'title': 'Name', 'type': 'string'}},   'required': \"\n",
      " \"['content', 'name']},  'ToolMessage': {'title': 'ToolMessage',   \"\n",
      " \"'description': 'A Message for passing the result of executing a tool back to \"\n",
      " \"a model.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'tool',     'enum': ['tool'],     \"\n",
      " \"'type': 'string'},    'tool_call_id': {'title': 'Tool Call Id', 'type': \"\n",
      " \"'string'}},   'required': ['content', 'tool_call_id']}}}Stream\\u200bfor s in \"\n",
      " 'chain.stream({\"topic\": \"bears\"}):    print(s.content, end=\"\", '\n",
      " \"flush=True)Sure, here's a bear-themed joke for you:Why don't bears wear \"\n",
      " 'shoes?Because they already have bear feet!Invoke\\u200bchain.invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\"Why don\\'t bears wear shoes? \\\\n\\\\nBecause they '\n",
      " 'have bear feet!\")Batch\\u200bchain.batch([{\"topic\": \"bears\"}, {\"topic\": '\n",
      " '\"cats\"}])[AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\"), '\n",
      " 'AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many '\n",
      " 'cheetahs!\")]You can set the number of concurrent requests by using the '\n",
      " 'max_concurrency parameterchain.batch([{\"topic\": \"bears\"}, {\"topic\": '\n",
      " '\"cats\"}], config={\"max_concurrency\": 5})[AIMessage(content=\"Why don\\'t bears '\n",
      " 'wear shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why '\n",
      " 'don\\'t cats play poker in the wild? Too many cheetahs!\")]Async '\n",
      " 'Stream\\u200basync for s in chain.astream({\"topic\": \"bears\"}):    '\n",
      " 'print(s.content, end=\"\", flush=True)Why don\\'t bears wear shoes?Because they '\n",
      " 'have bear feet!Async Invoke\\u200bawait chain.ainvoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\"Why don\\'t bears ever wear shoes?\\\\n\\\\nBecause '\n",
      " 'they already have bear feet!\")Async Batch\\u200bawait chain.abatch([{\"topic\": '\n",
      " '\"bears\"}])[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they '\n",
      " 'have bear feet!\")]Async Stream Events (beta)\\u200bEvent Streaming is a beta '\n",
      " 'API, and may change a bit based on feedback.Note: Introduced in '\n",
      " 'langchain-core 0.2.0For now, when using the astream_events API, for '\n",
      " 'everything to work properly please:Use async throughout the code (including '\n",
      " 'async tools etc)Propagate callbacks if defining custom functions / '\n",
      " 'runnables. Whenever using runnables without LCEL, make sure to call '\n",
      " '.astream() on LLMs rather than .ainvoke to force the LLM to stream '\n",
      " 'tokens.Event Reference\\u200bHere is a reference table that shows some events '\n",
      " 'that might be emitted by the various Runnable objects.\\n'\n",
      " 'Definitions for some of the Runnable are included after the table.‚ö†Ô∏è When '\n",
      " 'streaming the inputs for the runnable will not be available until the input '\n",
      " 'stream has been entirely consumed This means that the inputs will be '\n",
      " 'available at for the corresponding end hook rather than start '\n",
      " 'event.eventnamechunkinputoutputon_chat_model_start[model name]{\"messages\": '\n",
      " '[[SystemMessage, HumanMessage]]}on_chat_model_stream[model '\n",
      " 'name]AIMessageChunk(content=\"hello\")on_chat_model_end[model '\n",
      " 'name]{\"messages\": [[SystemMessage, HumanMessage]]}{\"generations\": [...], '\n",
      " '\"llm_output\": None, ...}on_llm_start[model name]{\\'input\\': '\n",
      " \"'hello'}on_llm_stream[model name]'Hello'on_llm_end[model name]'Hello \"\n",
      " 'human!\\'on_chain_startformat_docson_chain_streamformat_docs\"hello world!, '\n",
      " 'goodbye world!\"on_chain_endformat_docs[Document(...)]\"hello world!, goodbye '\n",
      " 'world!\"on_tool_startsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_streamsome_tool{\"x\": '\n",
      " '1, \"y\": \"2\"}on_tool_endsome_tool{\"x\": 1, \"y\": '\n",
      " '\"2\"}on_retriever_start[retriever name]{\"query\": '\n",
      " '\"hello\"}on_retriever_chunk[retriever name]{documents: '\n",
      " '[...]}on_retriever_end[retriever name]{\"query\": \"hello\"}{documents: '\n",
      " '[...]}on_prompt_start[template_name]{\"question\": '\n",
      " '\"hello\"}on_prompt_end[template_name]{\"question\": '\n",
      " '\"hello\"}ChatPromptValue(messages: [SystemMessage, ...])Here are declarations '\n",
      " 'associated with the events shown above:format_docs:def format_docs(docs: '\n",
      " 'List[Document]) -> str:    \\'\\'\\'Format the docs.\\'\\'\\'    return \", '\n",
      " '\".join([doc.page_content for doc in docs])format_docs = '\n",
      " 'RunnableLambda(format_docs)some_tool:@tooldef some_tool(x: int, y: str) -> '\n",
      " 'dict:    \\'\\'\\'Some_tool.\\'\\'\\'    return {\"x\": x, \"y\": y}prompt:template = '\n",
      " 'ChatPromptTemplate.from_messages(    [(\"system\", \"You are Cat Agent 007\"), '\n",
      " '(\"human\", \"{question}\")]).with_config({\"run_name\": \"my_template\", \"tags\": '\n",
      " '[\"my_template\"]})Let\\'s define a new chain to make it more interesting to '\n",
      " 'show off the astream_events interface (and later the astream_log '\n",
      " 'interface).from langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the '\n",
      " 'following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()retrieval_chain = (    {        \"context\": '\n",
      " 'retriever.with_config(run_name=\"Docs\"),        \"question\": '\n",
      " 'RunnablePassthrough(),    }    | prompt    | '\n",
      " 'model.with_config(run_name=\"my_llm\")    | StrOutputParser())Now let\\'s use '\n",
      " 'astream_events to get events from the retriever and the LLM.async for event '\n",
      " 'in retrieval_chain.astream_events(    \"where did harrison work?\", '\n",
      " 'version=\"v1\", include_names=[\"Docs\", \"my_llm\"]):    kind = event[\"event\"]    '\n",
      " 'if kind == \"on_chat_model_stream\":        '\n",
      " 'print(event[\"data\"][\"chunk\"].content, end=\"|\")    elif kind in '\n",
      " '{\"on_chat_model_start\"}:        print()        print(\"Streaming LLM:\")    '\n",
      " 'elif kind in {\"on_chat_model_end\"}:        print()        print(\"Done '\n",
      " 'streaming LLM.\")    elif kind == \"on_retriever_end\":        '\n",
      " 'print(\"--\")        print(\"Retrieved the following documents:\")        '\n",
      " 'print(event[\"data\"][\"output\"][\"documents\"])    elif kind == '\n",
      " '\"on_tool_end\":        print(f\"Ended tool: {event[\\'name\\']}\")    '\n",
      " 'else:        '\n",
      " 'pass/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: '\n",
      " 'LangChainBetaWarning: This API is in beta and may change in the future.  '\n",
      " 'warn_beta(``````output--Retrieved the following '\n",
      " \"documents:[Document(page_content='harrison worked at kensho')]Streaming \"\n",
      " 'LLM:|H|arrison| worked| at| Kens|ho|.||Done streaming LLM.Async Stream '\n",
      " 'Intermediate Steps\\u200bAll runnables also have a method .astream_log() '\n",
      " 'which is used to stream (as they happen) all or part of the intermediate '\n",
      " 'steps of your chain/sequence. This is useful to show progress to the user, '\n",
      " 'to use intermediate results, or to debug your chain.You can stream all steps '\n",
      " '(default) or include/exclude steps by name, tags or metadata.This method '\n",
      " 'yields JSONPatch ops that when applied in the same order as received build '\n",
      " 'up the RunState.class LogEntry(TypedDict):    id: str    \"\"\"ID of the '\n",
      " 'sub-run.\"\"\"    name: str    \"\"\"Name of the object being run.\"\"\"    type: '\n",
      " 'str    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"    '\n",
      " 'tags: List[str]    \"\"\"List of tags for the run.\"\"\"    metadata: Dict[str, '\n",
      " 'Any]    \"\"\"Key-value pairs of metadata for the run.\"\"\"    start_time: str    '\n",
      " '\"\"\"ISO-8601 timestamp of when the run started.\"\"\"    streamed_output_str: '\n",
      " 'List[str]    \"\"\"List of LLM tokens streamed by this run, if '\n",
      " 'applicable.\"\"\"    final_output: Optional[Any]    \"\"\"Final output of this '\n",
      " 'run.    Only available after the run has finished successfully.\"\"\"    '\n",
      " 'end_time: Optional[str]    \"\"\"ISO-8601 timestamp of when the run ended.    '\n",
      " 'Only available after the run has finished.\"\"\"class RunState(TypedDict):    '\n",
      " 'id: str    \"\"\"ID of the run.\"\"\"    streamed_output: List[Any]    \"\"\"List of '\n",
      " 'output chunks streamed by Runnable.stream()\"\"\"    final_output: '\n",
      " 'Optional[Any]    \"\"\"Final output of the run, usually the result of '\n",
      " 'aggregating (`+`) streamed_output.    Only available after the run has '\n",
      " 'finished successfully.\"\"\"    logs: Dict[str, LogEntry]    \"\"\"Map of run '\n",
      " 'names to sub-runs. If filters were supplied, this list will    contain only '\n",
      " 'the runs that matched the filters.\"\"\"Streaming JSONPatch chunks\\u200bThis is '\n",
      " 'useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops '\n",
      " 'on the client to rebuild the run state there. See LangServe for tooling to '\n",
      " 'make it easier to build a webserver from any Runnable.async for chunk in '\n",
      " 'retrieval_chain.astream_log(    \"where did harrison work?\", '\n",
      " 'include_names=[\"Docs\"]):    print(\"-\" * 40)    '\n",
      " \"print(chunk)----------------------------------------RunLogPatch({'op': \"\n",
      " \"'replace',  'path': '',  'value': {'final_output': None,            'id': \"\n",
      " \"'82e9b4b1-3dd6-4732-8db9-90e79c4da48c',            'logs': {},            \"\n",
      " \"'name': 'RunnableSequence',            'streamed_output': [],            \"\n",
      " \"'type': 'chain'}})----------------------------------------RunLogPatch({'op': \"\n",
      " \"'add',  'path': '/logs/Docs',  'value': {'end_time': None,            \"\n",
      " \"'final_output': None,            'id': \"\n",
      " \"'9206e94a-57bd-48ee-8c5e-fdd1c52a6da2',            'metadata': \"\n",
      " \"{},            'name': 'Docs',            'start_time': \"\n",
      " \"'2024-01-19T22:33:55.902+00:00',            'streamed_output': \"\n",
      " \"[],            'streamed_output_str': [],            'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],            'type': \"\n",
      " \"'retriever'}})----------------------------------------RunLogPatch({'op': \"\n",
      " \"'add',  'path': '/logs/Docs/final_output',  'value': {'documents': \"\n",
      " \"[Document(page_content='harrison worked at kensho')]}}, {'op': 'add',  \"\n",
      " \"'path': '/logs/Docs/end_time',  'value': \"\n",
      " \"'2024-01-19T22:33:56.064+00:00'})----------------------------------------RunLogPatch({'op': \"\n",
      " \"'add', 'path': '/streamed_output/-', 'value': ''}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': \"\n",
      " \"''})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': 'H'}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': \"\n",
      " \"'H'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': 'arrison'}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': \"\n",
      " \"'Harrison'})----------------------------------------RunLogPatch({'op': \"\n",
      " \"'add', 'path': '/streamed_output/-', 'value': ' worked'}, {'op': 'replace', \"\n",
      " \"'path': '/final_output', 'value': 'Harrison \"\n",
      " \"worked'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': ' at'}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': 'Harrison worked \"\n",
      " \"at'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': ' Kens'}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': 'Harrison worked at \"\n",
      " \"Kens'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': 'ho'}, {'op': 'replace',  'path': \"\n",
      " \"'/final_output',  'value': 'Harrison worked at \"\n",
      " \"Kensho'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': '.'}, {'op': 'replace',  'path': \"\n",
      " \"'/final_output',  'value': 'Harrison worked at \"\n",
      " \"Kensho.'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': ''})Streaming the incremental \"\n",
      " 'RunState\\u200bYou can simply pass diff=False to get incremental values of '\n",
      " 'RunState.\\n'\n",
      " 'You get more verbose output with more repetitive parts.async for chunk in '\n",
      " 'retrieval_chain.astream_log(    \"where did harrison work?\", '\n",
      " 'include_names=[\"Docs\"], diff=False):    print(\"-\" * 70)    '\n",
      " \"print(chunk)----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {}, 'name': \"\n",
      " \"'RunnableSequence', 'streamed_output': [], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': None,                   'final_output': None,                   \"\n",
      " \"'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [''], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'H', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H'], \"\n",
      " \"'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': \"\n",
      " \"{'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': \"\n",
      " \"{'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked', ' at'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at Kens', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', \"\n",
      " \"'logs': {'Docs': {'end_time': \"\n",
      " \"'2024-01-19T22:33:57.120+00:00',                   'final_output': \"\n",
      " \"{'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked', ' at', ' Kens'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at Kensho', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', \"\n",
      " \"'logs': {'Docs': {'end_time': \"\n",
      " \"'2024-01-19T22:33:57.120+00:00',                   'final_output': \"\n",
      " \"{'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked', ' at', ' Kens', 'ho'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at Kensho.', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', \"\n",
      " \"'logs': {'Docs': {'end_time': \"\n",
      " \"'2024-01-19T22:33:57.120+00:00',                   'final_output': \"\n",
      " \"{'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked', ' at', ' Kens', 'ho', '.'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at Kensho.', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', \"\n",
      " \"'logs': {'Docs': {'end_time': \"\n",
      " \"'2024-01-19T22:33:57.120+00:00',                   'final_output': \"\n",
      " \"{'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': \"\n",
      " \"['',                     'H',                     \"\n",
      " \"'arrison',                     ' worked',                     ' \"\n",
      " \"at',                     ' Kens',                     \"\n",
      " \"'ho',                     '.',                     ''], 'type': \"\n",
      " \"'chain'})Parallelism\\u200bLet's take a look at how LangChain Expression \"\n",
      " 'Language supports parallel requests.\\n'\n",
      " 'For example, when using a RunnableParallel (often written as a dictionary) '\n",
      " 'it executes each element in parallel.from langchain_core.runnables import '\n",
      " 'RunnableParallelchain1 = ChatPromptTemplate.from_template(\"tell me a joke '\n",
      " 'about {topic}\") | modelchain2 = (    ChatPromptTemplate.from_template(\"write '\n",
      " 'a short (2 line) poem about {topic}\")    | model)combined = '\n",
      " 'RunnableParallel(joke=chain1, poem=chain2)%%timechain1.invoke({\"topic\": '\n",
      " '\"bears\"})CPU times: user 18 ms, sys: 1.27 ms, total: 19.3 msWall time: 692 '\n",
      " 'msAIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already '\n",
      " 'have bear feet!\")%%timechain2.invoke({\"topic\": \"bears\"})CPU times: user 10.5 '\n",
      " 'ms, sys: 166 ¬µs, total: 10.7 msWall time: 579 msAIMessage(content=\"In '\n",
      " 'forest\\'s embrace,\\\\nMajestic bears pace.\")%%timecombined.invoke({\"topic\": '\n",
      " '\"bears\"})CPU times: user 32 ms, sys: 2.59 ms, total: 34.6 msWall time: 816 '\n",
      " 'ms{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear-related joke for '\n",
      " 'you:\\\\n\\\\nWhy did the bear bring a ladder to the bar?\\\\n\\\\nBecause he heard '\n",
      " 'the drinks were on the house!\"), \\'poem\\': AIMessage(content=\"In wilderness '\n",
      " 'they roam,\\\\nMajestic strength, nature\\'s throne.\")}Parallelism on '\n",
      " 'batches\\u200bParallelism can be combined with other runnables.\\n'\n",
      " 'Let\\'s try to use parallelism with batches.%%timechain1.batch([{\"topic\": '\n",
      " '\"bears\"}, {\"topic\": \"cats\"}])CPU times: user 17.3 ms, sys: 4.84 ms, total: '\n",
      " '22.2 msWall time: 628 ms[AIMessage(content=\"Why don\\'t bears wear '\n",
      " 'shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why don\\'t '\n",
      " 'cats play poker in the wild?\\\\n\\\\nToo many '\n",
      " 'cheetahs!\")]%%timechain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU '\n",
      " 'times: user 15.8 ms, sys: 3.83 ms, total: 19.7 msWall time: 718 '\n",
      " \"ms[AIMessage(content='In the wild, bears roam,\\\\nMajestic guardians of \"\n",
      " \"ancient home.'), AIMessage(content='Whiskers grace, eyes gleam,\\\\nCats dance \"\n",
      " 'through the moonbeam.\\')]%%timecombined.batch([{\"topic\": \"bears\"}, {\"topic\": '\n",
      " '\"cats\"}])CPU times: user 44.8 ms, sys: 3.17 ms, total: 48 msWall time: 721 '\n",
      " 'ms[{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"),  \\'poem\\': '\n",
      " 'AIMessage(content=\"Majestic bears roam,\\\\nNature\\'s strength, beauty '\n",
      " 'shown.\")}, {\\'joke\\': AIMessage(content=\"Why don\\'t cats play poker in the '\n",
      " 'wild?\\\\n\\\\nToo many cheetahs!\"),  \\'poem\\': AIMessage(content=\"Whiskers '\n",
      " 'dance, eyes aglow,\\\\nCats embrace the night\\'s gentle flow.\")}]Help us out '\n",
      " 'by providing feedback on this documentation page:PreviousGet '\n",
      " 'startedNextPrimitivesInput SchemaOutput SchemaStreamInvokeBatchAsync '\n",
      " 'StreamAsync InvokeAsync BatchAsync Stream Events (beta)Event ReferenceAsync '\n",
      " 'Stream Intermediate StepsStreaming JSONPatch chunksStreaming the incremental '\n",
      " 'RunStateParallelismParallelism on '\n",
      " 'batchesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Advantages of LCEL | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageAdvantages of LCELOn this pageAdvantages '\n",
      " 'of LCELtipWe recommend reading the LCEL Get started section first.LCEL is '\n",
      " 'designed to streamline the process of building useful apps with LLMs and '\n",
      " 'combining related components. It does this by providing:A unified interface: '\n",
      " 'Every LCEL object implements the Runnable interface, which defines a common '\n",
      " 'set of invocation methods (invoke, batch, stream, ainvoke, ...). This makes '\n",
      " 'it possible for chains of LCEL objects to also automatically support useful '\n",
      " 'operations like batching and streaming of intermediate steps, since every '\n",
      " 'chain of LCEL objects is itself an LCEL object.Composition primitives: LCEL '\n",
      " 'provides a number of primitives that make it easy to compose chains, '\n",
      " 'parallelize components, add fallbacks, dynamically configure chain '\n",
      " \"internals, and more.To better understand the value of LCEL, it's helpful to \"\n",
      " 'see it in action and think about how we might recreate similar functionality '\n",
      " \"without it. In this walkthrough we'll do just that with our basic example \"\n",
      " \"from the get started section. We'll take our simple prompt + model chain, \"\n",
      " 'which under the hood already defines a lot of functionality, and see what it '\n",
      " 'would take to recreate all of it.%pip install --upgrade --quiet  '\n",
      " 'langchain-core langchain-openai langchain-anthropicInvoke\\u200bIn the '\n",
      " 'simplest case, we just want to pass in a topic string and get back a joke '\n",
      " 'string:Without LCEL\\u200bfrom typing import Listimport openaiprompt_template '\n",
      " '= \"Tell me a short joke about {topic}\"client = openai.OpenAI()def '\n",
      " 'call_chat_model(messages: List[dict]) -> str:    response = '\n",
      " 'client.chat.completions.create(        model=\"gpt-3.5-turbo\",         '\n",
      " 'messages=messages,    )    return response.choices[0].message.contentdef '\n",
      " 'invoke_chain(topic: str) -> str:    prompt_value = '\n",
      " 'prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", '\n",
      " '\"content\": prompt_value}]    return '\n",
      " 'call_chat_model(messages)invoke_chain(\"ice cream\")LCEL\\u200bfrom '\n",
      " 'langchain_openai import ChatOpenAIfrom langchain_core.prompts import '\n",
      " 'ChatPromptTemplatefrom langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.runnables import '\n",
      " 'RunnablePassthroughprompt = ChatPromptTemplate.from_template(    \"Tell me a '\n",
      " 'short joke about {topic}\")output_parser = StrOutputParser()model = '\n",
      " 'ChatOpenAI(model=\"gpt-3.5-turbo\")chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt    | model    | '\n",
      " 'output_parser)chain.invoke(\"ice cream\")Stream\\u200bIf we want to stream '\n",
      " \"results instead, we'll need to change our function:Without LCEL\\u200bfrom \"\n",
      " 'typing import Iteratordef stream_chat_model(messages: List[dict]) -> '\n",
      " 'Iterator[str]:    stream = client.chat.completions.create(        '\n",
      " 'model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    '\n",
      " 'for response in stream:        content = '\n",
      " 'response.choices[0].delta.content        if content is not None:            '\n",
      " 'yield contentdef stream_chain(topic: str) -> Iterator[str]:    prompt_value '\n",
      " '= prompt.format(topic=topic)    return stream_chat_model([{\"role\": \"user\", '\n",
      " '\"content\": prompt_value}])for chunk in stream_chain(\"ice cream\"):    '\n",
      " 'print(chunk, end=\"\", flush=True)LCEL\\u200bfor chunk in chain.stream(\"ice '\n",
      " 'cream\"):    print(chunk, end=\"\", flush=True)Batch\\u200bIf we want to run on '\n",
      " \"a batch of inputs in parallel, we'll again need a new function:Without \"\n",
      " 'LCEL\\u200bfrom concurrent.futures import ThreadPoolExecutordef '\n",
      " 'batch_chain(topics: list) -> list:    with ThreadPoolExecutor(max_workers=5) '\n",
      " 'as executor:        return list(executor.map(invoke_chain, '\n",
      " 'topics))batch_chain([\"ice cream\", \"spaghetti\", '\n",
      " '\"dumplings\"])LCEL\\u200bchain.batch([\"ice cream\", \"spaghetti\", '\n",
      " '\"dumplings\"])## AsyncIf we need an asynchronous version:Without '\n",
      " 'LCEL\\u200basync_client = openai.AsyncOpenAI()async def '\n",
      " 'acall_chat_model(messages: List[dict]) -> str:    response = await '\n",
      " 'async_client.chat.completions.create(        model=\"gpt-3.5-turbo\",         '\n",
      " 'messages=messages,    )    return response.choices[0].message.contentasync '\n",
      " 'def ainvoke_chain(topic: str) -> str:    prompt_value = '\n",
      " 'prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", '\n",
      " '\"content\": prompt_value}]    return await acall_chat_model(messages)await '\n",
      " 'ainvoke_chain(\"ice cream\")LCEL\\u200bawait chain.ainvoke(\"ice cream\")Async '\n",
      " 'Batch\\u200bWithout LCEL\\u200bimport asyncioimport openaiasync def '\n",
      " 'abatch_chain(topics: list) -> list:    coros = map(ainvoke_chain, topics)    '\n",
      " 'return await asyncio.gather(*coros)await abatch_chain([\"ice cream\", '\n",
      " '\"spaghetti\", \"dumplings\"])LCEL\\u200bawait chain.abatch([\"ice cream\", '\n",
      " '\"spaghetti\", \"dumplings\"])LLM instead of chat model\\u200bIf we want to use a '\n",
      " 'completion endpoint instead of a chat endpoint: Without LCEL\\u200bdef '\n",
      " 'call_llm(prompt_value: str) -> str:    response = '\n",
      " 'client.completions.create(        model=\"gpt-3.5-turbo-instruct\",        '\n",
      " 'prompt=prompt_value,    )    return response.choices[0].textdef '\n",
      " 'invoke_llm_chain(topic: str) -> str:    prompt_value = '\n",
      " 'prompt_template.format(topic=topic)    return '\n",
      " 'call_llm(prompt_value)invoke_llm_chain(\"ice cream\")LCEL\\u200bfrom '\n",
      " 'langchain_openai import OpenAIllm = '\n",
      " 'OpenAI(model=\"gpt-3.5-turbo-instruct\")llm_chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt    | llm    | '\n",
      " 'output_parser)llm_chain.invoke(\"ice cream\")Different model provider\\u200bIf '\n",
      " 'we want to use Anthropic instead of OpenAI: Without LCEL\\u200bimport '\n",
      " 'anthropicanthropic_template = '\n",
      " 'f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"anthropic_client = '\n",
      " 'anthropic.Anthropic()def call_anthropic(prompt_value: str) -> str:    '\n",
      " 'response = anthropic_client.completions.create(        '\n",
      " 'model=\"claude-2\",        prompt=prompt_value,        '\n",
      " 'max_tokens_to_sample=256,    )    return response.completion    def '\n",
      " 'invoke_anthropic_chain(topic: str) -> str:    prompt_value = '\n",
      " 'anthropic_template.format(topic=topic)    return '\n",
      " 'call_anthropic(prompt_value)invoke_anthropic_chain(\"ice '\n",
      " 'cream\")LCEL\\u200bfrom langchain_anthropic import ChatAnthropicanthropic = '\n",
      " 'ChatAnthropic(model=\"claude-2\")anthropic_chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt     | anthropic    | '\n",
      " 'output_parser)anthropic_chain.invoke(\"ice cream\")Runtime '\n",
      " 'configurability\\u200bIf we wanted to make the choice of chat model or LLM '\n",
      " 'configurable at runtime:Without LCEL\\u200bdef invoke_configurable_chain(    '\n",
      " 'topic: str,     *,     model: str = \"chat_openai\") -> str:    if model == '\n",
      " '\"chat_openai\":        return invoke_chain(topic)    elif model == '\n",
      " '\"openai\":        return invoke_llm_chain(topic)    elif model == '\n",
      " '\"anthropic\":        return invoke_anthropic_chain(topic)    else:        '\n",
      " 'raise ValueError(            f\"Received invalid model '\n",
      " '\\'{model}\\'.\"            \" Expected one of chat_openai, openai, '\n",
      " 'anthropic\"        )def stream_configurable_chain(    topic: str,     *,     '\n",
      " 'model: str = \"chat_openai\") -> Iterator[str]:    if model == '\n",
      " '\"chat_openai\":        return stream_chain(topic)    elif model == '\n",
      " '\"openai\":        # Note we haven\\'t implemented this yet.        return '\n",
      " 'stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we '\n",
      " \"haven't implemented this yet        return stream_anthropic_chain(topic)    \"\n",
      " 'else:        raise ValueError(            f\"Received invalid model '\n",
      " '\\'{model}\\'.\"            \" Expected one of chat_openai, openai, '\n",
      " 'anthropic\"        )def batch_configurable_chain(    topics: List[str],     '\n",
      " '*,     model: str = \"chat_openai\") -> List[str]:    # You get the idea    '\n",
      " '...async def abatch_configurable_chain(    topics: List[str],     *,     '\n",
      " 'model: str = \"chat_openai\") -> List[str]:    '\n",
      " '...invoke_configurable_chain(\"ice cream\", model=\"openai\")stream = '\n",
      " 'stream_configurable_chain(    \"ice_cream\",     model=\"anthropic\")for chunk '\n",
      " 'in stream:    print(chunk, end=\"\", flush=True)# '\n",
      " 'batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])# await '\n",
      " 'ainvoke_configurable_chain(\"ice cream\")With LCEL\\u200bfrom '\n",
      " 'langchain_core.runnables import ConfigurableFieldconfigurable_model = '\n",
      " 'model.configurable_alternatives(    ConfigurableField(id=\"model\"),     '\n",
      " 'default_key=\"chat_openai\",     openai=llm,    '\n",
      " 'anthropic=anthropic,)configurable_chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt     | configurable_model     | '\n",
      " 'output_parser)configurable_chain.invoke(    \"ice cream\",     '\n",
      " 'config={\"model\": \"openai\"})stream = configurable_chain.stream(    \"ice '\n",
      " 'cream\",     config={\"model\": \"anthropic\"})for chunk in stream:    '\n",
      " 'print(chunk, end=\"\", flush=True)configurable_chain.batch([\"ice cream\", '\n",
      " '\"spaghetti\", \"dumplings\"])# await configurable_chain.ainvoke(\"ice '\n",
      " 'cream\")Logging\\u200bIf we want to log our intermediate results:Without '\n",
      " \"LCEL\\u200bWe'll print intermediate steps for illustrative purposesdef \"\n",
      " 'invoke_anthropic_chain_with_logging(topic: str) -> str:    print(f\"Input: '\n",
      " '{topic}\")    prompt_value = anthropic_template.format(topic=topic)    '\n",
      " 'print(f\"Formatted prompt: {prompt_value}\")    output = '\n",
      " 'call_anthropic(prompt_value)    print(f\"Output: {output}\")    return '\n",
      " 'outputinvoke_anthropic_chain_with_logging(\"ice cream\")LCEL\\u200bEvery '\n",
      " 'component has built-in integrations with LangSmith. If we set the following '\n",
      " 'two environment variables, all chain traces are logged to LangSmith.import '\n",
      " 'osos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] '\n",
      " '= \"true\"anthropic_chain.invoke(\"ice cream\")Here\\'s what our LangSmith trace '\n",
      " 'looks like: '\n",
      " 'https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/rFallbacks\\u200bIf '\n",
      " 'we wanted to add fallback logic, in case one model API is down:Without '\n",
      " 'LCEL\\u200bdef invoke_chain_with_fallback(topic: str) -> str:    try:        '\n",
      " 'return invoke_chain(topic)    except Exception:        return '\n",
      " 'invoke_anthropic_chain(topic)async def ainvoke_chain_with_fallback(topic: '\n",
      " 'str) -> str:    try:        return await ainvoke_chain(topic)    except '\n",
      " \"Exception:        # Note: we haven't actually implemented this.        \"\n",
      " 'return await ainvoke_anthropic_chain(topic)async def '\n",
      " 'batch_chain_with_fallback(topics: List[str]) -> str:    try:        return '\n",
      " \"batch_chain(topics)    except Exception:        # Note: we haven't actually \"\n",
      " 'implemented this.        return '\n",
      " 'batch_anthropic_chain(topics)invoke_chain_with_fallback(\"ice cream\")# await '\n",
      " 'ainvoke_chain_with_fallback(\"ice cream\")batch_chain_with_fallback([\"ice '\n",
      " 'cream\", \"spaghetti\", \"dumplings\"]))LCEL\\u200bfallback_chain = '\n",
      " 'chain.with_fallbacks([anthropic_chain])fallback_chain.invoke(\"ice cream\")# '\n",
      " 'await fallback_chain.ainvoke(\"ice cream\")fallback_chain.batch([\"ice cream\", '\n",
      " '\"spaghetti\", \"dumplings\"])Full code comparison\\u200bEven in this simple '\n",
      " 'case, our LCEL chain succinctly packs in a lot of functionality. As chains '\n",
      " 'become more complex, this becomes especially valuable.Without LCEL\\u200bfrom '\n",
      " 'concurrent.futures import ThreadPoolExecutorfrom typing import Iterator, '\n",
      " 'List, Tupleimport anthropicimport openaiprompt_template = \"Tell me a short '\n",
      " 'joke about {topic}\"anthropic_template = '\n",
      " 'f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"client = '\n",
      " 'openai.OpenAI()async_client = openai.AsyncOpenAI()anthropic_client = '\n",
      " 'anthropic.Anthropic()def call_chat_model(messages: List[dict]) -> str:    '\n",
      " 'response = client.chat.completions.create(        '\n",
      " 'model=\"gpt-3.5-turbo\",         messages=messages,    )    return '\n",
      " 'response.choices[0].message.contentdef invoke_chain(topic: str) -> str:    '\n",
      " 'print(f\"Input: {topic}\")    prompt_value = '\n",
      " 'prompt_template.format(topic=topic)    print(f\"Formatted prompt: '\n",
      " '{prompt_value}\")    messages = [{\"role\": \"user\", \"content\": '\n",
      " 'prompt_value}]    output = call_chat_model(messages)    print(f\"Output: '\n",
      " '{output}\")    return outputdef stream_chat_model(messages: List[dict]) -> '\n",
      " 'Iterator[str]:    stream = client.chat.completions.create(        '\n",
      " 'model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    '\n",
      " 'for response in stream:        content = '\n",
      " 'response.choices[0].delta.content        if content is not None:            '\n",
      " 'yield contentdef stream_chain(topic: str) -> Iterator[str]:    '\n",
      " 'print(f\"Input: {topic}\")    prompt_value = prompt.format(topic=topic)    '\n",
      " 'print(f\"Formatted prompt: {prompt_value}\")    stream = '\n",
      " 'stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])    for chunk '\n",
      " 'in stream:        print(f\"Token: {chunk}\", end=\"\")        yield chunkdef '\n",
      " 'batch_chain(topics: list) -> list:    with ThreadPoolExecutor(max_workers=5) '\n",
      " 'as executor:        return list(executor.map(invoke_chain, topics))def '\n",
      " 'call_llm(prompt_value: str) -> str:    response = '\n",
      " 'client.completions.create(        model=\"gpt-3.5-turbo-instruct\",        '\n",
      " 'prompt=prompt_value,    )    return response.choices[0].textdef '\n",
      " 'invoke_llm_chain(topic: str) -> str:    print(f\"Input: {topic}\")    '\n",
      " 'prompt_value = promtp_template.format(topic=topic)    print(f\"Formatted '\n",
      " 'prompt: {prompt_value}\")    output = call_llm(prompt_value)    '\n",
      " 'print(f\"Output: {output}\")    return outputdef call_anthropic(prompt_value: '\n",
      " 'str) -> str:    response = anthropic_client.completions.create(        '\n",
      " 'model=\"claude-2\",        prompt=prompt_value,        '\n",
      " 'max_tokens_to_sample=256,    )    return response.completion   def '\n",
      " 'invoke_anthropic_chain(topic: str) -> str:    print(f\"Input: {topic}\")    '\n",
      " 'prompt_value = anthropic_template.format(topic=topic)    print(f\"Formatted '\n",
      " 'prompt: {prompt_value}\")    output = call_anthropic(prompt_value)    '\n",
      " 'print(f\"Output: {output}\")    return outputasync def '\n",
      " 'ainvoke_anthropic_chain(topic: str) -> str:    ...def '\n",
      " 'stream_anthropic_chain(topic: str) -> Iterator[str]:    ...def '\n",
      " 'batch_anthropic_chain(topics: List[str]) -> List[str]:    ...def '\n",
      " 'invoke_configurable_chain(    topic: str,     *,     model: str = '\n",
      " '\"chat_openai\") -> str:    if model == \"chat_openai\":        return '\n",
      " 'invoke_chain(topic)    elif model == \"openai\":        return '\n",
      " 'invoke_llm_chain(topic)    elif model == \"anthropic\":        return '\n",
      " 'invoke_anthropic_chain(topic)    else:        raise ValueError(            '\n",
      " 'f\"Received invalid model \\'{model}\\'.\"            \" Expected one of '\n",
      " 'chat_openai, openai, anthropic\"        )def stream_configurable_chain(    '\n",
      " 'topic: str,     *,     model: str = \"chat_openai\") -> Iterator[str]:    if '\n",
      " 'model == \"chat_openai\":        return stream_chain(topic)    elif model == '\n",
      " '\"openai\":        # Note we haven\\'t implemented this yet.        return '\n",
      " 'stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we '\n",
      " \"haven't implemented this yet        return stream_anthropic_chain(topic)    \"\n",
      " 'else:        raise ValueError(            f\"Received invalid model '\n",
      " '\\'{model}\\'.\"            \" Expected one of chat_openai, openai, '\n",
      " 'anthropic\"        )def batch_configurable_chain(    topics: List[str],     '\n",
      " '*,     model: str = \"chat_openai\") -> List[str]:    ...async def '\n",
      " 'abatch_configurable_chain(    topics: List[str],     *,     model: str = '\n",
      " '\"chat_openai\") -> List[str]:    ...def invoke_chain_with_fallback(topic: '\n",
      " 'str) -> str:    try:        return invoke_chain(topic)    except '\n",
      " 'Exception:        return invoke_anthropic_chain(topic)async def '\n",
      " 'ainvoke_chain_with_fallback(topic: str) -> str:    try:        return await '\n",
      " 'ainvoke_chain(topic)    except Exception:        return await '\n",
      " 'ainvoke_anthropic_chain(topic)async def batch_chain_with_fallback(topics: '\n",
      " 'List[str]) -> str:    try:        return batch_chain(topics)    except '\n",
      " 'Exception:        return batch_anthropic_chain(topics)LCEL\\u200bimport '\n",
      " 'osfrom langchain_anthropic import ChatAnthropicfrom langchain_openai import '\n",
      " 'ChatOpenAIfrom langchain_openai import OpenAIfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthrough, '\n",
      " 'ConfigurableFieldos.environ[\"LANGCHAIN_API_KEY\"] = '\n",
      " '\"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(    \"Tell me a short joke about '\n",
      " '{topic}\")chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")openai = '\n",
      " 'OpenAI(model=\"gpt-3.5-turbo-instruct\")anthropic = '\n",
      " 'ChatAnthropic(model=\"claude-2\")model = (    chat_openai    '\n",
      " '.with_fallbacks([anthropic])    .configurable_alternatives(        '\n",
      " 'ConfigurableField(id=\"model\"),        default_key=\"chat_openai\",        '\n",
      " 'openai=openai,        anthropic=anthropic,    ))chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt     | model     | StrOutputParser())Next '\n",
      " 'steps\\u200bTo continue learning about LCEL, we recommend:Reading up on the '\n",
      " \"full LCEL Interface, which we've only partially covered here.Exploring the \"\n",
      " 'primitives to learn more about what LCEL provides.Help us out by providing '\n",
      " 'feedback on this documentation '\n",
      " 'page:PreviousPrimitivesNextStreamingInvokeStreamBatchAsync BatchLLM instead '\n",
      " 'of chat modelDifferent model providerRuntime '\n",
      " 'configurabilityLoggingFallbacksFull code comparisonNext '\n",
      " 'stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Add message history (memory) | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageAdd message history (memory)On this '\n",
      " 'pageAdd message history (memory)The RunnableWithMessageHistory lets us add '\n",
      " 'message history to certain types of chains. It wraps another Runnable and '\n",
      " 'manages the chat message history for it.Specifically, it can be used for any '\n",
      " 'Runnable that takes as input one ofa sequence of BaseMessagea dict with a '\n",
      " 'key that takes a sequence of BaseMessagea dict with a key that takes the '\n",
      " 'latest message(s) as a string or sequence of BaseMessage, and a separate key '\n",
      " 'that takes historical messagesAnd returns as output one ofa string that can '\n",
      " 'be treated as the contents of an AIMessagea sequence of BaseMessagea dict '\n",
      " \"with a key that contains a sequence of BaseMessageLet's take a look at some \"\n",
      " 'examples to see how it works. First we construct a runnable (which here '\n",
      " 'accepts a dict as input and returns a message as output):from '\n",
      " 'langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom '\n",
      " 'langchain_openai.chat_models import ChatOpenAImodel = ChatOpenAI()prompt = '\n",
      " 'ChatPromptTemplate.from_messages(    [        (            '\n",
      " '\"system\",            \"You\\'re an assistant who\\'s good at {ability}. Respond '\n",
      " 'in 20 words or fewer\",        ),        '\n",
      " 'MessagesPlaceholder(variable_name=\"history\"),        (\"human\", '\n",
      " '\"{input}\"),    ])runnable = prompt | modelTo manage the message history, we '\n",
      " 'will need:This runnable;A callable that returns an instance of '\n",
      " 'BaseChatMessageHistory.Check out the memory integrations page for '\n",
      " 'implementations of chat message histories using Redis and other providers. '\n",
      " 'Here we demonstrate using an in-memory ChatMessageHistory as well as more '\n",
      " 'persistent storage using RedisChatMessageHistory.In-memory\\u200bBelow we '\n",
      " 'show a simple example in which the chat history lives in memory, in this '\n",
      " 'case via a global Python dict.We construct a callable get_session_history '\n",
      " 'that references this dict to return an instance of ChatMessageHistory. The '\n",
      " 'arguments to the callable can be specified by passing a configuration to the '\n",
      " 'RunnableWithMessageHistory at runtime. By default, the configuration '\n",
      " 'parameter is expected to be a single string session_id. This can be adjusted '\n",
      " 'via the history_factory_config kwarg.Using the single-parameter default:from '\n",
      " 'langchain_community.chat_message_histories import ChatMessageHistoryfrom '\n",
      " 'langchain_core.chat_history import BaseChatMessageHistoryfrom '\n",
      " 'langchain_core.runnables.history import RunnableWithMessageHistorystore = '\n",
      " '{}def get_session_history(session_id: str) -> BaseChatMessageHistory:    if '\n",
      " 'session_id not in store:        store[session_id] = ChatMessageHistory()    '\n",
      " 'return store[session_id]with_message_history = '\n",
      " 'RunnableWithMessageHistory(    runnable,    get_session_history,    '\n",
      " 'input_messages_key=\"input\",    history_messages_key=\"history\",)Note that '\n",
      " \"we've specified input_messages_key (the key to be treated as the latest \"\n",
      " 'input message) and history_messages_key (the key to add historical messages '\n",
      " 'to).When invoking this new runnable, we specify the corresponding chat '\n",
      " 'history via a configuration parameter:with_message_history.invoke(    '\n",
      " '{\"ability\": \"math\", \"input\": \"What does cosine mean?\"},    '\n",
      " 'config={\"configurable\": {\"session_id\": '\n",
      " '\"abc123\"}},)AIMessage(content=\\'Cosine is a trigonometric function that '\n",
      " 'calculates the ratio of the adjacent side to the hypotenuse of a right '\n",
      " 'triangle.\\')# Rememberswith_message_history.invoke(    {\"ability\": \"math\", '\n",
      " '\"input\": \"What?\"},    config={\"configurable\": {\"session_id\": '\n",
      " '\"abc123\"}},)AIMessage(content=\\'Cosine is a mathematical function used to '\n",
      " \"calculate the length of a side in a right triangle.')# New session_id --> \"\n",
      " 'does not remember.with_message_history.invoke(    {\"ability\": \"math\", '\n",
      " '\"input\": \"What?\"},    config={\"configurable\": {\"session_id\": '\n",
      " '\"def234\"}},)AIMessage(content=\\'I can help with math problems. What do you '\n",
      " \"need assistance with?')The configuration parameters by which we track \"\n",
      " 'message histories can be customized by passing in a list of '\n",
      " 'ConfigurableFieldSpec objects to the history_factory_config parameter. '\n",
      " 'Below, we use two parameters: a user_id and conversation_id.from '\n",
      " 'langchain_core.runnables import ConfigurableFieldSpecstore = {}def '\n",
      " 'get_session_history(user_id: str, conversation_id: str) -> '\n",
      " 'BaseChatMessageHistory:    if (user_id, conversation_id) not in '\n",
      " 'store:        store[(user_id, conversation_id)] = ChatMessageHistory()    '\n",
      " 'return store[(user_id, conversation_id)]with_message_history = '\n",
      " 'RunnableWithMessageHistory(    runnable,    get_session_history,    '\n",
      " 'input_messages_key=\"input\",    history_messages_key=\"history\",    '\n",
      " 'history_factory_config=[        ConfigurableFieldSpec(            '\n",
      " 'id=\"user_id\",            annotation=str,            name=\"User '\n",
      " 'ID\",            description=\"Unique identifier for the user.\",            '\n",
      " 'default=\"\",            is_shared=True,        ),        '\n",
      " 'ConfigurableFieldSpec(            id=\"conversation_id\",            '\n",
      " 'annotation=str,            name=\"Conversation ID\",            '\n",
      " 'description=\"Unique identifier for the conversation.\",            '\n",
      " 'default=\"\",            is_shared=True,        ),    '\n",
      " '],)with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"Hello\"},    '\n",
      " 'config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": '\n",
      " '\"1\"}},)Examples with runnables of different signatures\\u200bThe above '\n",
      " 'runnable takes a dict as input and returns a BaseMessage. Below we show some '\n",
      " 'alternatives.Messages input, dict output\\u200bfrom langchain_core.messages '\n",
      " 'import HumanMessagefrom langchain_core.runnables import '\n",
      " 'RunnableParallelchain = RunnableParallel({\"output_message\": '\n",
      " 'ChatOpenAI()})def get_session_history(session_id: str) -> '\n",
      " 'BaseChatMessageHistory:    if session_id not in store:        '\n",
      " 'store[session_id] = ChatMessageHistory()    return '\n",
      " 'store[session_id]with_message_history = RunnableWithMessageHistory(    '\n",
      " 'chain,    get_session_history,    '\n",
      " 'output_messages_key=\"output_message\",)with_message_history.invoke(    '\n",
      " '[HumanMessage(content=\"What did Simone de Beauvoir believe about free '\n",
      " 'will\")],    config={\"configurable\": {\"session_id\": '\n",
      " '\"baz\"}},){\\'output_message\\': AIMessage(content=\"Simone de Beauvoir believed '\n",
      " 'in the existence of free will. She argued that individuals have the ability '\n",
      " 'to make choices and determine their own actions, even in the face of social '\n",
      " 'and cultural constraints. She rejected the idea that individuals are purely '\n",
      " 'products of their environment or predetermined by biology or destiny. '\n",
      " 'Instead, she emphasized the importance of personal responsibility and the '\n",
      " 'need for individuals to actively engage in creating their own lives and '\n",
      " 'defining their own existence. De Beauvoir believed that freedom and agency '\n",
      " \"come from recognizing one's own freedom and actively exercising it in the \"\n",
      " 'pursuit of personal and collective '\n",
      " 'liberation.\")}with_message_history.invoke(    [HumanMessage(content=\"How did '\n",
      " 'this compare to Sartre\")],    config={\"configurable\": {\"session_id\": '\n",
      " '\"baz\"}},){\\'output_message\\': AIMessage(content=\\'Simone de Beauvoir\\\\\\'s '\n",
      " 'views on free will were closely aligned with those of her contemporary and '\n",
      " 'partner Jean-Paul Sartre. Both de Beauvoir and Sartre were existentialist '\n",
      " 'philosophers who emphasized the importance of individual freedom and the '\n",
      " 'rejection of determinism. They believed that human beings have the capacity '\n",
      " 'to transcend their circumstances and create their own meaning and '\n",
      " 'values.\\\\n\\\\nSartre, in his famous work \"Being and Nothingness,\" argued that '\n",
      " 'human beings are condemned to be free, meaning that we are burdened with the '\n",
      " 'responsibility of making choices and defining ourselves in a world that '\n",
      " 'lacks inherent meaning. Like de Beauvoir, Sartre believed that individuals '\n",
      " 'have the ability to exercise their freedom and make choices in the face of '\n",
      " 'external and internal constraints.\\\\n\\\\nWhile there may be some nuanced '\n",
      " 'differences in their philosophical writings, overall, de Beauvoir and Sartre '\n",
      " 'shared a similar belief in the existence of free will and the importance of '\n",
      " \"individual agency in shaping one\\\\'s own life.')}Messages input, messages \"\n",
      " 'output\\u200bRunnableWithMessageHistory(    ChatOpenAI(),    '\n",
      " 'get_session_history,)Dict with single key for all messages input, messages '\n",
      " 'output\\u200bfrom operator import itemgetterRunnableWithMessageHistory(    '\n",
      " 'itemgetter(\"input_messages\") | ChatOpenAI(),    get_session_history,    '\n",
      " 'input_messages_key=\"input_messages\",)Persistent storage\\u200bIn many cases '\n",
      " 'it is preferable to persist conversation histories. '\n",
      " 'RunnableWithMessageHistory is agnostic as to how the get_session_history '\n",
      " 'callable retrieves its chat message histories. See here for an example using '\n",
      " 'a local filesystem. Below we demonstrate how one could use Redis. Check out '\n",
      " 'the memory integrations page for implementations of chat message histories '\n",
      " \"using other providers.Setup\\u200bWe'll need to install Redis if it's not \"\n",
      " 'installed already:%pip install --upgrade --quiet redisStart a local Redis '\n",
      " \"Stack server if we don't have an existing Redis deployment to connect \"\n",
      " 'to:docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latestREDIS_URL '\n",
      " '= \"redis://localhost:6379/0\"LangSmith\\u200bLangSmith is especially useful '\n",
      " 'for something like message history injection, where it can be hard to '\n",
      " 'otherwise understand what the inputs are to various parts of the chain.Note '\n",
      " 'that LangSmith is not needed, but it is helpful.\\n'\n",
      " 'If you do want to use LangSmith, after you sign up at the link above, make '\n",
      " 'sure to uncoment the below and set your environment variables to start '\n",
      " 'logging traces:# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"# '\n",
      " 'os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()Updating the message '\n",
      " 'history implementation just requires us to define a new callable, this time '\n",
      " 'returning an instance of RedisChatMessageHistory:from '\n",
      " 'langchain_community.chat_message_histories import RedisChatMessageHistorydef '\n",
      " 'get_message_history(session_id: str) -> RedisChatMessageHistory:    return '\n",
      " 'RedisChatMessageHistory(session_id, url=REDIS_URL)with_message_history = '\n",
      " 'RunnableWithMessageHistory(    runnable,    get_message_history,    '\n",
      " 'input_messages_key=\"input\",    history_messages_key=\"history\",)We can invoke '\n",
      " 'as before:with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What '\n",
      " 'does cosine mean?\"},    config={\"configurable\": {\"session_id\": '\n",
      " '\"foobar\"}},)AIMessage(content=\\'Cosine is a trigonometric function that '\n",
      " 'represents the ratio of the adjacent side to the hypotenuse in a right '\n",
      " 'triangle.\\')with_message_history.invoke(    {\"ability\": \"math\", \"input\": '\n",
      " '\"What\\'s its inverse\"},    config={\"configurable\": {\"session_id\": '\n",
      " '\"foobar\"}},)AIMessage(content=\\'The inverse of cosine is the arccosine '\n",
      " 'function, denoted as acos or cos^-1, which gives the angle corresponding to '\n",
      " \"a given cosine value.')tipLangsmith traceLooking at the Langsmith trace for \"\n",
      " 'the second call, we can see that when constructing the prompt, a \"history\" '\n",
      " 'variable has been injected which is a list of two messages (our first input '\n",
      " 'and first output).Help us out by providing feedback on this documentation '\n",
      " 'page:PreviousStreamingNextRoute logic based on inputIn-memoryExamples with '\n",
      " 'runnables of different signaturesPersistent '\n",
      " 'storageSetupLangSmithCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Create a runnable with the @chain decorator | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreCreate a runnable with the @chain decoratorCreate a runnable '\n",
      " 'with the @chain decoratorYou can also turn an arbitrary function into a '\n",
      " 'chain by adding a @chain decorator. This is functionaly equivalent to '\n",
      " 'wrapping in a RunnableLambda.This will have the benefit of improved '\n",
      " 'observability by tracing your chain correctly. Any calls to runnables inside '\n",
      " 'this function will be traced as nested childen.It will also allow you to use '\n",
      " \"this as any other runnable, compose it in chain, etc.Let's take a look at \"\n",
      " 'this in action!%pip install --upgrade --quiet  langchain '\n",
      " 'langchain-openaifrom langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import chainfrom langchain_openai import '\n",
      " 'ChatOpenAIprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about '\n",
      " '{topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of '\n",
      " 'this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = '\n",
      " 'prompt1.invoke({\"topic\": text})    output1 = '\n",
      " 'ChatOpenAI().invoke(prompt_val1)    parsed_output1 = '\n",
      " 'StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | '\n",
      " 'StrOutputParser()    return chain2.invoke({\"joke\": '\n",
      " 'parsed_output1})custom_chain is now a runnable, meaning you will need to use '\n",
      " 'invokecustom_chain.invoke(\"bears\")\\'The subject of this joke is bears.\\'If '\n",
      " 'you check out your LangSmith traces, you should see a custom_chain trace in '\n",
      " 'there, with the calls to OpenAI nested underneathHelp us out by providing '\n",
      " 'feedback on this documentation page:PreviousInspect your '\n",
      " 'runnablesNextManaging prompt '\n",
      " 'sizeCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Multiple chains | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreMultiple chainsOn this pageMultiple chainsRunnables can easily '\n",
      " 'be used to string together multiple Chains%pip install --upgrade --quiet  '\n",
      " 'langchain langchain-openaifrom operator import itemgetterfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import '\n",
      " 'ChatOpenAIprompt1 = ChatPromptTemplate.from_template(\"what is the city '\n",
      " '{person} is from?\")prompt2 = ChatPromptTemplate.from_template(    \"what '\n",
      " 'country is the city {city} in? respond in {language}\")model = '\n",
      " 'ChatOpenAI()chain1 = prompt1 | model | StrOutputParser()chain2 = (    '\n",
      " '{\"city\": chain1, \"language\": itemgetter(\"language\")}    | prompt2    | '\n",
      " 'model    | StrOutputParser())chain2.invoke({\"person\": \"obama\", \"language\": '\n",
      " '\"spanish\"})\\'El pa√≠s donde se encuentra la ciudad de Honolulu, donde naci√≥ '\n",
      " 'Barack Obama, el 44¬∫ Presidente de los Estados Unidos, es Estados Unidos. '\n",
      " \"Honolulu se encuentra en la isla de Oahu, en el estado de Haw√°i.'from \"\n",
      " 'langchain_core.runnables import RunnablePassthroughprompt1 = '\n",
      " 'ChatPromptTemplate.from_template(    \"generate a {attribute} color. Return '\n",
      " 'the name of the color and nothing else:\")prompt2 = '\n",
      " 'ChatPromptTemplate.from_template(    \"what is a fruit of color: {color}. '\n",
      " 'Return the name of the fruit and nothing else:\")prompt3 = '\n",
      " 'ChatPromptTemplate.from_template(    \"what is a country with a flag that has '\n",
      " 'the color: {color}. Return the name of the country and nothing '\n",
      " 'else:\")prompt4 = ChatPromptTemplate.from_template(    \"What is the color of '\n",
      " '{fruit} and the flag of {country}?\")model_parser = model | '\n",
      " 'StrOutputParser()color_generator = (    {\"attribute\": RunnablePassthrough()} '\n",
      " '| prompt1 | {\"color\": model_parser})color_to_fruit = prompt2 | '\n",
      " 'model_parsercolor_to_country = prompt3 | model_parserquestion_generator = '\n",
      " '(    color_generator | {\"fruit\": color_to_fruit, \"country\": '\n",
      " 'color_to_country} | '\n",
      " 'prompt4)question_generator.invoke(\"warm\")ChatPromptValue(messages=[HumanMessage(content=\\'What '\n",
      " \"is the color of strawberry and the flag of China?', additional_kwargs={}, \"\n",
      " 'example=False)])prompt = '\n",
      " 'question_generator.invoke(\"warm\")model.invoke(prompt)AIMessage(content=\\'The '\n",
      " 'color of an apple is typically red or green. The flag of China is '\n",
      " 'predominantly red with a large yellow star in the upper left corner and four '\n",
      " \"smaller yellow stars surrounding it.', additional_kwargs={}, \"\n",
      " 'example=False)Branching and Merging\\u200bYou may want the output of one '\n",
      " 'component to be processed by 2 or more other components. RunnableParallels '\n",
      " 'let you split or fork the chain so multiple components can process the input '\n",
      " 'in parallel. Later, other components can join or merge the results to '\n",
      " 'synthesize a final response. This type of chain creates a computation graph '\n",
      " 'that looks like the following:     Input      / \\\\     /   \\\\ Branch1 '\n",
      " 'Branch2     \\\\   /      \\\\ /      Combineplanner = (    '\n",
      " 'ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")    | '\n",
      " 'ChatOpenAI()    | StrOutputParser()    | {\"base_response\": '\n",
      " 'RunnablePassthrough()})arguments_for = (    '\n",
      " 'ChatPromptTemplate.from_template(        \"List the pros or positive aspects '\n",
      " 'of {base_response}\"    )    | ChatOpenAI()    | '\n",
      " 'StrOutputParser())arguments_against = (    '\n",
      " 'ChatPromptTemplate.from_template(        \"List the cons or negative aspects '\n",
      " 'of {base_response}\"    )    | ChatOpenAI()    | '\n",
      " 'StrOutputParser())final_responder = (    '\n",
      " 'ChatPromptTemplate.from_messages(        [            (\"ai\", '\n",
      " '\"{original_response}\"),            (\"human\", '\n",
      " '\"Pros:\\\\n{results_1}\\\\n\\\\nCons:\\\\n{results_2}\"),            (\"system\", '\n",
      " '\"Generate a final response given the critique\"),        ]    )    | '\n",
      " 'ChatOpenAI()    | StrOutputParser())chain = (    planner    | {        '\n",
      " '\"results_1\": arguments_for,        \"results_2\": arguments_against,        '\n",
      " '\"original_response\": itemgetter(\"base_response\"),    }    | '\n",
      " 'final_responder)chain.invoke({\"input\": \"scrum\"})\\'While Scrum has its '\n",
      " 'potential cons and challenges, many organizations have successfully embraced '\n",
      " 'and implemented this project management framework to great effect. The cons '\n",
      " 'mentioned above can be mitigated or overcome with proper training, support, '\n",
      " 'and a commitment to continuous improvement. It is also important to note '\n",
      " 'that not all cons may be applicable to every organization or '\n",
      " 'project.\\\\n\\\\nFor example, while Scrum may be complex initially, with proper '\n",
      " 'training and guidance, teams can quickly grasp the concepts and practices. '\n",
      " 'The lack of predictability can be mitigated by implementing techniques such '\n",
      " 'as velocity tracking and release planning. The limited documentation can be '\n",
      " 'addressed by maintaining a balance between lightweight documentation and '\n",
      " 'clear communication among team members. The dependency on team collaboration '\n",
      " 'can be improved through effective communication channels and regular '\n",
      " 'team-building activities.\\\\n\\\\nScrum can be scaled and adapted to larger '\n",
      " 'projects by using frameworks like Scrum of Scrums or LeSS (Large Scale '\n",
      " 'Scrum). Concerns about speed versus quality can be addressed by '\n",
      " 'incorporating quality assurance practices, such as continuous integration '\n",
      " 'and automated testing, into the Scrum process. Scope creep can be managed by '\n",
      " 'having a well-defined and prioritized product backlog, and a strong product '\n",
      " 'owner can be developed through training and mentorship.\\\\n\\\\nResistance to '\n",
      " 'change can be overcome by providing proper education and communication to '\n",
      " 'stakeholders and involving them in the decision-making process. Ultimately, '\n",
      " 'the cons of Scrum can be seen as opportunities for growth and improvement, '\n",
      " 'and with the right mindset and support, they can be effectively '\n",
      " 'managed.\\\\n\\\\nIn conclusion, while Scrum may have its challenges and '\n",
      " 'potential cons, the benefits and advantages it offers in terms of '\n",
      " 'collaboration, flexibility, adaptability, transparency, and customer '\n",
      " 'satisfaction make it a widely adopted and successful project management '\n",
      " 'framework. With proper implementation and continuous improvement, '\n",
      " 'organizations can leverage Scrum to drive innovation, efficiency, and '\n",
      " \"project success.'Help us out by providing feedback on this documentation \"\n",
      " 'page:PreviousManaging prompt sizeNextü¶úüõ†Ô∏è LangSmithBranching and '\n",
      " 'MergingCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Lambda: Run custom functions | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesLambda: Run custom functionsOn '\n",
      " 'this pageRun custom functionsYou can use arbitrary functions in the '\n",
      " 'pipeline.Note that all inputs to these functions need to be a SINGLE '\n",
      " 'argument. If you have a function that accepts multiple arguments, you should '\n",
      " 'write a wrapper that accepts a single input and unpacks it into multiple '\n",
      " 'argument.\\n'\n",
      " '%pip install --upgrade --quiet langchain langchain-openaifrom operator '\n",
      " 'import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnableLambdafrom langchain_openai import '\n",
      " 'ChatOpenAIdef length_function(text):    return len(text)def '\n",
      " '_multiple_length_function(text1, text2):    return len(text1) * '\n",
      " 'len(text2)def multiple_length_function(_dict):    return '\n",
      " '_multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])prompt = '\n",
      " 'ChatPromptTemplate.from_template(\"what is {a} + {b}\")model = '\n",
      " 'ChatOpenAI()chain1 = prompt | modelchain = (    {        \"a\": '\n",
      " 'itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": '\n",
      " 'itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | '\n",
      " 'RunnableLambda(multiple_length_function),    }    | prompt    | '\n",
      " 'model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})AIMessage(content=\\'3 + 9 = '\n",
      " \"12', response_metadata={'token_usage': {'completion_tokens': 7, \"\n",
      " \"'prompt_tokens': 14, 'total_tokens': 21}, 'model_name': 'gpt-3.5-turbo', \"\n",
      " \"'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': \"\n",
      " \"None}, id='run-bd204541-81fd-429a-ad92-dd1913af9b1c-0')Accepting a Runnable \"\n",
      " 'Config\\u200bRunnable lambdas can optionally accept a RunnableConfig, which '\n",
      " 'they can use to pass callbacks, tags, and other configuration information to '\n",
      " 'nested runs.from langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.runnables import RunnableConfigimport jsondef '\n",
      " 'parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        '\n",
      " 'ChatPromptTemplate.from_template(            \"Fix the following '\n",
      " 'text:\\\\n\\\\n```text\\\\n{input}\\\\n```\\\\nError: {error}\"            \" Don\\'t '\n",
      " 'narrate, just respond with the fixed data.\"        )        | '\n",
      " 'ChatOpenAI()        | StrOutputParser()    )    for _ in range(3):        '\n",
      " 'try:            return json.loads(text)        except Exception as '\n",
      " 'e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, '\n",
      " 'config)    return \"Failed to parse\"from langchain_community.callbacks import '\n",
      " 'get_openai_callbackwith get_openai_callback() as cb:    output = '\n",
      " 'RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": '\n",
      " '[\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb){\\'foo\\': '\n",
      " \"'bar'}Tokens Used: 62    Prompt Tokens: 56    Completion Tokens: 6Successful \"\n",
      " 'Requests: 1Total Cost (USD): $9.6e-05StreamingYou can use generator '\n",
      " 'functions (ie. functions that use the yield keyword, and behave like '\n",
      " 'iterators) in a LCEL pipeline.The signature of these generators should be '\n",
      " 'Iterator[Input] -> Iterator[Output]. Or for async generators: '\n",
      " 'AsyncIterator[Input] -> AsyncIterator[Output].These are useful '\n",
      " 'for:implementing a custom output parsermodifying the output of a previous '\n",
      " \"step, while preserving streaming capabilitiesHere's an example of a custom \"\n",
      " 'output parser for comma-separated lists:from typing import Iterator, '\n",
      " 'Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated '\n",
      " 'list of 5 animals similar to: {animal}. Do not include numbers\")model = '\n",
      " 'ChatOpenAI(temperature=0.0)str_chain = prompt | model | StrOutputParser()for '\n",
      " 'chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", '\n",
      " 'flush=True)lion, tiger, wolf, gorilla, pandastr_chain.invoke({\"animal\": '\n",
      " '\"bear\"})\\'lion, tiger, wolf, gorilla, panda\\'# This is a custom parser that '\n",
      " 'splits an iterator of llm tokens# into a list of strings separated by '\n",
      " 'commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # '\n",
      " 'hold partial input until we get a comma    buffer = \"\"    for chunk in '\n",
      " 'input:        # add current chunk to buffer        buffer += chunk        # '\n",
      " 'while there are commas in the buffer        while \",\" in buffer:            '\n",
      " '# split buffer on comma            comma_index = '\n",
      " 'buffer.index(\",\")            # yield everything before the comma            '\n",
      " 'yield [buffer[:comma_index].strip()]            # save the rest for the next '\n",
      " 'iteration            buffer = buffer[comma_index + 1 :]    # yield the last '\n",
      " 'chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor '\n",
      " 'chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, '\n",
      " 'flush=True)[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']list_chain.invoke({\"animal\": '\n",
      " '\"bear\"})[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'elephant\\']Async '\n",
      " 'version\\u200bfrom typing import AsyncIteratorasync def asplit_into_list(    '\n",
      " 'input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    '\n",
      " 'buffer = \"\"    async for (        chunk    ) in input:  # `input` is a '\n",
      " '`async_generator` object, so use `async for`        buffer += chunk        '\n",
      " 'while \",\" in buffer:            comma_index = buffer.index(\",\")            '\n",
      " 'yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index '\n",
      " '+ 1 :]    yield [buffer.strip()]list_chain = str_chain | '\n",
      " 'asplit_into_listasync for chunk in list_chain.astream({\"animal\": '\n",
      " '\"bear\"}):    print(chunk, '\n",
      " \"flush=True)['lion']['tiger']['wolf']['gorilla']['panda']await \"\n",
      " 'list_chain.ainvoke({\"animal\": \"bear\"})[\\'lion\\', \\'tiger\\', \\'wolf\\', '\n",
      " \"'gorilla', 'panda']Help us out by providing feedback on this documentation \"\n",
      " 'page:PreviousBinding: Attach runtime argsNextPassthrough: Pass through '\n",
      " 'inputsAccepting a Runnable ConfigAsync '\n",
      " 'versionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Binding: Attach runtime args | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesBinding: Attach runtime argsOn '\n",
      " 'this pageBinding: Attach runtime argsSometimes we want to invoke a Runnable '\n",
      " 'within a Runnable sequence with constant arguments that are not part of the '\n",
      " 'output of the preceding Runnable in the sequence, and which are not part of '\n",
      " 'the user input. We can use Runnable.bind() to pass these arguments '\n",
      " 'in.Suppose we have a simple prompt + model sequence:%pip install --upgrade '\n",
      " '--quiet  langchain langchain-openaifrom langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAIprompt = ChatPromptTemplate.from_messages(    [        '\n",
      " '(            \"system\",            \"Write out the following equation using '\n",
      " 'algebraic symbols then solve it. Use the '\n",
      " 'format\\\\n\\\\nEQUATION:...\\\\nSOLUTION:...\\\\n\\\\n\",        ),        (\"human\", '\n",
      " '\"{equation_statement}\"),    ])model = ChatOpenAI(temperature=0)runnable = '\n",
      " '(    {\"equation_statement\": RunnablePassthrough()} | prompt | model | '\n",
      " 'StrOutputParser())print(runnable.invoke(\"x raised to the third plus seven '\n",
      " 'equals 12\"))EQUATION: x^3 + 7 = 12SOLUTION:Subtracting 7 from both sides of '\n",
      " 'the equation, we get:x^3 = 12 - 7x^3 = 5Taking the cube root of both sides, '\n",
      " 'we get:x = ‚àõ5Therefore, the solution to the equation x^3 + 7 = 12 is x = '\n",
      " '‚àõ5.and want to call the model with certain stop words:runnable = (    '\n",
      " '{\"equation_statement\": RunnablePassthrough()}    | prompt    | '\n",
      " 'model.bind(stop=\"SOLUTION\")    | StrOutputParser())print(runnable.invoke(\"x '\n",
      " 'raised to the third plus seven equals 12\"))EQUATION: x^3 + 7 = 12Attaching '\n",
      " 'OpenAI functions\\u200bOne particularly useful application of binding is to '\n",
      " 'attach OpenAI functions to a compatible OpenAI model:function = {    \"name\": '\n",
      " '\"solver\",    \"description\": \"Formulates and solves an equation\",    '\n",
      " '\"parameters\": {        \"type\": \"object\",        \"properties\": {            '\n",
      " '\"equation\": {                \"type\": \"string\",                \"description\": '\n",
      " '\"The algebraic expression of the equation\",            },            '\n",
      " '\"solution\": {                \"type\": \"string\",                \"description\": '\n",
      " '\"The solution to the equation\",            },        },        \"required\": '\n",
      " '[\"equation\", \"solution\"],    },}# Need gpt-4 to solve this one '\n",
      " 'correctlyprompt = ChatPromptTemplate.from_messages(    [        (            '\n",
      " '\"system\",            \"Write out the following equation using algebraic '\n",
      " 'symbols then solve it.\",        ),        (\"human\", '\n",
      " '\"{equation_statement}\"),    ])model = ChatOpenAI(model=\"gpt-4\", '\n",
      " 'temperature=0).bind(    function_call={\"name\": \"solver\"}, '\n",
      " 'functions=[function])runnable = {\"equation_statement\": '\n",
      " 'RunnablePassthrough()} | prompt | modelrunnable.invoke(\"x raised to the '\n",
      " 'third plus seven equals 12\")AIMessage(content=\\'\\', '\n",
      " \"additional_kwargs={'function_call': {'name': 'solver', 'arguments': \"\n",
      " '\\'{\\\\n\"equation\": \"x^3 + 7 = 12\",\\\\n\"solution\": \"x = ‚àõ5\"\\\\n}\\'}}, '\n",
      " 'example=False)Attaching OpenAI tools\\u200btools = [    {        \"type\": '\n",
      " '\"function\",        \"function\": {            \"name\": '\n",
      " '\"get_current_weather\",            \"description\": \"Get the current weather in '\n",
      " 'a given location\",            \"parameters\": {                \"type\": '\n",
      " '\"object\",                \"properties\": {                    \"location\": '\n",
      " '{                        \"type\": \"string\",                        '\n",
      " '\"description\": \"The city and state, e.g. San Francisco, '\n",
      " 'CA\",                    },                    \"unit\": {\"type\": \"string\", '\n",
      " '\"enum\": [\"celsius\", \"fahrenheit\"]},                },                '\n",
      " '\"required\": [\"location\"],            },        },    }]model = '\n",
      " 'ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools)model.invoke(\"What\\'s '\n",
      " 'the weather in SF, NYC and LA?\")AIMessage(content=\\'\\', '\n",
      " \"additional_kwargs={'tool_calls': [{'id': 'call_zHN0ZHwrxM7nZDdqTp6dkPko', \"\n",
      " '\\'function\\': {\\'arguments\\': \\'{\"location\": \"San Francisco, CA\", \"unit\": '\n",
      " '\"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, '\n",
      " \"{'id': 'call_aqdMm9HBSlFW9c9rqxTa7eQv', 'function': {'arguments': \"\n",
      " '\\'{\"location\": \"New York, NY\", \"unit\": \"celsius\"}\\', \\'name\\': '\n",
      " \"'get_current_weather'}, 'type': 'function'}, {'id': \"\n",
      " \"'call_cx8E567zcLzYV2WSWVgO63f1', 'function': {'arguments': \"\n",
      " '\\'{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}\\', \\'name\\': '\n",
      " \"'get_current_weather'}, 'type': 'function'}]})Help us out by providing \"\n",
      " 'feedback on this documentation page:PreviousParallel: Format dataNextLambda: '\n",
      " 'Run custom functionsAttaching OpenAI functionsAttaching OpenAI '\n",
      " 'toolsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Passthrough: Pass through inputs | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesPassthrough: Pass through '\n",
      " 'inputsOn this pagePassing data throughRunnablePassthrough on its own allows '\n",
      " 'you to pass inputs unchanged. This typically is used in conjuction with '\n",
      " 'RunnableParallel to pass data through to a new key in the map. See the '\n",
      " 'example below:%pip install --upgrade --quiet  langchain langchain-openaifrom '\n",
      " 'langchain_core.runnables import RunnableParallel, '\n",
      " 'RunnablePassthroughrunnable = RunnableParallel(    '\n",
      " 'passed=RunnablePassthrough(),    modified=lambda x: x[\"num\"] + '\n",
      " '1,)runnable.invoke({\"num\": 1}){\\'passed\\': {\\'num\\': 1}, \\'extra\\': '\n",
      " \"{'num': 1, 'mult': 3}, 'modified': 2}As seen above, passed key was called \"\n",
      " \"with RunnablePassthrough() and so it simply passed on {'num': 1}. We also \"\n",
      " 'set a second key in the map with modified. This uses a lambda to set a '\n",
      " 'single value adding 1 to the num, which resulted in modified key with the '\n",
      " 'value of 2.Retrieval Example\\u200bIn the example below, we see a use case '\n",
      " 'where we use RunnablePassthrough along with RunnableParallel. from '\n",
      " 'langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)model = '\n",
      " 'ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": '\n",
      " 'RunnablePassthrough()}    | prompt    | model    | '\n",
      " 'StrOutputParser())retrieval_chain.invoke(\"where did harrison '\n",
      " 'work?\")\\'Harrison worked at Kensho.\\'Here the input to prompt is expected to '\n",
      " 'be a map with keys \"context\" and \"question\". The user input is just the '\n",
      " 'question. So we need to get the context using our retriever and passthrough '\n",
      " 'the user input under the \"question\" key. In this case, the '\n",
      " \"RunnablePassthrough allows us to pass on the user's question to the prompt \"\n",
      " 'and model.Help us out by providing feedback on this documentation '\n",
      " 'page:PreviousLambda: Run custom functionsNextAssign: Add values to '\n",
      " 'stateRetrieval '\n",
      " 'ExampleCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Sequences: Chaining runnables | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesSequences: Chaining '\n",
      " 'runnablesOn this pageChaining runnablesOne key advantage of the Runnable '\n",
      " 'interface is that any two runnables can be \"chained\" together into '\n",
      " \"sequences. The output of the previous runnable's .invoke() call is passed as \"\n",
      " 'input to the next runnable. This can be done using the pipe operator (|), or '\n",
      " 'the more explicit .pipe() method, which does the same thing. The resulting '\n",
      " 'RunnableSequence is itself a runnable, which means it can be invoked, '\n",
      " 'streamed, or piped just like any other runnable.The pipe operator\\u200bTo '\n",
      " \"show off how this works, let's go through an example. We'll walk through a \"\n",
      " 'common pattern in LangChain: using a prompt template to format input into a '\n",
      " 'chat model, and finally converting the chat message output into a string '\n",
      " 'with an output parser.%pip install --upgrade --quiet langchain '\n",
      " 'langchain-anthropicfrom langchain_anthropic import ChatAnthropicfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplateprompt = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")model = '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")chain = prompt | model | '\n",
      " 'StrOutputParser()Prompts and models are both runnable, and the output type '\n",
      " 'from the prompt call is the same as the input type of the chat model, so we '\n",
      " 'can chain them together. We can then invoke the resulting sequence like any '\n",
      " 'other runnable:chain.invoke({\"topic\": \"bears\"})\"Here\\'s a bear joke for '\n",
      " \"you:\\\\n\\\\nWhy don't bears wear socks? \\\\nBecause they have bear \"\n",
      " \"feet!\\\\n\\\\nHow's that? I tried to keep it light and silly. Bears can make \"\n",
      " \"for some fun puns and jokes. Let me know if you'd like to hear another \"\n",
      " 'one!\"Coercion\\u200bWe can even combine this chain with more runnables to '\n",
      " 'create another chain. This may involve some input/output formatting using '\n",
      " 'other types of runnables, depending on the required inputs and outputs of '\n",
      " \"the chain components.For example, let's say we wanted to compose the joke \"\n",
      " 'generating chain with another chain that evaluates whether or not the '\n",
      " 'generated joke was funny.We would need to be careful with how we format the '\n",
      " 'input into the next chain. In the below example, the dict in the chain is '\n",
      " 'automatically parsed and converted into a RunnableParallel, which runs all '\n",
      " 'of its values in parallel and returns a dict with the results.This happens '\n",
      " 'to be the same format the next prompt template expects. Here it is in '\n",
      " 'action:from langchain_core.output_parsers import '\n",
      " 'StrOutputParseranalysis_prompt = ChatPromptTemplate.from_template(\"is this a '\n",
      " 'funny joke? {joke}\")composed_chain = {\"joke\": chain} | analysis_prompt | '\n",
      " 'model | StrOutputParser()composed_chain.invoke({\"topic\": \"bears\"})\"That\\'s a '\n",
      " \"pretty classic and well-known bear pun joke. Whether it's considered funny \"\n",
      " 'is quite subjective, as humor is very personal. Some people may find that '\n",
      " 'type of pun-based joke amusing, while others may not find it that humorous. '\n",
      " 'Ultimately, the funniness of a joke is in the eye (or ear) of the beholder. '\n",
      " \"If you enjoyed the joke and got a chuckle out of it, then that's what \"\n",
      " 'matters most.\"Functions will also be coerced into runnables, so you can add '\n",
      " 'custom logic to your chains too. The below chain results in the same logical '\n",
      " 'flow as before:composed_chain_with_lambda = (    chain    | (lambda input: '\n",
      " '{\"joke\": input})    | analysis_prompt    | model    | '\n",
      " 'StrOutputParser())composed_chain_with_lambda.invoke({\"topic\": \"beets\"})\\'I '\n",
      " \"appreciate the effort, but I have to be honest - I didn\\\\'t find that joke \"\n",
      " 'particularly funny. Beet-themed puns can be quite hit-or-miss, and this one '\n",
      " 'falls more on the \"miss\" side for me. The premise is a bit too '\n",
      " 'straightforward and predictable. While I can see the logic behind it, the '\n",
      " \"punchline just doesn\\\\'t pack much of a comedic punch. \\\\n\\\\nThat said, I do \"\n",
      " 'admire your willingness to explore puns and wordplay around vegetables. '\n",
      " 'Cultivating a good sense of humor takes practice, and not every joke is '\n",
      " 'going to land. The important thing is to keep experimenting and finding what '\n",
      " 'works. Maybe try for a more unexpected or creative twist on beet-related '\n",
      " 'humor next time. But thanks for sharing - I always appreciate when humans '\n",
      " \"test out jokes on me, even if they don\\\\'t always make me laugh out \"\n",
      " \"loud.'However, keep in mind that using functions like this may interfere \"\n",
      " 'with operations like streaming. See this section for more information.The '\n",
      " '.pipe() method\\u200bWe could also compose the same sequence using the '\n",
      " \".pipe() method. Here's what that looks like:from langchain_core.runnables \"\n",
      " 'import RunnableParallelcomposed_chain_with_pipe = (    '\n",
      " 'RunnableParallel({\"joke\": chain})    .pipe(analysis_prompt)    '\n",
      " '.pipe(model)    '\n",
      " '.pipe(StrOutputParser()))composed_chain_with_pipe.invoke({\"topic\": '\n",
      " '\"battlestar galactica\"})\\'That\\\\\\'s a pretty good Battlestar '\n",
      " 'Galactica-themed pun! I appreciated the clever play on words with '\n",
      " '\"Centurion\" and \"center on.\" It\\\\\\'s the kind of nerdy, science '\n",
      " 'fiction-inspired humor that fans of the show would likely enjoy. The joke is '\n",
      " 'clever and demonstrates a good understanding of the Battlestar Galactica '\n",
      " \"universe. I\\\\'d be curious to hear any other Battlestar-related jokes you \"\n",
      " \"might have up your sleeve. As long as they don\\\\'t reproduce copyrighted \"\n",
      " \"material, I\\\\'m happy to provide my thoughts on the humor and appeal for \"\n",
      " \"fans of the show.'Help us out by providing feedback on this documentation \"\n",
      " 'page:PreviousPrimitivesNextParallel: Format dataThe pipe operatorCoercionThe '\n",
      " '.pipe() '\n",
      " 'methodCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Assign: Add values to state | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesAssign: Add values to stateOn '\n",
      " 'this pageAdding values to chain stateThe RunnablePassthrough.assign(...) '\n",
      " 'static method takes an input value and adds the extra arguments passed to '\n",
      " 'the assign function.This is useful when additively creating a dictionary to '\n",
      " \"use as input to a later step, which is a common LCEL pattern.Here's an \"\n",
      " 'example:%pip install --upgrade --quiet langchain '\n",
      " 'langchain-openai\\x1b[33mWARNING: You are using pip version 22.0.4; however, '\n",
      " 'version 24.0 is available.You should consider upgrading via the '\n",
      " \"'/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade \"\n",
      " \"pip' command.\\x1b[0m\\x1b[33m\\x1b[0mNote: you may need to restart the kernel \"\n",
      " 'to use updated packages.from langchain_core.runnables import '\n",
      " 'RunnableParallel, RunnablePassthroughrunnable = RunnableParallel(    '\n",
      " 'extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),    '\n",
      " 'modified=lambda x: x[\"num\"] + 1,)runnable.invoke({\"num\": 1}){\\'extra\\': '\n",
      " \"{'num': 1, 'mult': 3}, 'modified': 2}Let's break down what's happening \"\n",
      " 'here.The input to the chain is {\"num\": 1}. This is passed into a '\n",
      " 'RunnableParallel, which invokes the runnables it is passed in parallel with '\n",
      " 'that input.The value under the extra key is invoked. '\n",
      " 'RunnablePassthrough.assign() keeps the original keys in the input dict '\n",
      " '({\"num\": 1}), and assigns a new key called mult. The value is lambda x: '\n",
      " 'x[\"num\"] * 3), which is 3. Thus, the result is {\"num\": 1, \"mult\": 3}.{\"num\": '\n",
      " '1, \"mult\": 3} is returned to the RunnableParallel call, and is set as the '\n",
      " 'value to the key extra.At the same time, the modified key is called. The '\n",
      " 'result is 2, since the lambda extracts a key called \"num\" from its input and '\n",
      " \"adds one.Thus, the result is {'extra': {'num': 1, 'mult': 3}, 'modified': \"\n",
      " '2}.Streaming\\u200bOne nice feature of this method is that it allows values '\n",
      " \"to pass through as soon as they are available. To show this off, we'll use \"\n",
      " 'RunnablePassthrough.assign() to immediately return source docs in a '\n",
      " 'retrieval chain:from langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)model = '\n",
      " 'ChatOpenAI()generation_chain = prompt | model | '\n",
      " 'StrOutputParser()retrieval_chain = {    \"context\": retriever,    \"question\": '\n",
      " 'RunnablePassthrough(),} | '\n",
      " 'RunnablePassthrough.assign(output=generation_chain)stream = '\n",
      " 'retrieval_chain.stream(\"where did harrison work?\")for chunk in stream:    '\n",
      " \"print(chunk){'question': 'where did harrison work?'}{'context': \"\n",
      " \"[Document(page_content='harrison worked at kensho')]}{'output': \"\n",
      " \"''}{'output': 'H'}{'output': 'arrison'}{'output': ' worked'}{'output': ' \"\n",
      " \"at'}{'output': ' Kens'}{'output': 'ho'}{'output': '.'}{'output': ''}We can \"\n",
      " 'see that the first chunk contains the original \"question\" since that is '\n",
      " 'immediately available. The second chunk contains \"context\" since the '\n",
      " 'retriever finishes second. Finally, the output from the generation_chain '\n",
      " 'streams in chunks as soon as it is available.Help us out by providing '\n",
      " 'feedback on this documentation page:PreviousPassthrough: Pass through '\n",
      " 'inputsNextConfigure runtime chain '\n",
      " 'internalsStreamingCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Quickstart | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput '\n",
      " 'ParsersCustom Output ParserstypesRetrievalDocument loadersText '\n",
      " 'splittersEmbedding modelsVector '\n",
      " 'storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel '\n",
      " 'I/OOutput parsersQuickstartOn this pageQuickstartLanguage models output '\n",
      " 'text. But many times you may want to get more structured information than '\n",
      " 'just text back. This is where output parsers come in.Output parsers are '\n",
      " 'classes that help structure language model responses. There are two main '\n",
      " 'methods an output parser must implement:\"Get format instructions\": A method '\n",
      " 'which returns a string containing instructions for how the output of a '\n",
      " 'language model should be formatted.\"Parse\": A method which takes in a string '\n",
      " '(assumed to be the response from a language model) and parses it into some '\n",
      " 'structure.And then one optional one:\"Parse with prompt\": A method which '\n",
      " 'takes in a string (assumed to be the response from a language model) and a '\n",
      " 'prompt (assumed to be the prompt that generated such a response) and parses '\n",
      " 'it into some structure. The prompt is largely provided in the event the '\n",
      " 'OutputParser wants to retry or fix the output in some way, and needs '\n",
      " 'information from the prompt to do so.Get started\\u200bBelow we go over the '\n",
      " 'main type of output parser, the PydanticOutputParser.from '\n",
      " 'langchain.output_parsers import PydanticOutputParserfrom '\n",
      " 'langchain_core.prompts import PromptTemplatefrom langchain_core.pydantic_v1 '\n",
      " 'import BaseModel, Field, validatorfrom langchain_openai import OpenAImodel = '\n",
      " 'OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your '\n",
      " 'desired data structure.class Joke(BaseModel):    setup: str = '\n",
      " 'Field(description=\"question to set up a joke\")    punchline: str = '\n",
      " 'Field(description=\"answer to resolve the joke\")    # You can add custom '\n",
      " 'validation logic easily with Pydantic.    @validator(\"setup\")    def '\n",
      " 'question_ends_with_question_mark(cls, field):        if field[-1] != '\n",
      " '\"?\":            raise ValueError(\"Badly formed question!\")        return '\n",
      " 'field# Set up a parser + inject instructions into the prompt template.parser '\n",
      " '= PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    '\n",
      " 'template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    '\n",
      " 'input_variables=[\"query\"],    partial_variables={\"format_instructions\": '\n",
      " 'parser.get_format_instructions()},)# And a query intended to prompt a '\n",
      " 'language model to populate the data structure.prompt_and_model = prompt | '\n",
      " 'modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a '\n",
      " 'joke.\"})parser.invoke(output)Joke(setup=\\'Why did the chicken cross the '\n",
      " \"road?', punchline='To get to the other side!')LCEL\\u200bOutput parsers \"\n",
      " 'implement the Runnable interface, the basic building block of the LangChain '\n",
      " 'Expression Language (LCEL). This means they support invoke, ainvoke, stream, '\n",
      " 'astream, batch, abatch, astream_log calls.Output parsers accept a string or '\n",
      " 'BaseMessage as input and can return an arbitrary '\n",
      " \"type.parser.invoke(output)Joke(setup='Why did the chicken cross the road?', \"\n",
      " \"punchline='To get to the other side!')Instead of manually invoking the \"\n",
      " \"parser, we also could've just added it to our Runnable sequence:chain = \"\n",
      " 'prompt | model | parserchain.invoke({\"query\": \"Tell me a '\n",
      " 'joke.\"})Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To '\n",
      " \"get to the other side!')While all parsers support the streaming interface, \"\n",
      " 'only certain parsers can stream through partially parsed objects, since this '\n",
      " 'is highly dependent on the output type. Parsers which cannot construct '\n",
      " 'partial objects will simply yield the fully parsed output.The '\n",
      " 'SimpleJsonOutputParser for example can stream through partial outputs:from '\n",
      " 'langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = '\n",
      " 'PromptTemplate.from_template(    \"Return a JSON object with an `answer` key '\n",
      " 'that answers the following question: {question}\")json_parser = '\n",
      " 'SimpleJsonOutputParser()json_chain = json_prompt | model | '\n",
      " 'json_parserlist(json_chain.stream({\"question\": \"Who invented the '\n",
      " 'microscope?\"}))[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': '\n",
      " \"'Anton'}, {'answer': 'Antonie'}, {'answer': 'Antonie van'}, {'answer': \"\n",
      " \"'Antonie van Lee'}, {'answer': 'Antonie van Leeu'}, {'answer': 'Antonie van \"\n",
      " \"Leeuwen'}, {'answer': 'Antonie van Leeuwenho'}, {'answer': 'Antonie van \"\n",
      " \"Leeuwenhoek'}]While the PydanticOutputParser \"\n",
      " 'cannot:list(chain.stream({\"query\": \"Tell me a joke.\"}))[Joke(setup=\\'Why did '\n",
      " \"the chicken cross the road?', punchline='To get to the other side!')]Help us \"\n",
      " 'out by providing feedback on this documentation page:PreviousOutput '\n",
      " 'ParsersNextOutput ParsersGet '\n",
      " 'startedLCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Self-querying | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument '\n",
      " 'loadersText splittersEmbedding modelsVector storesRetrieversVector '\n",
      " 'store-backed retrieverRetrieversMultiQueryRetrieverContextual '\n",
      " 'compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector '\n",
      " 'RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store '\n",
      " 'retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn '\n",
      " 'this pageSelf-queryinginfoHead to Integrations for documentation on vector '\n",
      " 'stores with built-in support for self-querying.A self-querying retriever is '\n",
      " 'one that, as the name suggests, has the ability to query itself. '\n",
      " 'Specifically, given any natural language query, the retriever uses a '\n",
      " 'query-constructing LLM chain to write a structured query and then applies '\n",
      " 'that structured query to its underlying VectorStore. This allows the '\n",
      " 'retriever to not only use the user-input query for semantic similarity '\n",
      " 'comparison with the contents of stored documents but to also extract filters '\n",
      " 'from the user query on the metadata of stored documents and to execute those '\n",
      " \"filters.Get started\\u200bFor demonstration purposes we'll use a Chroma \"\n",
      " \"vector store. We've created a small demo set of documents that contain \"\n",
      " 'summaries of movies.Note: The self-query retriever requires you to have lark '\n",
      " 'package installed.%pip install --upgrade --quiet  lark langchain-chromafrom '\n",
      " 'langchain_chroma import Chromafrom langchain_core.documents import '\n",
      " 'Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    '\n",
      " 'Document(        page_content=\"A bunch of scientists bring back dinosaurs '\n",
      " 'and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, '\n",
      " '\"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo '\n",
      " 'DiCaprio gets lost in a dream within a dream within a dream within a '\n",
      " '...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", '\n",
      " '\"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / '\n",
      " 'detective gets lost in a series of dreams within dreams within dreams and '\n",
      " 'Inception reused the idea\",        metadata={\"year\": 2006, \"director\": '\n",
      " '\"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A '\n",
      " 'bunch of normal-sized women are supremely wholesome and some men pine after '\n",
      " 'them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": '\n",
      " '8.3},    ),    Document(        page_content=\"Toys come alive and have a '\n",
      " 'blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    '\n",
      " '),    Document(        page_content=\"Three men walk into the Zone, three men '\n",
      " 'walk out of the Zone\",        metadata={            \"year\": 1979,            '\n",
      " '\"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            '\n",
      " '\"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, '\n",
      " 'OpenAIEmbeddings())Creating our self-querying retriever\\u200bNow we can '\n",
      " \"instantiate our retriever. To do this we'll need to provide some information \"\n",
      " 'upfront about the metadata fields that our documents support and a short '\n",
      " 'description of the document contents.from '\n",
      " 'langchain.chains.query_constructor.base import AttributeInfofrom '\n",
      " 'langchain.retrievers.self_query.base import SelfQueryRetrieverfrom '\n",
      " 'langchain_openai import ChatOpenAImetadata_field_info = [    '\n",
      " 'AttributeInfo(        name=\"genre\",        description=\"The genre of the '\n",
      " \"movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', \"\n",
      " '\\'action\\', \\'animated\\']\",        type=\"string\",    ),    '\n",
      " 'AttributeInfo(        name=\"year\",        description=\"The year the movie '\n",
      " 'was released\",        type=\"integer\",    ),    AttributeInfo(        '\n",
      " 'name=\"director\",        description=\"The name of the movie director\",        '\n",
      " 'type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A '\n",
      " '1-10 rating for the movie\", type=\"float\"    ),]document_content_description '\n",
      " '= \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = '\n",
      " 'SelfQueryRetriever.from_llm(    llm,    vectorstore,    '\n",
      " 'document_content_description,    metadata_field_info,)Testing it '\n",
      " 'out\\u200bAnd now we can actually try using our retriever!# This example only '\n",
      " 'specifies a filterretriever.invoke(\"I want to watch a movie rated higher '\n",
      " 'than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men '\n",
      " \"walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': \"\n",
      " \"'thriller', 'rating': 9.9, 'year': 1979}), Document(page_content='A \"\n",
      " 'psychologist / detective gets lost in a series of dreams within dreams '\n",
      " \"within dreams and Inception reused the idea', metadata={'director': 'Satoshi \"\n",
      " \"Kon', 'rating': 8.6, 'year': 2006})]# This example specifies a query and a \"\n",
      " 'filterretriever.invoke(\"Has Greta Gerwig directed any movies about '\n",
      " 'women\")[Document(page_content=\\'A bunch of normal-sized women are supremely '\n",
      " \"wholesome and some men pine after them', metadata={'director': 'Greta \"\n",
      " \"Gerwig', 'rating': 8.3, 'year': 2019})]# This example specifies a composite \"\n",
      " 'filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction '\n",
      " 'film?\")[Document(page_content=\\'A psychologist / detective gets lost in a '\n",
      " \"series of dreams within dreams within dreams and Inception reused the idea', \"\n",
      " \"metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}), \"\n",
      " \"Document(page_content='Three men walk into the Zone, three men walk out of \"\n",
      " \"the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', \"\n",
      " \"'rating': 9.9, 'year': 1979})]# This example specifies a query and composite \"\n",
      " 'filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 '\n",
      " \"that's all about toys, and preferably is \"\n",
      " 'animated\")[Document(page_content=\\'Toys come alive and have a blast doing '\n",
      " \"so', metadata={'genre': 'animated', 'year': 1995})]Filter k\\u200bWe can also \"\n",
      " 'use the self query retriever to specify k: the number of documents to '\n",
      " 'fetch.We can do this by passing enable_limit=True to the '\n",
      " 'constructor.retriever = SelfQueryRetriever.from_llm(    llm,    '\n",
      " 'vectorstore,    document_content_description,    metadata_field_info,    '\n",
      " 'enable_limit=True,)# This example only specifies a relevant '\n",
      " 'queryretriever.invoke(\"What are two movies about '\n",
      " 'dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back '\n",
      " \"dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', \"\n",
      " \"'rating': 7.7, 'year': 1993}), Document(page_content='Toys come alive and \"\n",
      " \"have a blast doing so', metadata={'genre': 'animated', 'year': \"\n",
      " \"1995})]Constructing from scratch with LCEL\\u200bTo see what's going on under \"\n",
      " 'the hood, and to have more custom control, we can reconstruct our retriever '\n",
      " 'from scratch.First, we need to create a query-construction chain. This chain '\n",
      " 'will take a user query and generated a StructuredQuery object which captures '\n",
      " 'the filters specified by the user. We provide some helper functions for '\n",
      " 'creating a prompt and output parser. These have a number of tunable params '\n",
      " \"that we'll ignore here for simplicity.from \"\n",
      " 'langchain.chains.query_constructor.base import (    '\n",
      " 'StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = '\n",
      " 'get_query_constructor_prompt(    document_content_description,    '\n",
      " 'metadata_field_info,)output_parser = '\n",
      " 'StructuredQueryOutputParser.from_components()query_constructor = prompt | '\n",
      " \"llm | output_parserLet's look at our \"\n",
      " 'prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure '\n",
      " \"the user's query to match the request schema provided below.<< Structured \"\n",
      " 'Request Schema >>When responding use a markdown code snippet with a JSON '\n",
      " 'object formatted in the following schema:```json{    \"query\": string \\\\ text '\n",
      " 'string to compare to document contents    \"filter\": string \\\\ logical '\n",
      " 'condition statement for filtering documents}The query string should contain '\n",
      " 'only text that is expected to match the contents of documents. Any '\n",
      " 'conditions in the filter should not be mentioned in the query as well.A '\n",
      " 'logical condition statement is composed of one or more comparison and '\n",
      " 'logical operation statements.A comparison statement takes the form: '\n",
      " 'comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | '\n",
      " 'nin): comparatorattr (string):  name of attribute to apply the comparison '\n",
      " 'toval (string): is the comparison valueA logical operation statement takes '\n",
      " 'the form op(statement1, statement2, ...):op (and | or | not): logical '\n",
      " 'operatorstatement1, statement2, ... (comparison statements or logical '\n",
      " 'operation statements): one or more statements to apply the operation toMake '\n",
      " 'sure that you only use the comparators and logical operators listed above '\n",
      " 'and no others.\\n'\n",
      " 'Make sure that filters only refer to attributes that exist in the data '\n",
      " 'source.\\n'\n",
      " 'Make sure that filters only use the attributed names with its function names '\n",
      " 'if there are functions applied on them.\\n'\n",
      " 'Make sure that filters only use format YYYY-MM-DD when handling date data '\n",
      " 'typed values.\\n'\n",
      " 'Make sure that filters take into account the descriptions of attributes and '\n",
      " 'only make comparisons that are feasible given the type of data being '\n",
      " 'stored.\\n'\n",
      " 'Make sure that filters are only used as needed. If there are no filters that '\n",
      " 'should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        '\n",
      " '\"artist\": {            \"type\": \"string\",            \"description\": \"Name of '\n",
      " 'the song artist\"        },        \"length\": {            \"type\": '\n",
      " '\"integer\",            \"description\": \"Length of the song in seconds\"        '\n",
      " '},        \"genre\": {            \"type\": \"string\",            \"description\": '\n",
      " '\"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\n'\n",
      " 'What are songs by Taylor Swift or Katy Perry about teenage romance under 3 '\n",
      " 'minutes long in the dance pop genreStructured Request:{    \"query\": '\n",
      " '\"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), '\n",
      " 'eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", '\n",
      " '\\\\\"pop\\\\\"))\"}<< Example 2. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        '\n",
      " '\"artist\": {            \"type\": \"string\",            \"description\": \"Name of '\n",
      " 'the song artist\"        },        \"length\": {            \"type\": '\n",
      " '\"integer\",            \"description\": \"Length of the song in seconds\"        '\n",
      " '},        \"genre\": {            \"type\": \"string\",            \"description\": '\n",
      " '\"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\n'\n",
      " 'What are songs that were not published on SpotifyStructured Request:{    '\n",
      " '\"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": '\n",
      " '{    \"genre\": {        \"description\": \"The genre of the movie. One of '\n",
      " \"['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', \"\n",
      " '\\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        '\n",
      " '\"description\": \"The year the movie was released\",        \"type\": '\n",
      " '\"integer\"    },    \"director\": {        \"description\": \"The name of the '\n",
      " 'movie director\",        \"type\": \"string\"    },    \"rating\": {        '\n",
      " '\"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    '\n",
      " '}}}User Query:\\n'\n",
      " 'dummy questionStructured Request:And what our full chain '\n",
      " 'produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are '\n",
      " \"some sci-fi movies from the 90's directed by Luc Besson about taxi \"\n",
      " 'drivers\"    })StructuredQuery(query=\\'taxi driver\\', '\n",
      " \"filter=Operation(operator=<Operator.AND: 'and'>, \"\n",
      " \"arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', \"\n",
      " \"value='science fiction'), Operation(operator=<Operator.AND: 'and'>, \"\n",
      " \"arguments=[Comparison(comparator=<Comparator.GTE: 'gte'>, attribute='year', \"\n",
      " \"value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', \"\n",
      " \"value=2000)]), Comparison(comparator=<Comparator.EQ: 'eq'>, \"\n",
      " \"attribute='director', value='Luc Besson')]), limit=None)The query \"\n",
      " 'constructor is the key element of the self-query retriever. To make a great '\n",
      " \"retrieval system you'll need to make sure your query constructor works well. \"\n",
      " 'Often this requires adjusting the prompt, the examples in the prompt, the '\n",
      " 'attribute descriptions, etc. For an example that walks through refining a '\n",
      " 'query constructor on some hotel inventory data, check out this cookbook.The '\n",
      " 'next key element is the structured query translator. This is the object '\n",
      " 'responsible for translating the generic StructuredQuery object into a '\n",
      " \"metadata filter in the syntax of the vector store you're using. LangChain \"\n",
      " 'comes with a number of built-in translators. To see them all head to the '\n",
      " 'Integrations section.from langchain.retrievers.self_query.chroma import '\n",
      " 'ChromaTranslatorretriever = SelfQueryRetriever(    '\n",
      " 'query_constructor=query_constructor,    vectorstore=vectorstore,    '\n",
      " 'structured_query_translator=ChromaTranslator(),)retriever.invoke(    '\n",
      " '\"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and '\n",
      " 'preferably is animated\")[Document(page_content=\\'Toys come alive and have a '\n",
      " \"blast doing so', metadata={'genre': 'animated', 'year': 1995})]Help us out \"\n",
      " 'by providing feedback on this documentation page:PreviousParent Document '\n",
      " 'RetrieverNextTime-weighted vector store retrieverGet startedCreating our '\n",
      " 'self-querying retrieverTesting it outFilter kConstructing from scratch with '\n",
      " 'LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n']\n"
     ]
    }
   ],
   "source": [
    "pprint(docs_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate No. of tokens for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[736,\n",
       " 1356,\n",
       " 2395,\n",
       " 671,\n",
       " 10466,\n",
       " 4528,\n",
       " 16290,\n",
       " 787,\n",
       " 2790,\n",
       " 9447,\n",
       " 4084,\n",
       " 2435,\n",
       " 555,\n",
       " 1482,\n",
       " 1522,\n",
       " 1322,\n",
       " 739,\n",
       " 1416,\n",
       " 1061,\n",
       " 1099,\n",
       " 3048]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts= [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets plot histogram to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins= 50, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token count\")\n",
    "plt.xlabel(\"Token count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68229"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COncat all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Number of tokens in all content: 68289'\n"
     ]
    }
   ],
   "source": [
    "d_sorted= sorted(docs, key= lambda x: x.metadata['source'])\n",
    "d_reversed= list(reversed(d_sorted))\n",
    "concatenated_content= \"\\n\\n\\n ----- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "pprint(\"Number of tokens in all content: %s\"\n",
    "       % num_tokens_from_string(concatenated_content, \"cl100k_base\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok= 2000\n",
    "text_splitter= RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size= chunk_size_tok,\n",
    "                                                                    chunk_overlap=0)\n",
    "\n",
    "text_splits= text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quickstart | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput '\n",
      " 'ParsersCustom Output ParserstypesRetrievalDocument loadersText '\n",
      " 'splittersEmbedding modelsVector '\n",
      " 'storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel '\n",
      " 'I/OOutput parsersQuickstartOn this pageQuickstartLanguage models output '\n",
      " 'text. But many times you may want to get more structured information than '\n",
      " 'just text back. This is where output parsers come in.Output parsers are '\n",
      " 'classes that help structure language model responses. There are two main '\n",
      " 'methods an output parser must implement:\"Get format instructions\": A method '\n",
      " 'which returns a string containing instructions for how the output of a '\n",
      " 'language model should be formatted.\"Parse\": A method which takes in a string '\n",
      " '(assumed to be the response from a language model) and parses it into some '\n",
      " 'structure.And then one optional one:\"Parse with prompt\": A method which '\n",
      " 'takes in a string (assumed to be the response from a language model) and a '\n",
      " 'prompt (assumed to be the prompt that generated such a response) and parses '\n",
      " 'it into some structure. The prompt is largely provided in the event the '\n",
      " 'OutputParser wants to retry or fix the output in some way, and needs '\n",
      " 'information from the prompt to do so.Get started\\u200bBelow we go over the '\n",
      " 'main type of output parser, the PydanticOutputParser.from '\n",
      " 'langchain.output_parsers import PydanticOutputParserfrom '\n",
      " 'langchain_core.prompts import PromptTemplatefrom langchain_core.pydantic_v1 '\n",
      " 'import BaseModel, Field, validatorfrom langchain_openai import OpenAImodel = '\n",
      " 'OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your '\n",
      " 'desired data structure.class Joke(BaseModel):    setup: str = '\n",
      " 'Field(description=\"question to set up a joke\")    punchline: str = '\n",
      " 'Field(description=\"answer to resolve the joke\")    # You can add custom '\n",
      " 'validation logic easily with Pydantic.    @validator(\"setup\")    def '\n",
      " 'question_ends_with_question_mark(cls, field):        if field[-1] != '\n",
      " '\"?\":            raise ValueError(\"Badly formed question!\")        return '\n",
      " 'field# Set up a parser + inject instructions into the prompt template.parser '\n",
      " '= PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    '\n",
      " 'template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    '\n",
      " 'input_variables=[\"query\"],    partial_variables={\"format_instructions\": '\n",
      " 'parser.get_format_instructions()},)# And a query intended to prompt a '\n",
      " 'language model to populate the data structure.prompt_and_model = prompt | '\n",
      " 'modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a '\n",
      " 'joke.\"})parser.invoke(output)Joke(setup=\\'Why did the chicken cross the '\n",
      " \"road?', punchline='To get to the other side!')LCEL\\u200bOutput parsers \"\n",
      " 'implement the Runnable interface, the basic building block of the LangChain '\n",
      " 'Expression Language (LCEL). This means they support invoke, ainvoke, stream, '\n",
      " 'astream, batch, abatch, astream_log calls.Output parsers accept a string or '\n",
      " 'BaseMessage as input and can return an arbitrary '\n",
      " \"type.parser.invoke(output)Joke(setup='Why did the chicken cross the road?', \"\n",
      " \"punchline='To get to the other side!')Instead of manually invoking the \"\n",
      " \"parser, we also could've just added it to our Runnable sequence:chain = \"\n",
      " 'prompt | model | parserchain.invoke({\"query\": \"Tell me a '\n",
      " 'joke.\"})Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To '\n",
      " \"get to the other side!')While all parsers support the streaming interface, \"\n",
      " 'only certain parsers can stream through partially parsed objects, since this '\n",
      " 'is highly dependent on the output type. Parsers which cannot construct '\n",
      " 'partial objects will simply yield the fully parsed output.The '\n",
      " 'SimpleJsonOutputParser for example can stream through partial outputs:from '\n",
      " 'langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = '\n",
      " 'PromptTemplate.from_template(    \"Return a JSON object with an `answer` key '\n",
      " 'that answers the following question: {question}\")json_parser = '\n",
      " 'SimpleJsonOutputParser()json_chain = json_prompt | model | '\n",
      " 'json_parserlist(json_chain.stream({\"question\": \"Who invented the '\n",
      " 'microscope?\"}))[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': '\n",
      " \"'Anton'}, {'answer': 'Antonie'}, {'answer': 'Antonie van'}, {'answer': \"\n",
      " \"'Antonie van Lee'}, {'answer': 'Antonie van Leeu'}, {'answer': 'Antonie van \"\n",
      " \"Leeuwen'}, {'answer': 'Antonie van Leeuwenho'}, {'answer': 'Antonie van \"\n",
      " \"Leeuwenhoek'}]While the PydanticOutputParser \"\n",
      " 'cannot:list(chain.stream({\"query\": \"Tell me a joke.\"}))[Joke(setup=\\'Why did '\n",
      " \"the chicken cross the road?', punchline='To get to the other side!')]Help us \"\n",
      " 'out by providing feedback on this documentation page:PreviousOutput '\n",
      " 'ParsersNextOutput ParsersGet '\n",
      " 'startedLCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' ----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Self-querying | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument '\n",
      " 'loadersText splittersEmbedding modelsVector storesRetrieversVector '\n",
      " 'store-backed retrieverRetrieversMultiQueryRetrieverContextual '\n",
      " 'compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector '\n",
      " 'RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store '\n",
      " 'retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn '\n",
      " 'this pageSelf-queryinginfoHead to Integrations for documentation on vector '\n",
      " 'stores with built-in support for self-querying.A self-querying retriever is '\n",
      " 'one that, as the name suggests, has the ability to query itself. '\n",
      " 'Specifically, given any natural language query, the retriever uses a '\n",
      " 'query-constructing LLM chain to write a structured query and then applies '\n",
      " 'that structured query to its underlying VectorStore. This allows the '\n",
      " 'retriever to not only use the user-input query for semantic similarity '\n",
      " 'comparison with the contents of stored documents but to also extract filters '\n",
      " 'from the user query on the metadata of stored documents and to execute those '\n",
      " \"filters.Get started\\u200bFor demonstration purposes we'll use a Chroma \"\n",
      " \"vector store. We've created a small demo set of documents that contain \"\n",
      " 'summaries of movies.Note: The self-query retriever requires you to have lark '\n",
      " 'package installed.%pip install --upgrade --quiet  lark langchain-chromafrom '\n",
      " 'langchain_chroma import Chromafrom langchain_core.documents import '\n",
      " 'Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    '\n",
      " 'Document(        page_content=\"A bunch of scientists bring back dinosaurs '\n",
      " 'and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, '\n",
      " '\"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo '\n",
      " 'DiCaprio gets lost in a dream within a dream within a dream within a '\n",
      " '...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", '\n",
      " '\"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / '\n",
      " 'detective gets lost in a series of dreams within dreams within dreams and '\n",
      " 'Inception reused the idea\",        metadata={\"year\": 2006, \"director\": '\n",
      " '\"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A '\n",
      " 'bunch of normal-sized women are supremely wholesome and some men pine after '\n",
      " 'them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": '\n",
      " '8.3},    ),    Document(        page_content=\"Toys come alive and have a '\n",
      " 'blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    '\n",
      " '),    Document(        page_content=\"Three men walk into the Zone, three men '\n",
      " 'walk out of the Zone\",        metadata={            \"year\": 1979,            '\n",
      " '\"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            '\n",
      " '\"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, '\n",
      " 'OpenAIEmbeddings())Creating our self-querying retriever\\u200bNow we can '\n",
      " \"instantiate our retriever. To do this we'll need to provide some information \"\n",
      " 'upfront about the metadata fields that our documents support and a short '\n",
      " 'description of the document contents.from '\n",
      " 'langchain.chains.query_constructor.base import AttributeInfofrom '\n",
      " 'langchain.retrievers.self_query.base import SelfQueryRetrieverfrom '\n",
      " 'langchain_openai import ChatOpenAImetadata_field_info = [    '\n",
      " 'AttributeInfo(        name=\"genre\",        description=\"The genre of the '\n",
      " \"movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', \"\n",
      " '\\'action\\', \\'animated\\']\",        type=\"string\",    ),    '\n",
      " 'AttributeInfo(        name=\"year\",        description=\"The year the movie '\n",
      " 'was released\",        type=\"integer\",    ),    AttributeInfo(        '\n",
      " 'name=\"director\",        description=\"The name of the movie director\",        '\n",
      " 'type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A '\n",
      " '1-10 rating for the movie\", type=\"float\"    ),]document_content_description '\n",
      " '= \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = '\n",
      " 'SelfQueryRetriever.from_llm(    llm,    vectorstore,    '\n",
      " 'document_content_description,    metadata_field_info,)Testing it '\n",
      " 'out\\u200bAnd now we can actually try using our retriever!# This example only '\n",
      " 'specifies a filterretriever.invoke(\"I want to watch a movie rated higher '\n",
      " 'than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men '\n",
      " \"walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': \"\n",
      " \"'thriller', 'rating': 9.9, 'year': 1979}), Document(page_content='A \"\n",
      " 'psychologist / detective gets lost in a series of dreams within dreams '\n",
      " \"within dreams and Inception reused the idea', metadata={'director': 'Satoshi \"\n",
      " \"Kon', 'rating': 8.6, 'year': 2006})]# This example specifies a query and a \"\n",
      " 'filterretriever.invoke(\"Has Greta Gerwig directed any movies about '\n",
      " 'women\")[Document(page_content=\\'A bunch of normal-sized women are supremely '\n",
      " \"wholesome and some men pine after them', metadata={'director': 'Greta \"\n",
      " \"Gerwig', 'rating': 8.3, 'year': 2019})]# This example specifies a composite \"\n",
      " 'filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction '\n",
      " 'film?\")[Document(page_content=\\'A psychologist / detective gets lost in a '\n",
      " \"series of dreams within dreams within dreams and Inception reused the idea', \"\n",
      " \"metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}), \"\n",
      " \"Document(page_content='Three men walk into the Zone, three men walk out of \"\n",
      " \"the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', \"\n",
      " \"'rating': 9.9, 'year': 1979})]# This example specifies a query and composite \"\n",
      " 'filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 '\n",
      " \"that's all about toys, and preferably is \"\n",
      " 'animated\")[Document(page_content=\\'Toys come alive and have a blast doing '\n",
      " \"so', metadata={'genre': 'animated', 'year': 1995})]Filter k\\u200bWe can also \"\n",
      " 'use the self query retriever to specify k: the number of documents to '\n",
      " 'fetch.We can do this by passing enable_limit=True to the '\n",
      " 'constructor.retriever = SelfQueryRetriever.from_llm(    llm,    '\n",
      " 'vectorstore,    document_content_description,    metadata_field_info,    '\n",
      " 'enable_limit=True,)# This example only specifies a relevant '\n",
      " 'queryretriever.invoke(\"What are two movies about '\n",
      " 'dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back '\n",
      " \"dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', \"\n",
      " \"'rating': 7.7, 'year': 1993}), Document(page_content='Toys come alive and \"\n",
      " \"have a blast doing so', metadata={'genre': 'animated', 'year': \"\n",
      " \"1995})]Constructing from scratch with LCEL\\u200bTo see what's going on under \"\n",
      " 'the hood, and to have more custom control, we can reconstruct our retriever '\n",
      " 'from scratch.First, we need to create a query-construction chain. This chain '\n",
      " 'will take a user query and generated a StructuredQuery object which captures '\n",
      " 'the filters specified by the user. We provide some helper functions for '\n",
      " 'creating a prompt and output parser. These have a number of tunable params '\n",
      " \"that we'll ignore here for simplicity.from \"\n",
      " 'langchain.chains.query_constructor.base import (    '\n",
      " 'StructuredQueryOutputParser,',\n",
      " 'get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    '\n",
      " 'document_content_description,    metadata_field_info,)output_parser = '\n",
      " 'StructuredQueryOutputParser.from_components()query_constructor = prompt | '\n",
      " \"llm | output_parserLet's look at our \"\n",
      " 'prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure '\n",
      " \"the user's query to match the request schema provided below.<< Structured \"\n",
      " 'Request Schema >>When responding use a markdown code snippet with a JSON '\n",
      " 'object formatted in the following schema:```json{    \"query\": string \\\\ text '\n",
      " 'string to compare to document contents    \"filter\": string \\\\ logical '\n",
      " 'condition statement for filtering documents}The query string should contain '\n",
      " 'only text that is expected to match the contents of documents. Any '\n",
      " 'conditions in the filter should not be mentioned in the query as well.A '\n",
      " 'logical condition statement is composed of one or more comparison and '\n",
      " 'logical operation statements.A comparison statement takes the form: '\n",
      " 'comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | '\n",
      " 'nin): comparatorattr (string):  name of attribute to apply the comparison '\n",
      " 'toval (string): is the comparison valueA logical operation statement takes '\n",
      " 'the form op(statement1, statement2, ...):op (and | or | not): logical '\n",
      " 'operatorstatement1, statement2, ... (comparison statements or logical '\n",
      " 'operation statements): one or more statements to apply the operation toMake '\n",
      " 'sure that you only use the comparators and logical operators listed above '\n",
      " 'and no others.',\n",
      " 'Make sure that filters only refer to attributes that exist in the data '\n",
      " 'source.\\n'\n",
      " 'Make sure that filters only use the attributed names with its function names '\n",
      " 'if there are functions applied on them.\\n'\n",
      " 'Make sure that filters only use format YYYY-MM-DD when handling date data '\n",
      " 'typed values.\\n'\n",
      " 'Make sure that filters take into account the descriptions of attributes and '\n",
      " 'only make comparisons that are feasible given the type of data being '\n",
      " 'stored.\\n'\n",
      " 'Make sure that filters are only used as needed. If there are no filters that '\n",
      " 'should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        '\n",
      " '\"artist\": {            \"type\": \"string\",            \"description\": \"Name of '\n",
      " 'the song artist\"        },        \"length\": {            \"type\": '\n",
      " '\"integer\",            \"description\": \"Length of the song in seconds\"        '\n",
      " '},        \"genre\": {            \"type\": \"string\",            \"description\": '\n",
      " '\"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\n'\n",
      " 'What are songs by Taylor Swift or Katy Perry about teenage romance under 3 '\n",
      " 'minutes long in the dance pop genreStructured Request:{    \"query\": '\n",
      " '\"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), '\n",
      " 'eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", '\n",
      " '\\\\\"pop\\\\\"))\"}<< Example 2. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        '\n",
      " '\"artist\": {            \"type\": \"string\",            \"description\": \"Name of '\n",
      " 'the song artist\"        },        \"length\": {            \"type\": '\n",
      " '\"integer\",            \"description\": \"Length of the song in seconds\"        '\n",
      " '},        \"genre\": {            \"type\": \"string\",            \"description\": '\n",
      " '\"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\n'\n",
      " 'What are songs that were not published on SpotifyStructured Request:{    '\n",
      " '\"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": '\n",
      " '{    \"genre\": {        \"description\": \"The genre of the movie. One of '\n",
      " \"['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', \"\n",
      " '\\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        '\n",
      " '\"description\": \"The year the movie was released\",        \"type\": '\n",
      " '\"integer\"    },    \"director\": {        \"description\": \"The name of the '\n",
      " 'movie director\",        \"type\": \"string\"    },    \"rating\": {        '\n",
      " '\"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    '\n",
      " '}}}User Query:\\n'\n",
      " 'dummy questionStructured Request:And what our full chain '\n",
      " 'produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are '\n",
      " \"some sci-fi movies from the 90's directed by Luc Besson about taxi \"\n",
      " 'drivers\"    })StructuredQuery(query=\\'taxi driver\\', '\n",
      " \"filter=Operation(operator=<Operator.AND: 'and'>, \"\n",
      " \"arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', \"\n",
      " \"value='science fiction'), Operation(operator=<Operator.AND: 'and'>, \"\n",
      " \"arguments=[Comparison(comparator=<Comparator.GTE: 'gte'>, attribute='year', \"\n",
      " \"value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', \"\n",
      " \"value=2000)]), Comparison(comparator=<Comparator.EQ: 'eq'>, \"\n",
      " \"attribute='director', value='Luc Besson')]), limit=None)The query \"\n",
      " 'constructor is the key element of the self-query retriever. To make a great '\n",
      " \"retrieval system you'll need to make sure your query constructor works well. \"\n",
      " 'Often this requires adjusting the prompt, the examples in the prompt, the '\n",
      " 'attribute descriptions, etc. For an example that walks through refining a '\n",
      " 'query constructor on some hotel inventory data, check out this cookbook.The '\n",
      " 'next key element is the structured query translator. This is the object '\n",
      " 'responsible for translating the generic StructuredQuery object into a '\n",
      " \"metadata filter in the syntax of the vector store you're using. LangChain \"\n",
      " 'comes with a number of built-in translators. To see them all head to the '\n",
      " 'Integrations section.from langchain.retrievers.self_query.chroma import '\n",
      " 'ChromaTranslatorretriever = SelfQueryRetriever(    '\n",
      " 'query_constructor=query_constructor,    vectorstore=vectorstore,    '\n",
      " 'structured_query_translator=ChromaTranslator(),)retriever.invoke(    '\n",
      " '\"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and '\n",
      " 'preferably is animated\")[Document(page_content=\\'Toys come alive and have a '\n",
      " \"blast doing so', metadata={'genre': 'animated', 'year': 1995})]Help us out \"\n",
      " 'by providing feedback on this documentation page:PreviousParent Document '\n",
      " 'RetrieverNextTime-weighted vector store retrieverGet startedCreating our '\n",
      " 'self-querying retrieverTesting it outFilter kConstructing from scratch with '\n",
      " 'LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n\\n\\n\\n\\n\\n\\n\\nAdvantages of LCEL | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageAdvantages of LCELOn this pageAdvantages '\n",
      " 'of LCELtipWe recommend reading the LCEL Get started section first.LCEL is '\n",
      " 'designed to streamline the process of building useful apps with LLMs and '\n",
      " 'combining related components. It does this by providing:A unified interface: '\n",
      " 'Every LCEL object implements the Runnable interface, which defines a common '\n",
      " 'set of invocation methods (invoke, batch, stream, ainvoke, ...). This makes '\n",
      " 'it possible for chains of LCEL objects to also automatically support useful '\n",
      " 'operations like batching and streaming of intermediate steps, since every '\n",
      " 'chain of LCEL objects is itself an LCEL object.Composition primitives: LCEL '\n",
      " 'provides a number of primitives that make it easy to compose chains, '\n",
      " 'parallelize components, add fallbacks, dynamically configure chain '\n",
      " \"internals, and more.To better understand the value of LCEL, it's helpful to \"\n",
      " 'see it in action and think about how we might recreate similar functionality '\n",
      " \"without it. In this walkthrough we'll do just that with our basic example \"\n",
      " \"from the get started section. We'll take our simple prompt + model chain, \"\n",
      " 'which under the hood already defines a lot of functionality, and see what it '\n",
      " 'would take to recreate all of it.%pip install --upgrade --quiet  '\n",
      " 'langchain-core langchain-openai langchain-anthropicInvoke\\u200bIn the '\n",
      " 'simplest case, we just want to pass in a topic string and get back a joke '\n",
      " 'string:Without LCEL\\u200bfrom typing import Listimport openaiprompt_template '\n",
      " '= \"Tell me a short joke about {topic}\"client = openai.OpenAI()def '\n",
      " 'call_chat_model(messages: List[dict]) -> str:    response = '\n",
      " 'client.chat.completions.create(        model=\"gpt-3.5-turbo\",         '\n",
      " 'messages=messages,    )    return response.choices[0].message.contentdef '\n",
      " 'invoke_chain(topic: str) -> str:    prompt_value = '\n",
      " 'prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", '\n",
      " '\"content\": prompt_value}]    return '\n",
      " 'call_chat_model(messages)invoke_chain(\"ice cream\")LCEL\\u200bfrom '\n",
      " 'langchain_openai import ChatOpenAIfrom langchain_core.prompts import '\n",
      " 'ChatPromptTemplatefrom langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.runnables import '\n",
      " 'RunnablePassthroughprompt = ChatPromptTemplate.from_template(    \"Tell me a '\n",
      " 'short joke about {topic}\")output_parser = StrOutputParser()model = '\n",
      " 'ChatOpenAI(model=\"gpt-3.5-turbo\")chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt    | model    | '\n",
      " 'output_parser)chain.invoke(\"ice cream\")Stream\\u200bIf we want to stream '\n",
      " \"results instead, we'll need to change our function:Without LCEL\\u200bfrom \"\n",
      " 'typing import Iteratordef stream_chat_model(messages: List[dict]) -> '\n",
      " 'Iterator[str]:    stream = client.chat.completions.create(        '\n",
      " 'model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    '\n",
      " 'for response in stream:        content = '\n",
      " 'response.choices[0].delta.content        if content is not None:            '\n",
      " 'yield contentdef stream_chain(topic: str) -> Iterator[str]:    prompt_value '\n",
      " '= prompt.format(topic=topic)    return stream_chat_model([{\"role\": \"user\", '\n",
      " '\"content\": prompt_value}])for chunk in stream_chain(\"ice cream\"):    '\n",
      " 'print(chunk, end=\"\", flush=True)LCEL\\u200bfor chunk in chain.stream(\"ice '\n",
      " 'cream\"):    print(chunk, end=\"\", flush=True)Batch\\u200bIf we want to run on '\n",
      " \"a batch of inputs in parallel, we'll again need a new function:Without \"\n",
      " 'LCEL\\u200bfrom concurrent.futures import ThreadPoolExecutordef '\n",
      " 'batch_chain(topics: list) -> list:    with ThreadPoolExecutor(max_workers=5) '\n",
      " 'as executor:        return list(executor.map(invoke_chain, '\n",
      " 'topics))batch_chain([\"ice cream\", \"spaghetti\", '\n",
      " '\"dumplings\"])LCEL\\u200bchain.batch([\"ice cream\", \"spaghetti\", '\n",
      " '\"dumplings\"])## AsyncIf we need an asynchronous version:Without '\n",
      " 'LCEL\\u200basync_client = openai.AsyncOpenAI()async def '\n",
      " 'acall_chat_model(messages: List[dict]) -> str:    response = await '\n",
      " 'async_client.chat.completions.create(        model=\"gpt-3.5-turbo\",         '\n",
      " 'messages=messages,    )    return response.choices[0].message.contentasync '\n",
      " 'def ainvoke_chain(topic: str) -> str:    prompt_value = '\n",
      " 'prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", '\n",
      " '\"content\": prompt_value}]    return await acall_chat_model(messages)await '\n",
      " 'ainvoke_chain(\"ice cream\")LCEL\\u200bawait chain.ainvoke(\"ice cream\")Async '\n",
      " 'Batch\\u200bWithout LCEL\\u200bimport asyncioimport openaiasync def '\n",
      " 'abatch_chain(topics: list) -> list:    coros = map(ainvoke_chain, topics)    '\n",
      " 'return await asyncio.gather(*coros)await abatch_chain([\"ice cream\", '\n",
      " '\"spaghetti\", \"dumplings\"])LCEL\\u200bawait chain.abatch([\"ice cream\", '\n",
      " '\"spaghetti\", \"dumplings\"])LLM instead of chat model\\u200bIf we want to use a '\n",
      " 'completion endpoint instead of a chat endpoint: Without LCEL\\u200bdef '\n",
      " 'call_llm(prompt_value: str) -> str:    response = '\n",
      " 'client.completions.create(        model=\"gpt-3.5-turbo-instruct\",        '\n",
      " 'prompt=prompt_value,    )    return response.choices[0].textdef '\n",
      " 'invoke_llm_chain(topic: str) -> str:    prompt_value = '\n",
      " 'prompt_template.format(topic=topic)    return '\n",
      " 'call_llm(prompt_value)invoke_llm_chain(\"ice cream\")LCEL\\u200bfrom '\n",
      " 'langchain_openai import OpenAIllm = '\n",
      " 'OpenAI(model=\"gpt-3.5-turbo-instruct\")llm_chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt    | llm    | '\n",
      " 'output_parser)llm_chain.invoke(\"ice cream\")Different model provider\\u200bIf '\n",
      " 'we want to use Anthropic instead of OpenAI: Without LCEL\\u200bimport '\n",
      " 'anthropicanthropic_template = '\n",
      " 'f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"anthropic_client = '\n",
      " 'anthropic.Anthropic()def call_anthropic(prompt_value: str) -> str:    '\n",
      " 'response = anthropic_client.completions.create(        '\n",
      " 'model=\"claude-2\",        prompt=prompt_value,        '\n",
      " 'max_tokens_to_sample=256,    )    return response.completion    def '\n",
      " 'invoke_anthropic_chain(topic: str) -> str:    prompt_value = '\n",
      " 'anthropic_template.format(topic=topic)    return '\n",
      " 'call_anthropic(prompt_value)invoke_anthropic_chain(\"ice '\n",
      " 'cream\")LCEL\\u200bfrom langchain_anthropic import ChatAnthropicanthropic =',\n",
      " 'ChatAnthropic(model=\"claude-2\")anthropic_chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt     | anthropic    | '\n",
      " 'output_parser)anthropic_chain.invoke(\"ice cream\")Runtime '\n",
      " 'configurability\\u200bIf we wanted to make the choice of chat model or LLM '\n",
      " 'configurable at runtime:Without LCEL\\u200bdef invoke_configurable_chain(    '\n",
      " 'topic: str,     *,     model: str = \"chat_openai\") -> str:    if model == '\n",
      " '\"chat_openai\":        return invoke_chain(topic)    elif model == '\n",
      " '\"openai\":        return invoke_llm_chain(topic)    elif model == '\n",
      " '\"anthropic\":        return invoke_anthropic_chain(topic)    else:        '\n",
      " 'raise ValueError(            f\"Received invalid model '\n",
      " '\\'{model}\\'.\"            \" Expected one of chat_openai, openai, '\n",
      " 'anthropic\"        )def stream_configurable_chain(    topic: str,     *,     '\n",
      " 'model: str = \"chat_openai\") -> Iterator[str]:    if model == '\n",
      " '\"chat_openai\":        return stream_chain(topic)    elif model == '\n",
      " '\"openai\":        # Note we haven\\'t implemented this yet.        return '\n",
      " 'stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we '\n",
      " \"haven't implemented this yet        return stream_anthropic_chain(topic)    \"\n",
      " 'else:        raise ValueError(            f\"Received invalid model '\n",
      " '\\'{model}\\'.\"            \" Expected one of chat_openai, openai, '\n",
      " 'anthropic\"        )def batch_configurable_chain(    topics: List[str],     '\n",
      " '*,     model: str = \"chat_openai\") -> List[str]:    # You get the idea    '\n",
      " '...async def abatch_configurable_chain(    topics: List[str],     *,     '\n",
      " 'model: str = \"chat_openai\") -> List[str]:    '\n",
      " '...invoke_configurable_chain(\"ice cream\", model=\"openai\")stream = '\n",
      " 'stream_configurable_chain(    \"ice_cream\",     model=\"anthropic\")for chunk '\n",
      " 'in stream:    print(chunk, end=\"\", flush=True)# '\n",
      " 'batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])# await '\n",
      " 'ainvoke_configurable_chain(\"ice cream\")With LCEL\\u200bfrom '\n",
      " 'langchain_core.runnables import ConfigurableFieldconfigurable_model = '\n",
      " 'model.configurable_alternatives(    ConfigurableField(id=\"model\"),     '\n",
      " 'default_key=\"chat_openai\",     openai=llm,    '\n",
      " 'anthropic=anthropic,)configurable_chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt     | configurable_model     | '\n",
      " 'output_parser)configurable_chain.invoke(    \"ice cream\",     '\n",
      " 'config={\"model\": \"openai\"})stream = configurable_chain.stream(    \"ice '\n",
      " 'cream\",     config={\"model\": \"anthropic\"})for chunk in stream:    '\n",
      " 'print(chunk, end=\"\", flush=True)configurable_chain.batch([\"ice cream\", '\n",
      " '\"spaghetti\", \"dumplings\"])# await configurable_chain.ainvoke(\"ice '\n",
      " 'cream\")Logging\\u200bIf we want to log our intermediate results:Without '\n",
      " \"LCEL\\u200bWe'll print intermediate steps for illustrative purposesdef \"\n",
      " 'invoke_anthropic_chain_with_logging(topic: str) -> str:    print(f\"Input: '\n",
      " '{topic}\")    prompt_value = anthropic_template.format(topic=topic)    '\n",
      " 'print(f\"Formatted prompt: {prompt_value}\")    output = '\n",
      " 'call_anthropic(prompt_value)    print(f\"Output: {output}\")    return '\n",
      " 'outputinvoke_anthropic_chain_with_logging(\"ice cream\")LCEL\\u200bEvery '\n",
      " 'component has built-in integrations with LangSmith. If we set the following '\n",
      " 'two environment variables, all chain traces are logged to LangSmith.import '\n",
      " 'osos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] '\n",
      " '= \"true\"anthropic_chain.invoke(\"ice cream\")Here\\'s what our LangSmith trace '\n",
      " 'looks like: '\n",
      " 'https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/rFallbacks\\u200bIf '\n",
      " 'we wanted to add fallback logic, in case one model API is down:Without '\n",
      " 'LCEL\\u200bdef invoke_chain_with_fallback(topic: str) -> str:    try:        '\n",
      " 'return invoke_chain(topic)    except Exception:        return '\n",
      " 'invoke_anthropic_chain(topic)async def ainvoke_chain_with_fallback(topic: '\n",
      " 'str) -> str:    try:        return await ainvoke_chain(topic)    except '\n",
      " \"Exception:        # Note: we haven't actually implemented this.        \"\n",
      " 'return await ainvoke_anthropic_chain(topic)async def '\n",
      " 'batch_chain_with_fallback(topics: List[str]) -> str:    try:        return '\n",
      " \"batch_chain(topics)    except Exception:        # Note: we haven't actually \"\n",
      " 'implemented this.        return '\n",
      " 'batch_anthropic_chain(topics)invoke_chain_with_fallback(\"ice cream\")# await '\n",
      " 'ainvoke_chain_with_fallback(\"ice cream\")batch_chain_with_fallback([\"ice '\n",
      " 'cream\", \"spaghetti\", \"dumplings\"]))LCEL\\u200bfallback_chain = '\n",
      " 'chain.with_fallbacks([anthropic_chain])fallback_chain.invoke(\"ice cream\")# '\n",
      " 'await fallback_chain.ainvoke(\"ice cream\")fallback_chain.batch([\"ice cream\", '\n",
      " '\"spaghetti\", \"dumplings\"])Full code comparison\\u200bEven in this simple '\n",
      " 'case, our LCEL chain succinctly packs in a lot of functionality. As chains '\n",
      " 'become more complex, this becomes especially valuable.Without LCEL\\u200bfrom '\n",
      " 'concurrent.futures import ThreadPoolExecutorfrom typing import Iterator, '\n",
      " 'List, Tupleimport anthropicimport openaiprompt_template = \"Tell me a short '\n",
      " 'joke about {topic}\"anthropic_template = '\n",
      " 'f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"client = '\n",
      " 'openai.OpenAI()async_client = openai.AsyncOpenAI()anthropic_client = '\n",
      " 'anthropic.Anthropic()def call_chat_model(messages: List[dict]) -> str:    '\n",
      " 'response = client.chat.completions.create(        '\n",
      " 'model=\"gpt-3.5-turbo\",         messages=messages,    )    return '\n",
      " 'response.choices[0].message.contentdef invoke_chain(topic: str) -> str:    '\n",
      " 'print(f\"Input: {topic}\")    prompt_value = '\n",
      " 'prompt_template.format(topic=topic)    print(f\"Formatted prompt: '\n",
      " '{prompt_value}\")    messages = [{\"role\": \"user\", \"content\": '\n",
      " 'prompt_value}]    output = call_chat_model(messages)    print(f\"Output: '\n",
      " '{output}\")    return outputdef stream_chat_model(messages: List[dict]) -> '\n",
      " 'Iterator[str]:    stream = client.chat.completions.create(        '\n",
      " 'model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    '\n",
      " 'for response in stream:',\n",
      " 'content = response.choices[0].delta.content        if content is not '\n",
      " 'None:            yield contentdef stream_chain(topic: str) -> '\n",
      " 'Iterator[str]:    print(f\"Input: {topic}\")    prompt_value = '\n",
      " 'prompt.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    '\n",
      " 'stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])    '\n",
      " 'for chunk in stream:        print(f\"Token: {chunk}\", end=\"\")        yield '\n",
      " 'chunkdef batch_chain(topics: list) -> list:    with '\n",
      " 'ThreadPoolExecutor(max_workers=5) as executor:        return '\n",
      " 'list(executor.map(invoke_chain, topics))def call_llm(prompt_value: str) -> '\n",
      " 'str:    response = client.completions.create(        '\n",
      " 'model=\"gpt-3.5-turbo-instruct\",        prompt=prompt_value,    )    return '\n",
      " 'response.choices[0].textdef invoke_llm_chain(topic: str) -> str:    '\n",
      " 'print(f\"Input: {topic}\")    prompt_value = '\n",
      " 'promtp_template.format(topic=topic)    print(f\"Formatted prompt: '\n",
      " '{prompt_value}\")    output = call_llm(prompt_value)    print(f\"Output: '\n",
      " '{output}\")    return outputdef call_anthropic(prompt_value: str) -> str:    '\n",
      " 'response = anthropic_client.completions.create(        '\n",
      " 'model=\"claude-2\",        prompt=prompt_value,        '\n",
      " 'max_tokens_to_sample=256,    )    return response.completion   def '\n",
      " 'invoke_anthropic_chain(topic: str) -> str:    print(f\"Input: {topic}\")    '\n",
      " 'prompt_value = anthropic_template.format(topic=topic)    print(f\"Formatted '\n",
      " 'prompt: {prompt_value}\")    output = call_anthropic(prompt_value)    '\n",
      " 'print(f\"Output: {output}\")    return outputasync def '\n",
      " 'ainvoke_anthropic_chain(topic: str) -> str:    ...def '\n",
      " 'stream_anthropic_chain(topic: str) -> Iterator[str]:    ...def '\n",
      " 'batch_anthropic_chain(topics: List[str]) -> List[str]:    ...def '\n",
      " 'invoke_configurable_chain(    topic: str,     *,     model: str = '\n",
      " '\"chat_openai\") -> str:    if model == \"chat_openai\":        return '\n",
      " 'invoke_chain(topic)    elif model == \"openai\":        return '\n",
      " 'invoke_llm_chain(topic)    elif model == \"anthropic\":        return '\n",
      " 'invoke_anthropic_chain(topic)    else:        raise ValueError(            '\n",
      " 'f\"Received invalid model \\'{model}\\'.\"            \" Expected one of '\n",
      " 'chat_openai, openai, anthropic\"        )def stream_configurable_chain(    '\n",
      " 'topic: str,     *,     model: str = \"chat_openai\") -> Iterator[str]:    if '\n",
      " 'model == \"chat_openai\":        return stream_chain(topic)    elif model == '\n",
      " '\"openai\":        # Note we haven\\'t implemented this yet.        return '\n",
      " 'stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we '\n",
      " \"haven't implemented this yet        return stream_anthropic_chain(topic)    \"\n",
      " 'else:        raise ValueError(            f\"Received invalid model '\n",
      " '\\'{model}\\'.\"            \" Expected one of chat_openai, openai, '\n",
      " 'anthropic\"        )def batch_configurable_chain(    topics: List[str],     '\n",
      " '*,     model: str = \"chat_openai\") -> List[str]:    ...async def '\n",
      " 'abatch_configurable_chain(    topics: List[str],     *,     model: str = '\n",
      " '\"chat_openai\") -> List[str]:    ...def invoke_chain_with_fallback(topic: '\n",
      " 'str) -> str:    try:        return invoke_chain(topic)    except '\n",
      " 'Exception:        return invoke_anthropic_chain(topic)async def '\n",
      " 'ainvoke_chain_with_fallback(topic: str) -> str:    try:        return await '\n",
      " 'ainvoke_chain(topic)    except Exception:        return await '\n",
      " 'ainvoke_anthropic_chain(topic)async def batch_chain_with_fallback(topics: '\n",
      " 'List[str]) -> str:    try:        return batch_chain(topics)    except '\n",
      " 'Exception:        return batch_anthropic_chain(topics)LCEL\\u200bimport '\n",
      " 'osfrom langchain_anthropic import ChatAnthropicfrom langchain_openai import '\n",
      " 'ChatOpenAIfrom langchain_openai import OpenAIfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthrough, '\n",
      " 'ConfigurableFieldos.environ[\"LANGCHAIN_API_KEY\"] = '\n",
      " '\"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(    \"Tell me a short joke about '\n",
      " '{topic}\")chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")openai = '\n",
      " 'OpenAI(model=\"gpt-3.5-turbo-instruct\")anthropic = '\n",
      " 'ChatAnthropic(model=\"claude-2\")model = (    chat_openai    '\n",
      " '.with_fallbacks([anthropic])    .configurable_alternatives(        '\n",
      " 'ConfigurableField(id=\"model\"),        default_key=\"chat_openai\",        '\n",
      " 'openai=openai,        anthropic=anthropic,    ))chain = (    {\"topic\": '\n",
      " 'RunnablePassthrough()}     | prompt     | model     | StrOutputParser())Next '\n",
      " 'steps\\u200bTo continue learning about LCEL, we recommend:Reading up on the '\n",
      " \"full LCEL Interface, which we've only partially covered here.Exploring the \"\n",
      " 'primitives to learn more about what LCEL provides.Help us out by providing '\n",
      " 'feedback on this documentation '\n",
      " 'page:PreviousPrimitivesNextStreamingInvokeStreamBatchAsync BatchLLM instead '\n",
      " 'of chat modelDifferent model providerRuntime '\n",
      " 'configurabilityLoggingFallbacksFull code comparisonNext '\n",
      " 'stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n\\n\\n\\n\\n\\n\\n\\nStreaming | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageStreamingOn this pageStreaming With '\n",
      " 'LangChainStreaming is critical in making applications based on LLMs feel '\n",
      " 'responsive to end-users.Important LangChain primitives like LLMs, parsers, '\n",
      " 'prompts, retrievers, and agents implement the LangChain Runnable '\n",
      " 'Interface.This interface provides two general approaches to stream '\n",
      " 'content:sync stream and async astream: a default implementation of streaming '\n",
      " 'that streams the final output from the chain.async astream_events and async '\n",
      " 'astream_log: these provide a way to stream both intermediate steps and final '\n",
      " \"output from the chain.Let's take a look at both approaches, and try to \"\n",
      " 'understand how to use them. ü•∑Using Stream\\u200bAll Runnable objects '\n",
      " 'implement a sync method called stream and an async variant called astream. '\n",
      " 'These methods are designed to stream the final output in chunks, yielding '\n",
      " 'each chunk as soon as it is available.Streaming is only possible if all '\n",
      " 'steps in the program know how to process an input stream; i.e., process an '\n",
      " 'input chunk one at a time, and yield a corresponding output chunk.The '\n",
      " 'complexity of this processing can vary, from straightforward tasks like '\n",
      " 'emitting tokens produced by an LLM, to more challenging ones like streaming '\n",
      " 'parts of JSON results before the entire JSON is complete.The best place to '\n",
      " 'start exploring streaming is with the single most important components in '\n",
      " 'LLMs apps-- the LLMs themselves!LLMs and Chat Models\\u200bLarge language '\n",
      " 'models and their chat variants are the primary bottleneck in LLM based apps. '\n",
      " 'üôäLarge language models can take several seconds to generate a complete '\n",
      " 'response to a query. This is far slower than the ~200-300 ms threshold at '\n",
      " 'which an application feels responsive to an end user.The key strategy to '\n",
      " 'make the application feel more responsive is to show intermediate progress; '\n",
      " 'viz., to stream the output from the model token by token.We will show '\n",
      " 'examples of streaming using the chat model from Anthropic. To use the model, '\n",
      " 'you will need to install the langchain-anthropic package. You can do this '\n",
      " 'with the following command:pip install -qU langchain-anthropic# Showing the '\n",
      " 'example using anthropic, but you can use# your favorite chat model!from '\n",
      " 'langchain_anthropic import ChatAnthropicmodel = ChatAnthropic()chunks = '\n",
      " '[]async for chunk in model.astream(\"hello. tell me something about '\n",
      " 'yourself\"):    chunks.append(chunk)    print(chunk.content, end=\"|\", '\n",
      " \"flush=True) Hello|!| My| name| is| Claude|.| I|'m| an| AI| assistant| \"\n",
      " 'created| by| An|throp|ic| to| be| helpful|,| harmless|,| and| '\n",
      " \"honest|.||Let's inspect one of the chunkschunks[0]AIMessageChunk(content=' \"\n",
      " \"Hello')We got back something called an AIMessageChunk. This chunk represents \"\n",
      " 'a part of an AIMessage.Message chunks are additive by design -- one can '\n",
      " 'simply add them up to get the state of the response so far!chunks[0] + '\n",
      " \"chunks[1] + chunks[2] + chunks[3] + chunks[4]AIMessageChunk(content=' Hello! \"\n",
      " \"My name is')Chains\\u200bVirtually all LLM applications involve more steps \"\n",
      " \"than just a call to a language model.Let's build a simple chain using \"\n",
      " 'LangChain Expression Language (LCEL) that combines a prompt, model and a '\n",
      " 'parser and verify that streaming works.We will use StrOutputParser to parse '\n",
      " 'the output from the model. This is a simple parser that extracts the content '\n",
      " 'field from an AIMessageChunk, giving us the token returned by the '\n",
      " 'model.tipLCEL is a declarative way to specify a \"program\" by chainining '\n",
      " 'together different LangChain primitives. Chains created using LCEL benefit '\n",
      " 'from an automatic implementation of stream and astream allowing streaming of '\n",
      " 'the final output. In fact, chains created with LCEL implement the entire '\n",
      " 'standard Runnable interface.from langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = '\n",
      " 'StrOutputParser()chain = prompt | model | parserasync for chunk in '\n",
      " 'chain.astream({\"topic\": \"parrot\"}):    print(chunk, end=\"|\", flush=True) '\n",
      " \"Here|'s| a| silly| joke| about| a| par|rot|:|What| kind| of| teacher| gives| \"\n",
      " 'good| advice|?| An| ap|-|parent| (|app|arent|)| one|!||You might notice '\n",
      " \"above that parser actually doesn't block the streaming output from the \"\n",
      " 'model, and instead processes each chunk individually. Many of the LCEL '\n",
      " 'primitives also support this kind of transform-style passthrough streaming, '\n",
      " 'which can be very convenient when constructing apps.Certain runnables, like '\n",
      " 'prompt templates and chat models, cannot process individual chunks and '\n",
      " 'instead aggregate all previous steps. This will interrupt the streaming '\n",
      " 'process. Custom functions can be designed to return generators, whichnoteIf '\n",
      " \"the above functionality is not relevant to what you're building, you do not \"\n",
      " 'have to use the LangChain Expression Language to use LangChain and can '\n",
      " 'instead rely on a standard imperative programming approach by',\n",
      " 'caling invoke, batch or stream on each component individually, assigning the '\n",
      " 'results to variables and then using them downstream as you see fit.If that '\n",
      " \"works for your needs, then that's fine by us üëå!Working with Input \"\n",
      " 'Streams\\u200bWhat if you wanted to stream JSON from the output as it was '\n",
      " 'being generated?If you were to rely on json.loads to parse the partial json, '\n",
      " \"the parsing would fail as the partial json wouldn't be valid json.You'd \"\n",
      " \"likely be at a complete loss of what to do and claim that it wasn't possible \"\n",
      " 'to stream JSON.Well, turns out there is a way to do it -- the parser needs '\n",
      " 'to operate on the input stream, and attempt to \"auto-complete\" the partial '\n",
      " \"json into a valid state.Let's see such a parser in action to understand what \"\n",
      " 'this means.from langchain_core.output_parsers import JsonOutputParserchain = '\n",
      " '(    model | JsonOutputParser())  # Due to a bug in older versions of '\n",
      " 'Langchain, JsonOutputParser did not stream results from some modelsasync for '\n",
      " \"text in chain.astream(    'output a list of the countries france, spain and \"\n",
      " 'japan and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " \"key `name` and `population`'):    print(text, flush=True){}{'countries': \"\n",
      " \"[]}{'countries': [{}]}{'countries': [{'name': ''}]}{'countries': [{'name': \"\n",
      " \"'France'}]}{'countries': [{'name': 'France', 'population': \"\n",
      " \"67}]}{'countries': [{'name': 'France', 'population': 6739}]}{'countries': \"\n",
      " \"[{'name': 'France', 'population': 673915}]}{'countries': [{'name': 'France', \"\n",
      " \"'population': 67391582}]}{'countries': [{'name': 'France', 'population': \"\n",
      " \"67391582}, {}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': ''}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Sp'}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Spain'}]}{'countries': [{'name': 'France', 'population': \"\n",
      " \"67391582}, {'name': 'Spain', 'population': 46}]}{'countries': [{'name': \"\n",
      " \"'France', 'population': 67391582}, {'name': 'Spain', 'population': \"\n",
      " \"4675}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': \"\n",
      " \"'Spain', 'population': 467547}]}{'countries': [{'name': 'France', \"\n",
      " \"'population': 67391582}, {'name': 'Spain', 'population': \"\n",
      " \"46754778}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Spain', 'population': 46754778}, {}]}{'countries': [{'name': \"\n",
      " \"'France', 'population': 67391582}, {'name': 'Spain', 'population': \"\n",
      " \"46754778}, {'name': ''}]}{'countries': [{'name': 'France', 'population': \"\n",
      " \"67391582}, {'name': 'Spain', 'population': 46754778}, {'name': \"\n",
      " \"'Japan'}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': \"\n",
      " \"12}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': \"\n",
      " \"'Spain', 'population': 46754778}, {'name': 'Japan', 'population': \"\n",
      " \"12647}]}{'countries': [{'name': 'France', 'population': 67391582}, {'name': \"\n",
      " \"'Spain', 'population': 46754778}, {'name': 'Japan', 'population': \"\n",
      " \"1264764}]}{'countries': [{'name': 'France', 'population': 67391582}, \"\n",
      " \"{'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': \"\n",
      " \"126476461}]}Now, let's break streaming. We'll use the previous example and \"\n",
      " 'append an extraction function at the end that extracts the country names '\n",
      " 'from the finalized JSON.dangerAny steps in the chain that operate on '\n",
      " 'finalized inputs rather than on input streams can break streaming '\n",
      " 'functionality via stream or astream.tipLater, we will discuss the '\n",
      " 'astream_events API which streams results from intermediate steps. This API '\n",
      " 'will stream results from intermediate steps even if the chain contains steps '\n",
      " 'that only operate on finalized inputs.from langchain_core.output_parsers '\n",
      " 'import (    JsonOutputParser,)# A function that operates on finalized '\n",
      " 'inputs# rather than on an input_streamdef _extract_country_names(inputs):    '\n",
      " '\"\"\"A function that does not operates on input streams and breaks '\n",
      " 'streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if '\n",
      " '\"countries\" not in inputs:        return \"\"    countries = '\n",
      " 'inputs[\"countries\"]    if not isinstance(countries, list):        return '\n",
      " '\"\"    country_names = [        country.get(\"name\") for country in countries '\n",
      " 'if isinstance(country, dict)    ]    return country_nameschain = model | '\n",
      " 'JsonOutputParser() | _extract_country_namesasync for text in '\n",
      " \"chain.astream(    'output a list of the countries france, spain and japan \"\n",
      " 'and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " 'key `name` and `population`\\'):    print(text, end=\"|\", '\n",
      " \"flush=True)['France', 'Spain', 'Japan']|Generator Functions\\u200bLe'ts fix \"\n",
      " 'the streaming using a generator function that can operate on the input '\n",
      " 'stream.tipA generator function (a function that uses yield) allows writing '\n",
      " 'code that operators on input streamsfrom langchain_core.output_parsers '\n",
      " 'import JsonOutputParserasync def '\n",
      " '_extract_country_names_streaming(input_stream):    \"\"\"A function that '\n",
      " 'operates on input streams.\"\"\"    country_names_so_far = set()    async for '\n",
      " 'input in input_stream:        if not isinstance(input, dict):            '\n",
      " 'continue        if \"countries\" not in input:            continue        '\n",
      " 'countries = input[\"countries\"]        if not isinstance(countries, '\n",
      " 'list):            continue        for country in countries:            name '\n",
      " '= country.get(\"name\")            if not name:                '\n",
      " 'continue            if name not in country_names_so_far:                '\n",
      " 'yield name                country_names_so_far.add(name)chain = model | '\n",
      " 'JsonOutputParser() | _extract_country_names_streamingasync for text in '\n",
      " \"chain.astream(    'output a list of the countries france, spain and japan \"\n",
      " 'and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " 'key `name` and `population`\\'):    print(text, end=\"|\", '\n",
      " 'flush=True)France|Sp|Spain|Japan|noteBecause the code above is relying on '\n",
      " 'JSON auto-completion, you may see partial names of countries (e.g., Sp and '\n",
      " \"Spain), which is not what one would want for an extraction result!We're \"\n",
      " 'focusing on streaming concepts, not necessarily the results of the '\n",
      " 'chains.Non-streaming components\\u200bSome built-in components like '\n",
      " 'Retrievers do not offer any streaming. What happens if we try to stream '\n",
      " 'them? ü§®from langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import',\n",
      " 'RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingstemplate = '\n",
      " '\"\"\"Answer the question based only on the following '\n",
      " 'context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\", \"harrison likes spicy food\"],    '\n",
      " 'embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()chunks = '\n",
      " '[chunk for chunk in retriever.stream(\"where did harrison '\n",
      " 'work?\")]chunks[[Document(page_content=\\'harrison worked at kensho\\'),  '\n",
      " \"Document(page_content='harrison likes spicy food')]]Stream just yielded the \"\n",
      " 'final result from that component.This is OK \\U0001f979! Not all components '\n",
      " 'have to implement streaming -- in some cases streaming is either '\n",
      " \"unnecessary, difficult or just doesn't make sense.tipAn LCEL chain \"\n",
      " 'constructed using non-streaming components, will still be able to stream in '\n",
      " 'a lot of cases, with streaming of partial output starting after the last '\n",
      " 'non-streaming step in the chain.retrieval_chain = (    {        \"context\": '\n",
      " 'retriever.with_config(run_name=\"Docs\"),        \"question\": '\n",
      " 'RunnablePassthrough(),    }    | prompt    | model    | '\n",
      " 'StrOutputParser())for chunk in retrieval_chain.stream(    \"Where did '\n",
      " 'harrison work? \" \"Write 3 made up sentences about this place.\"):    '\n",
      " 'print(chunk, end=\"|\", flush=True) Based| on| the| given| context|,| the| '\n",
      " 'only| information| provided| about| where| Harrison| worked| is| that| he| '\n",
      " 'worked| at| Ken|sh|o|.| Since| there| are| no| other| details| provided| '\n",
      " 'about| Ken|sh|o|,| I| do| not| have| enough| information| to| write| 3| '\n",
      " 'additional| made| up| sentences| about| this| place|.| I| can| only| state| '\n",
      " \"that| Harrison| worked| at| Ken|sh|o|.||Now that we've seen how stream and \"\n",
      " \"astream work, let's venture into the world of streaming events. üèûÔ∏èUsing \"\n",
      " 'Stream Events\\u200bEvent Streaming is a beta API. This API may change a bit '\n",
      " 'based on feedback.noteIntroduced in langchain-core 0.1.14.import '\n",
      " \"langchain_corelangchain_core.__version__'0.1.18'For the astream_events API \"\n",
      " 'to work properly:Use async throughout the code to the extent possible (e.g., '\n",
      " 'async tools etc)Propagate callbacks if defining custom functions / '\n",
      " 'runnablesWhenever using runnables without LCEL, make sure to call .astream() '\n",
      " 'on LLMs rather than .ainvoke to force the LLM to stream tokens.Let us know '\n",
      " \"if anything doesn't work as expected! :)Event Reference\\u200bBelow is a \"\n",
      " 'reference table that shows some events that might be emitted by the various '\n",
      " 'Runnable objects.noteWhen streaming is implemented properly, the inputs to a '\n",
      " 'runnable will not be known until after the input stream has been entirely '\n",
      " 'consumed. This means that inputs will often be included only for end events '\n",
      " 'and rather than for start '\n",
      " 'events.eventnamechunkinputoutputon_chat_model_start[model name]{\"messages\": '\n",
      " '[[SystemMessage, HumanMessage]]}on_chat_model_stream[model '\n",
      " 'name]AIMessageChunk(content=\"hello\")on_chat_model_end[model '\n",
      " 'name]{\"messages\": [[SystemMessage, HumanMessage]]}{\"generations\": [...], '\n",
      " '\"llm_output\": None, ...}on_llm_start[model name]{\\'input\\': '\n",
      " \"'hello'}on_llm_stream[model name]'Hello'on_llm_end[model name]'Hello \"\n",
      " 'human!\\'on_chain_startformat_docson_chain_streamformat_docs\"hello world!, '\n",
      " 'goodbye world!\"on_chain_endformat_docs[Document(...)]\"hello world!, goodbye '\n",
      " 'world!\"on_tool_startsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_streamsome_tool{\"x\": '\n",
      " '1, \"y\": \"2\"}on_tool_endsome_tool{\"x\": 1, \"y\": '\n",
      " '\"2\"}on_retriever_start[retriever name]{\"query\": '\n",
      " '\"hello\"}on_retriever_chunk[retriever name]{documents: '\n",
      " '[...]}on_retriever_end[retriever name]{\"query\": \"hello\"}{documents: '\n",
      " '[...]}on_prompt_start[template_name]{\"question\": '\n",
      " '\"hello\"}on_prompt_end[template_name]{\"question\": '\n",
      " '\"hello\"}ChatPromptValue(messages: [SystemMessage, ...])Chat '\n",
      " \"Model\\u200bLet's start off by looking at the events produced by a chat \"\n",
      " 'model.events = []async for event in model.astream_events(\"hello\", '\n",
      " 'version=\"v1\"):    '\n",
      " 'events.append(event)/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: '\n",
      " 'LangChainBetaWarning: This API is in beta and may change in the future.  '\n",
      " 'warn_beta(noteHey what\\'s that funny version=\"v1\" parameter in the API?! '\n",
      " \"üòæThis is a beta API, and we're almost certainly going to make some changes \"\n",
      " 'to it.This version parameter will allow us to minimize such breaking changes '\n",
      " \"to your code. In short, we are annoying you now, so we don't have to annoy \"\n",
      " \"you later.Let's take a look at the few of the start event and a few of the \"\n",
      " \"end events.events[:3][{'event': 'on_chat_model_start',  'run_id': \"\n",
      " \"'555843ed-3d24-4774-af25-fbf030d5e8c4',  'name': 'ChatAnthropic',  'tags': \"\n",
      " \"[],  'metadata': {},  'data': {'input': 'hello'}}, {'event': \"\n",
      " \"'on_chat_model_stream',  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  \"\n",
      " \"'tags': [],  'metadata': {},  'name': 'ChatAnthropic',  'data': {'chunk': \"\n",
      " \"AIMessageChunk(content=' Hello')}}, {'event': 'on_chat_model_stream',  \"\n",
      " \"'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': [],  'metadata': \"\n",
      " \"{},  'name': 'ChatAnthropic',  'data': {'chunk': \"\n",
      " \"AIMessageChunk(content='!')}}]events[-2:][{'event': 'on_chat_model_stream',  \"\n",
      " \"'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': [],  'metadata': \"\n",
      " \"{},  'name': 'ChatAnthropic',  'data': {'chunk': \"\n",
      " \"AIMessageChunk(content='')}}, {'event': 'on_chat_model_end',  'name': \"\n",
      " \"'ChatAnthropic',  'run_id': '555843ed-3d24-4774-af25-fbf030d5e8c4',  'tags': \"\n",
      " \"[],  'metadata': {},  'data': {'output': AIMessageChunk(content=' \"\n",
      " \"Hello!')}}]Chain\\u200bLet's revisit the example chain that parsed streaming \"\n",
      " 'JSON to explore the streaming events API.chain = (    model | '\n",
      " 'JsonOutputParser())  # Due to a bug in older versions of Langchain, '\n",
      " 'JsonOutputParser did not stream results from some modelsevents = [    '\n",
      " \"event    async for event in chain.astream_events(        'output a list of \"\n",
      " 'the countries france, spain and japan and their populations in JSON format. '\n",
      " 'Use a dict with an outer key of \"countries\" which contains a list of '\n",
      " \"countries. Each country should have the key `name` and `population`',        \"\n",
      " 'version=\"v1\",    )]If you examine at the first few events, you\\'ll notice '\n",
      " 'that there are 3 different start events rather than 2 start events.The three '\n",
      " 'start events correspond to:The chain (model + parser)The modelThe '\n",
      " \"parserevents[:3][{'event': 'on_chain_start',  'run_id': \"\n",
      " \"'b1074bff-2a17-458b-9e7b-625211710df4',  'name': 'RunnableSequence',  \"\n",
      " \"'tags': [],  'metadata': {},  'data':\",\n",
      " \"{'input': 'output a list of the countries france, spain and japan and their \"\n",
      " 'populations in JSON format. Use a dict with an outer key of \"countries\" '\n",
      " 'which contains a list of countries. Each country should have the key `name` '\n",
      " \"and `population`'}}, {'event': 'on_chat_model_start',  'name': \"\n",
      " \"'ChatAnthropic',  'run_id': '6072be59-1f43-4f1c-9470-3b92e8406a99',  'tags': \"\n",
      " \"['seq:step:1'],  'metadata': {},  'data': {'input': {'messages': \"\n",
      " \"[[HumanMessage(content='output a list of the countries france, spain and \"\n",
      " 'japan and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " \"key `name` and `population`')]]}}}, {'event': 'on_parser_start',  'name': \"\n",
      " \"'JsonOutputParser',  'run_id': 'bf978194-0eda-4494-ad15-3a5bfe69cd59',  \"\n",
      " \"'tags': ['seq:step:2'],  'metadata': {},  'data': {}}]What do you think \"\n",
      " \"you'd see if you looked at the last 3 events? what about the middle?Let's \"\n",
      " 'use this API to take output the stream events from the model and the parser. '\n",
      " \"We're ignoring start events, end events and events from the chain.num_events \"\n",
      " \"= 0async for event in chain.astream_events(    'output a list of the \"\n",
      " 'countries france, spain and japan and their populations in JSON format. Use '\n",
      " 'a dict with an outer key of \"countries\" which contains a list of countries. '\n",
      " \"Each country should have the key `name` and `population`',    \"\n",
      " 'version=\"v1\",):    kind = event[\"event\"]    if kind == '\n",
      " '\"on_chat_model_stream\":        print(            f\"Chat model chunk: '\n",
      " '{repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        '\n",
      " ')    if kind == \"on_parser_stream\":        print(f\"Parser chunk: '\n",
      " '{event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if '\n",
      " 'num_events > 30:        # Truncate the output        print(\"...\")        '\n",
      " \"breakChat model chunk: ' Here'Chat model chunk: ' is'Chat model chunk: ' \"\n",
      " \"the'Chat model chunk: ' JSON'Chat model chunk: ' with'Chat model chunk: ' \"\n",
      " \"the'Chat model chunk: ' requested'Chat model chunk: ' countries'Chat model \"\n",
      " \"chunk: ' and'Chat model chunk: ' their'Chat model chunk: ' populations'Chat \"\n",
      " \"model chunk: ':'Chat model chunk: '\\\\n\\\\n```'Chat model chunk: 'json'Parser \"\n",
      " \"chunk: {}Chat model chunk: '\\\\n{'Chat model chunk: '\\\\n 'Chat model chunk: ' \"\n",
      " '\"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: '\n",
      " \"{'countries': []}Chat model chunk: ' ['Chat model chunk: '\\\\n   'Parser \"\n",
      " \"chunk: {'countries': [{}]}Chat model chunk: ' {'...Because both the model \"\n",
      " 'and the parser support streaming, we see sreaming events from both '\n",
      " \"components in real time! Kind of cool isn't it? ü¶úFiltering \"\n",
      " 'Events\\u200bBecause this API produces so many events, it is useful to be '\n",
      " 'able to filter on events.You can filter by either component name, component '\n",
      " 'tags or component type.By Name\\u200bchain = model.with_config({\"run_name\": '\n",
      " '\"model\"}) | JsonOutputParser().with_config(    {\"run_name\": '\n",
      " '\"my_parser\"})max_events = 0async for event in chain.astream_events(    '\n",
      " \"'output a list of the countries france, spain and japan and their \"\n",
      " 'populations in JSON format. Use a dict with an outer key of \"countries\" '\n",
      " 'which contains a list of countries. Each country should have the key `name` '\n",
      " 'and `population`\\',    version=\"v1\",    include_names=[\"my_parser\"],):    '\n",
      " 'print(event)    max_events += 1    if max_events > 10:        # Truncate '\n",
      " 'output        print(\"...\")        break{\\'event\\': \\'on_parser_start\\', '\n",
      " \"'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', \"\n",
      " \"'tags': ['seq:step:2'], 'metadata': {}, 'data': {}}{'event': \"\n",
      " \"'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {}}}{'event': 'on_parser_stream', 'name': 'my_parser', \"\n",
      " \"'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': {'countries': []}}}{'event': \"\n",
      " \"'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{}]}}}{'event': 'on_parser_stream', \"\n",
      " \"'name': 'my_parser', 'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', \"\n",
      " \"'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': \"\n",
      " \"[{'name': ''}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', \"\n",
      " \"'run_id': 'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': {'countries': [{'name': \"\n",
      " \"'France'}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': \"\n",
      " \"67}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': \"\n",
      " \"6739}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': \"\n",
      " \"673915}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags': ['seq:step:2'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': \"\n",
      " \"67391582}]}}}{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': \"\n",
      " \"'f2ac1d1c-e14a-45fc-8990-e5c24e707299', 'tags':\",\n",
      " \"['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': \"\n",
      " \"'France', 'population': 67391582}, {}]}}}...By Type\\u200bchain = \"\n",
      " 'model.with_config({\"run_name\": \"model\"}) | '\n",
      " 'JsonOutputParser().with_config(    {\"run_name\": \"my_parser\"})max_events = '\n",
      " \"0async for event in chain.astream_events(    'output a list of the countries \"\n",
      " 'france, spain and japan and their populations in JSON format. Use a dict '\n",
      " 'with an outer key of \"countries\" which contains a list of countries. Each '\n",
      " 'country should have the key `name` and `population`\\',    version=\"v1\",    '\n",
      " 'include_types=[\"chat_model\"],):    print(event)    max_events += 1    if '\n",
      " 'max_events > 10:        # Truncate output        print(\"...\")        '\n",
      " \"break{'event': 'on_chat_model_start', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'input': {'messages': [[HumanMessage(content='output a list of \"\n",
      " 'the countries france, spain and japan and their populations in JSON format. '\n",
      " 'Use a dict with an outer key of \"countries\" which contains a list of '\n",
      " 'countries. Each country should have the key `name` and '\n",
      " \"`population`')]]}}}{'event': 'on_chat_model_stream', 'name': 'model', \"\n",
      " \"'run_id': '98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' Here')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' is')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' JSON')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' with')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' requested')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' countries')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' and')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'model', 'run_id': \"\n",
      " \"'98a6e192-8159-460c-ba73-6dfc921e3777', 'tags': ['seq:step:1'], 'metadata': \"\n",
      " \"{}, 'data': {'chunk': AIMessageChunk(content=' their')}}...By \"\n",
      " 'Tags\\u200bcautionTags are inherited by child components of a given runnable. '\n",
      " \"If you're using tags to filter, make sure that this is what you want.chain = \"\n",
      " '(model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})max_events = '\n",
      " \"0async for event in chain.astream_events(    'output a list of the countries \"\n",
      " 'france, spain and japan and their populations in JSON format. Use a dict '\n",
      " 'with an outer key of \"countries\" which contains a list of countries. Each '\n",
      " 'country should have the key `name` and `population`\\',    version=\"v1\",    '\n",
      " 'include_tags=[\"my_chain\"],):    print(event)    max_events += 1    if '\n",
      " 'max_events > 10:        # Truncate output        print(\"...\")        '\n",
      " \"break{'event': 'on_chain_start', 'run_id': \"\n",
      " \"'190875f3-3fb7-49ad-9b6e-f49da22f3e49', 'name': 'RunnableSequence', 'tags': \"\n",
      " \"['my_chain'], 'metadata': {}, 'data': {'input': 'output a list of the \"\n",
      " 'countries france, spain and japan and their populations in JSON format. Use '\n",
      " 'a dict with an outer key of \"countries\" which contains a list of countries. '\n",
      " \"Each country should have the key `name` and `population`'}}{'event': \"\n",
      " \"'on_chat_model_start', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'input': {'messages': \"\n",
      " \"[[HumanMessage(content='output a list of the countries france, spain and \"\n",
      " 'japan and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " \"key `name` and `population`')]]}}}{'event': 'on_parser_start', 'name': \"\n",
      " \"'JsonOutputParser', 'run_id': '3b5e4ca1-40fe-4a02-9a19-ba2a43a6115c', \"\n",
      " \"'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' Here')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' is')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic',\",\n",
      " \"'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', \"\n",
      " \"'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' \"\n",
      " \"the')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' JSON')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' with')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' the')}}{'event': \"\n",
      " \"'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': \"\n",
      " \"'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', 'my_chain'], \"\n",
      " \"'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' \"\n",
      " \"requested')}}{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', \"\n",
      " \"'run_id': 'ff58f732-b494-4ff9-852a-783d42f4455d', 'tags': ['seq:step:1', \"\n",
      " \"'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' \"\n",
      " \"countries')}}...Non-streaming components\\u200bRemember how some components \"\n",
      " \"don't stream well because they don't operate on input streams?While such \"\n",
      " 'components can break streaming of the final output when using astream, '\n",
      " 'astream_events will still yield streaming events from intermediate steps '\n",
      " 'that support streaming!# Function that does not support streaming.# It '\n",
      " 'operates on the finalizes inputs rather than# operating on the input '\n",
      " 'stream.def _extract_country_names(inputs):    \"\"\"A function that does not '\n",
      " 'operates on input streams and breaks streaming.\"\"\"    if not '\n",
      " 'isinstance(inputs, dict):        return \"\"    if \"countries\" not in '\n",
      " 'inputs:        return \"\"    countries = inputs[\"countries\"]    if not '\n",
      " 'isinstance(countries, list):        return \"\"    country_names = [        '\n",
      " 'country.get(\"name\") for country in countries if isinstance(country, dict)    '\n",
      " ']    return country_nameschain = (    model | JsonOutputParser() | '\n",
      " '_extract_country_names)  # This parser only works with OpenAI right nowAs '\n",
      " \"expected, the astream API doesn't work correctly because \"\n",
      " \"_extract_country_names doesn't operate on streams.async for chunk in \"\n",
      " \"chain.astream(    'output a list of the countries france, spain and japan \"\n",
      " 'and their populations in JSON format. Use a dict with an outer key of '\n",
      " '\"countries\" which contains a list of countries. Each country should have the '\n",
      " \"key `name` and `population`',):    print(chunk, flush=True)['France', \"\n",
      " \"'Spain', 'Japan']Now, let's confirm that with astream_events we're still \"\n",
      " 'seeing streaming output from the model and the parser.num_events = 0async '\n",
      " \"for event in chain.astream_events(    'output a list of the countries \"\n",
      " 'france, spain and japan and their populations in JSON format. Use a dict '\n",
      " 'with an outer key of \"countries\" which contains a list of countries. Each '\n",
      " 'country should have the key `name` and `population`\\',    version=\"v1\",):    '\n",
      " 'kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        '\n",
      " 'print(            f\"Chat model chunk: '\n",
      " '{repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        '\n",
      " ')    if kind == \"on_parser_stream\":        print(f\"Parser chunk: '\n",
      " '{event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if '\n",
      " 'num_events > 30:        # Truncate the output        print(\"...\")        '\n",
      " \"breakChat model chunk: ' Here'Chat model chunk: ' is'Chat model chunk: ' \"\n",
      " \"the'Chat model chunk: ' JSON'Chat model chunk: ' with'Chat model chunk: ' \"\n",
      " \"the'Chat model chunk: ' requested'Chat model chunk: ' countries'Chat model \"\n",
      " \"chunk: ' and'Chat model chunk: ' their'Chat model chunk: ' populations'Chat \"\n",
      " \"model chunk: ':'Chat model chunk: '\\\\n\\\\n```'Chat model chunk: 'json'Parser \"\n",
      " \"chunk: {}Chat model chunk: '\\\\n{'Chat model chunk: '\\\\n 'Chat model chunk: ' \"\n",
      " '\"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: '\n",
      " \"{'countries': []}Chat model chunk: ' ['Chat model chunk: '\\\\n   'Parser \"\n",
      " \"chunk: {'countries': [{}]}Chat model chunk: ' {'Chat model chunk: '\\\\n     \"\n",
      " '\\'Chat model chunk: \\' \"\\'...Propagating Callbacks\\u200bcautionIf you\\'re '\n",
      " 'using invoking runnables inside your tools, you need to propagate callbacks '\n",
      " 'to the runnable; otherwise, no stream events will be generated.noteWhen '\n",
      " 'using RunnableLambdas or @chain decorator, callbacks are propagated '\n",
      " 'automatically behind the scenes.from langchain_core.runnables import '\n",
      " 'RunnableLambdafrom langchain_core.tools import tooldef reverse_word(word: '\n",
      " 'str):    return word[::-1]reverse_word = '\n",
      " 'RunnableLambda(reverse_word)@tooldef bad_tool(word: str):    \"\"\"Custom tool '\n",
      " 'that doesn\\'t propagate callbacks.\"\"\"    return '\n",
      " 'reverse_word.invoke(word)async for event in bad_tool.astream_events(\"hello\", '\n",
      " 'version=\"v1\"):    print(event){\\'event\\': \\'on_tool_start\\', \\'run_id\\': '\n",
      " \"'ae7690f8-ebc9-4886-9bbe-cb336ff274f2', 'name': 'bad_tool', 'tags': [], \"\n",
      " \"'metadata': {}, 'data': {'input': 'hello'}}{'event': 'on_tool_stream', \"\n",
      " \"'run_id': 'ae7690f8-ebc9-4886-9bbe-cb336ff274f2', 'tags': [], 'metadata': \"\n",
      " \"{}, 'name': 'bad_tool', 'data': {'chunk': 'olleh'}}{'event': 'on_tool_end', \"\n",
      " \"'name': 'bad_tool', 'run_id': 'ae7690f8-ebc9-4886-9bbe-cb336ff274f2', \"\n",
      " \"'tags': [], 'metadata': {}, 'data': {'output': 'olleh'}}Here's a \"\n",
      " \"re-implementation that does propagate callbacks correctly. You'll notice \"\n",
      " \"that now we're getting events from the reverse_word runnable as \"\n",
      " 'well.@tooldef correct_tool(word: str, callbacks):    \"\"\"A tool that '\n",
      " 'correctly propagates callbacks.\"\"\"    return reverse_word.invoke(word, '\n",
      " '{\"callbacks\": callbacks})async for event in '\n",
      " 'correct_tool.astream_events(\"hello\", version=\"v1\"):    '\n",
      " \"print(event){'event': 'on_tool_start', 'run_id': \"\n",
      " \"'384f1710-612e-4022-a6d4-8a7bb0cc757e', 'name': 'correct_tool', 'tags': [], \"\n",
      " \"'metadata': {}, 'data': {'input':\",\n",
      " \"'hello'}}{'event': 'on_chain_start', 'name': 'reverse_word', 'run_id': \"\n",
      " \"'c4882303-8867-4dff-b031-7d9499b39dda', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'input': 'hello'}}{'event': 'on_chain_end', 'name': 'reverse_word', \"\n",
      " \"'run_id': 'c4882303-8867-4dff-b031-7d9499b39dda', 'tags': [], 'metadata': \"\n",
      " \"{}, 'data': {'input': 'hello', 'output': 'olleh'}}{'event': \"\n",
      " \"'on_tool_stream', 'run_id': '384f1710-612e-4022-a6d4-8a7bb0cc757e', 'tags': \"\n",
      " \"[], 'metadata': {}, 'name': 'correct_tool', 'data': {'chunk': \"\n",
      " \"'olleh'}}{'event': 'on_tool_end', 'name': 'correct_tool', 'run_id': \"\n",
      " \"'384f1710-612e-4022-a6d4-8a7bb0cc757e', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'output': 'olleh'}}If you're invoking runnables from within Runnable \"\n",
      " 'Lambdas or @chains, then callbacks will be passed automatically on your '\n",
      " 'behalf.from langchain_core.runnables import RunnableLambdaasync def '\n",
      " 'reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * '\n",
      " '2reverse_and_double = RunnableLambda(reverse_and_double)await '\n",
      " 'reverse_and_double.ainvoke(\"1234\")async for event in '\n",
      " 'reverse_and_double.astream_events(\"1234\", version=\"v1\"):    '\n",
      " \"print(event){'event': 'on_chain_start', 'run_id': \"\n",
      " \"'4fe56c7b-6982-4999-a42d-79ba56151176', 'name': 'reverse_and_double', \"\n",
      " \"'tags': [], 'metadata': {}, 'data': {'input': '1234'}}{'event': \"\n",
      " \"'on_chain_start', 'name': 'reverse_word', 'run_id': \"\n",
      " \"'335fe781-8944-4464-8d2e-81f61d1f85f5', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'input': '1234'}}{'event': 'on_chain_end', 'name': 'reverse_word', \"\n",
      " \"'run_id': '335fe781-8944-4464-8d2e-81f61d1f85f5', 'tags': [], 'metadata': \"\n",
      " \"{}, 'data': {'input': '1234', 'output': '4321'}}{'event': 'on_chain_stream', \"\n",
      " \"'run_id': '4fe56c7b-6982-4999-a42d-79ba56151176', 'tags': [], 'metadata': \"\n",
      " \"{}, 'name': 'reverse_and_double', 'data': {'chunk': '43214321'}}{'event': \"\n",
      " \"'on_chain_end', 'name': 'reverse_and_double', 'run_id': \"\n",
      " \"'4fe56c7b-6982-4999-a42d-79ba56151176', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'output': '43214321'}}And with the @chain decorator:from \"\n",
      " 'langchain_core.runnables import chain@chainasync def '\n",
      " 'reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * '\n",
      " '2await reverse_and_double.ainvoke(\"1234\")async for event in '\n",
      " 'reverse_and_double.astream_events(\"1234\", version=\"v1\"):    '\n",
      " \"print(event){'event': 'on_chain_start', 'run_id': \"\n",
      " \"'7485eedb-1854-429c-a2f8-03d01452daef', 'name': 'reverse_and_double', \"\n",
      " \"'tags': [], 'metadata': {}, 'data': {'input': '1234'}}{'event': \"\n",
      " \"'on_chain_start', 'name': 'reverse_word', 'run_id': \"\n",
      " \"'e7cddab2-9b95-4e80-abaf-4b2429117835', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'input': '1234'}}{'event': 'on_chain_end', 'name': 'reverse_word', \"\n",
      " \"'run_id': 'e7cddab2-9b95-4e80-abaf-4b2429117835', 'tags': [], 'metadata': \"\n",
      " \"{}, 'data': {'input': '1234', 'output': '4321'}}{'event': 'on_chain_stream', \"\n",
      " \"'run_id': '7485eedb-1854-429c-a2f8-03d01452daef', 'tags': [], 'metadata': \"\n",
      " \"{}, 'name': 'reverse_and_double', 'data': {'chunk': '43214321'}}{'event': \"\n",
      " \"'on_chain_end', 'name': 'reverse_and_double', 'run_id': \"\n",
      " \"'7485eedb-1854-429c-a2f8-03d01452daef', 'tags': [], 'metadata': {}, 'data': \"\n",
      " \"{'output': '43214321'}}Help us out by providing feedback on this \"\n",
      " 'documentation page:PreviousAdvantages of LCELNextAdd message history '\n",
      " '(memory)Using StreamLLMs and Chat ModelsChainsWorking with Input '\n",
      " 'StreamsNon-streaming componentsUsing Stream EventsEvent ReferenceChat '\n",
      " 'ModelChainFiltering EventsNon-streaming componentsPropagating '\n",
      " 'CallbacksCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Sequences: Chaining runnables | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesSequences: Chaining '\n",
      " 'runnablesOn this pageChaining runnablesOne key advantage of the Runnable '\n",
      " 'interface is that any two runnables can be \"chained\" together into '\n",
      " \"sequences. The output of the previous runnable's .invoke() call is passed as \"\n",
      " 'input to the next runnable. This can be done using the pipe operator (|), or '\n",
      " 'the more explicit .pipe() method, which does the same thing. The resulting '\n",
      " 'RunnableSequence is itself a runnable, which means it can be invoked, '\n",
      " 'streamed, or piped just like any other runnable.The pipe operator\\u200bTo '\n",
      " \"show off how this works, let's go through an example. We'll walk through a \"\n",
      " 'common pattern in LangChain: using a prompt template to format input into a '\n",
      " 'chat model, and finally converting the chat message output into a string '\n",
      " 'with an output parser.%pip install --upgrade --quiet langchain '\n",
      " 'langchain-anthropicfrom langchain_anthropic import ChatAnthropicfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplateprompt = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")model = '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")chain = prompt | model | '\n",
      " 'StrOutputParser()Prompts and models are both runnable, and the output type '\n",
      " 'from the prompt call is the same as the input type of the chat model, so we '\n",
      " 'can chain them together. We can then invoke the resulting sequence like any '\n",
      " 'other runnable:chain.invoke({\"topic\": \"bears\"})\"Here\\'s a bear joke for '\n",
      " \"you:\\\\n\\\\nWhy don't bears wear socks? \\\\nBecause they have bear \"\n",
      " \"feet!\\\\n\\\\nHow's that? I tried to keep it light and silly. Bears can make \"\n",
      " \"for some fun puns and jokes. Let me know if you'd like to hear another \"\n",
      " 'one!\"Coercion\\u200bWe can even combine this chain with more runnables to '\n",
      " 'create another chain. This may involve some input/output formatting using '\n",
      " 'other types of runnables, depending on the required inputs and outputs of '\n",
      " \"the chain components.For example, let's say we wanted to compose the joke \"\n",
      " 'generating chain with another chain that evaluates whether or not the '\n",
      " 'generated joke was funny.We would need to be careful with how we format the '\n",
      " 'input into the next chain. In the below example, the dict in the chain is '\n",
      " 'automatically parsed and converted into a RunnableParallel, which runs all '\n",
      " 'of its values in parallel and returns a dict with the results.This happens '\n",
      " 'to be the same format the next prompt template expects. Here it is in '\n",
      " 'action:from langchain_core.output_parsers import '\n",
      " 'StrOutputParseranalysis_prompt = ChatPromptTemplate.from_template(\"is this a '\n",
      " 'funny joke? {joke}\")composed_chain = {\"joke\": chain} | analysis_prompt | '\n",
      " 'model | StrOutputParser()composed_chain.invoke({\"topic\": \"bears\"})\"That\\'s a '\n",
      " \"pretty classic and well-known bear pun joke. Whether it's considered funny \"\n",
      " 'is quite subjective, as humor is very personal. Some people may find that '\n",
      " 'type of pun-based joke amusing, while others may not find it that humorous. '\n",
      " 'Ultimately, the funniness of a joke is in the eye (or ear) of the beholder. '\n",
      " \"If you enjoyed the joke and got a chuckle out of it, then that's what \"\n",
      " 'matters most.\"Functions will also be coerced into runnables, so you can add '\n",
      " 'custom logic to your chains too. The below chain results in the same logical '\n",
      " 'flow as before:composed_chain_with_lambda = (    chain    | (lambda input: '\n",
      " '{\"joke\": input})    | analysis_prompt    | model    | '\n",
      " 'StrOutputParser())composed_chain_with_lambda.invoke({\"topic\": \"beets\"})\\'I '\n",
      " \"appreciate the effort, but I have to be honest - I didn\\\\'t find that joke \"\n",
      " 'particularly funny. Beet-themed puns can be quite hit-or-miss, and this one '\n",
      " 'falls more on the \"miss\" side for me. The premise is a bit too '\n",
      " 'straightforward and predictable. While I can see the logic behind it, the '\n",
      " \"punchline just doesn\\\\'t pack much of a comedic punch. \\\\n\\\\nThat said, I do \"\n",
      " 'admire your willingness to explore puns and wordplay around vegetables. '\n",
      " 'Cultivating a good sense of humor takes practice, and not every joke is '\n",
      " 'going to land. The important thing is to keep experimenting and finding what '\n",
      " 'works. Maybe try for a more unexpected or creative twist on beet-related '\n",
      " 'humor next time. But thanks for sharing - I always appreciate when humans '\n",
      " \"test out jokes on me, even if they don\\\\'t always make me laugh out \"\n",
      " \"loud.'However, keep in mind that using functions like this may interfere \"\n",
      " 'with operations like streaming. See this section for more information.The '\n",
      " '.pipe() method\\u200bWe could also compose the same sequence using the '\n",
      " \".pipe() method. Here's what that looks like:from langchain_core.runnables \"\n",
      " 'import RunnableParallelcomposed_chain_with_pipe = (    '\n",
      " 'RunnableParallel({\"joke\": chain})    .pipe(analysis_prompt)    '\n",
      " '.pipe(model)    '\n",
      " '.pipe(StrOutputParser()))composed_chain_with_pipe.invoke({\"topic\": '\n",
      " '\"battlestar galactica\"})\\'That\\\\\\'s a pretty good Battlestar '\n",
      " 'Galactica-themed pun! I appreciated the clever play on words with '\n",
      " '\"Centurion\" and \"center on.\" It\\\\\\'s the kind of nerdy, science '\n",
      " 'fiction-inspired humor that fans of the show would likely enjoy. The joke is '\n",
      " 'clever and demonstrates a good understanding of the Battlestar Galactica '\n",
      " \"universe. I\\\\'d be curious to hear any other Battlestar-related jokes you \"\n",
      " \"might have up your sleeve. As long as they don\\\\'t reproduce copyrighted \"\n",
      " \"material, I\\\\'m happy to provide my thoughts on the humor and appeal for \"\n",
      " \"fans of the show.'Help us out by providing feedback on this documentation \"\n",
      " 'page:PreviousPrimitivesNextParallel: Format dataThe pipe operatorCoercionThe '\n",
      " '.pipe() '\n",
      " 'methodCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' ----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Passthrough: Pass through inputs | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesPassthrough: Pass through '\n",
      " 'inputsOn this pagePassing data throughRunnablePassthrough on its own allows '\n",
      " 'you to pass inputs unchanged. This typically is used in conjuction with '\n",
      " 'RunnableParallel to pass data through to a new key in the map. See the '\n",
      " 'example below:%pip install --upgrade --quiet  langchain langchain-openaifrom '\n",
      " 'langchain_core.runnables import RunnableParallel, '\n",
      " 'RunnablePassthroughrunnable = RunnableParallel(    '\n",
      " 'passed=RunnablePassthrough(),    modified=lambda x: x[\"num\"] + '\n",
      " '1,)runnable.invoke({\"num\": 1}){\\'passed\\': {\\'num\\': 1}, \\'extra\\': '\n",
      " \"{'num': 1, 'mult': 3}, 'modified': 2}As seen above, passed key was called \"\n",
      " \"with RunnablePassthrough() and so it simply passed on {'num': 1}. We also \"\n",
      " 'set a second key in the map with modified. This uses a lambda to set a '\n",
      " 'single value adding 1 to the num, which resulted in modified key with the '\n",
      " 'value of 2.Retrieval Example\\u200bIn the example below, we see a use case '\n",
      " 'where we use RunnablePassthrough along with RunnableParallel. from '\n",
      " 'langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)model = '\n",
      " 'ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": '\n",
      " 'RunnablePassthrough()}    | prompt    | model    | '\n",
      " 'StrOutputParser())retrieval_chain.invoke(\"where did harrison '\n",
      " 'work?\")\\'Harrison worked at Kensho.\\'Here the input to prompt is expected to '\n",
      " 'be a map with keys \"context\" and \"question\". The user input is just the '\n",
      " 'question. So we need to get the context using our retriever and passthrough '\n",
      " 'the user input under the \"question\" key. In this case, the '\n",
      " \"RunnablePassthrough allows us to pass on the user's question to the prompt \"\n",
      " 'and model.Help us out by providing feedback on this documentation '\n",
      " 'page:PreviousLambda: Run custom functionsNextAssign: Add values to '\n",
      " 'stateRetrieval '\n",
      " 'ExampleCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' ----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Parallel: Format data | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesParallel: Format dataOn this '\n",
      " 'pageFormatting inputs & outputThe RunnableParallel primitive is essentially '\n",
      " 'a dict whose values are runnables (or things that can be coerced to '\n",
      " 'runnables, like functions). It runs all of its values in parallel, and each '\n",
      " 'value is called with the overall input of the RunnableParallel. The final '\n",
      " 'return value is a dict with the results of each value under its appropriate '\n",
      " 'key.It is useful for parallelizing operations, but can also be useful for '\n",
      " 'manipulating the output of one Runnable to match the input format of the '\n",
      " 'next Runnable in a sequence.Here the input to prompt is expected to be a map '\n",
      " 'with keys \"context\" and \"question\". The user input is just the question. So '\n",
      " 'we need to get the context using our retriever and passthrough the user '\n",
      " 'input under the \"question\" key.%pip install --upgrade --quiet  langchain '\n",
      " 'langchain-openaifrom langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)model = '\n",
      " 'ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": '\n",
      " 'RunnablePassthrough()}    | prompt    | model    | '\n",
      " 'StrOutputParser())retrieval_chain.invoke(\"where did harrison '\n",
      " 'work?\")\\'Harrison worked at Kensho.\\'::: {.callout-tip}\\n'\n",
      " \"Note that when composing a RunnableParallel with another Runnable we don't \"\n",
      " 'even need to wrap our dictionary in the RunnableParallel class ‚Äî\\xa0the type '\n",
      " 'conversion is handled for us. In the context of a chain, these are '\n",
      " 'equivalent:\\n'\n",
      " ':::{\"context\": retriever, \"question\": '\n",
      " 'RunnablePassthrough()}RunnableParallel({\"context\": retriever, \"question\": '\n",
      " 'RunnablePassthrough()})RunnableParallel(context=retriever, '\n",
      " 'question=RunnablePassthrough())Using itemgetter as shorthand\\u200bNote that '\n",
      " \"you can use Python's itemgetter as shorthand to extract data from the map \"\n",
      " 'when combining with RunnableParallel. You can find more information about '\n",
      " 'itemgetter in the Python Documentation. In the example below, we use '\n",
      " 'itemgetter to extract specific keys from the map:from operator import '\n",
      " 'itemgetterfrom langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}Answer in the following '\n",
      " 'language: {language}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)chain = (    {        \"context\": '\n",
      " 'itemgetter(\"question\") | retriever,        \"question\": '\n",
      " 'itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | '\n",
      " 'prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where '\n",
      " 'did harrison work\", \"language\": \"italian\"})\\'Harrison ha lavorato a '\n",
      " \"Kensho.'Parallelize steps\\u200bRunnableParallel (aka. RunnableMap) makes it \"\n",
      " 'easy to execute multiple Runnables in parallel, and to return the output of '\n",
      " 'these Runnables as a map.from langchain_core.prompts import '\n",
      " 'ChatPromptTemplatefrom langchain_core.runnables import RunnableParallelfrom '\n",
      " 'langchain_openai import ChatOpenAImodel = ChatOpenAI()joke_chain = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | '\n",
      " 'modelpoem_chain = (    ChatPromptTemplate.from_template(\"write a 2-line poem '\n",
      " 'about {topic}\") | model)map_chain = RunnableParallel(joke=joke_chain, '\n",
      " 'poem=poem_chain)map_chain.invoke({\"topic\": \"bear\"}){\\'joke\\': '\n",
      " 'AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear '\n",
      " 'feet!\"), \\'poem\\': AIMessage(content=\"In the wild\\'s embrace, bear roams '\n",
      " 'free,\\\\nStrength and grace, a majestic '\n",
      " 'decree.\")}Parallelism\\u200bRunnableParallel are also useful for running '\n",
      " 'independent processes in parallel, since each Runnable in the map is '\n",
      " 'executed in parallel. For example, we can see our earlier joke_chain, '\n",
      " 'poem_chain and map_chain all have about the same runtime, even though '\n",
      " 'map_chain executes both of the other two.%%timeitjoke_chain.invoke({\"topic\": '\n",
      " '\"bear\"})958 ms ¬± 402 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop '\n",
      " 'each)%%timeitpoem_chain.invoke({\"topic\": \"bear\"})1.22 s ¬± 508 ms per loop '\n",
      " '(mean ¬± std. dev. of 7 runs, 1 loop each)%%timeitmap_chain.invoke({\"topic\": '\n",
      " '\"bear\"})1.15 s ¬± 119 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop '\n",
      " 'each)Help us out by providing feedback on this documentation '\n",
      " 'page:PreviousSequences: Chaining runnablesNextBinding: Attach runtime '\n",
      " 'argsUsing itemgetter as shorthandParallelize '\n",
      " 'stepsParallelismCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' ----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Lambda: Run custom functions | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesLambda: Run custom functionsOn '\n",
      " 'this pageRun custom functionsYou can use arbitrary functions in the '\n",
      " 'pipeline.Note that all inputs to these functions need to be a SINGLE '\n",
      " 'argument. If you have a function that accepts multiple arguments, you should '\n",
      " 'write a wrapper that accepts a single input and unpacks it into multiple '\n",
      " 'argument.',\n",
      " '%pip install --upgrade --quiet langchain langchain-openaifrom operator '\n",
      " 'import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnableLambdafrom langchain_openai import '\n",
      " 'ChatOpenAIdef length_function(text):    return len(text)def '\n",
      " '_multiple_length_function(text1, text2):    return len(text1) * '\n",
      " 'len(text2)def multiple_length_function(_dict):    return '\n",
      " '_multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])prompt = '\n",
      " 'ChatPromptTemplate.from_template(\"what is {a} + {b}\")model = '\n",
      " 'ChatOpenAI()chain1 = prompt | modelchain = (    {        \"a\": '\n",
      " 'itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": '\n",
      " 'itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | '\n",
      " 'RunnableLambda(multiple_length_function),    }    | prompt    | '\n",
      " 'model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})AIMessage(content=\\'3 + 9 = '\n",
      " \"12', response_metadata={'token_usage': {'completion_tokens': 7, \"\n",
      " \"'prompt_tokens': 14, 'total_tokens': 21}, 'model_name': 'gpt-3.5-turbo', \"\n",
      " \"'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': \"\n",
      " \"None}, id='run-bd204541-81fd-429a-ad92-dd1913af9b1c-0')Accepting a Runnable \"\n",
      " 'Config\\u200bRunnable lambdas can optionally accept a RunnableConfig, which '\n",
      " 'they can use to pass callbacks, tags, and other configuration information to '\n",
      " 'nested runs.from langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.runnables import RunnableConfigimport jsondef '\n",
      " 'parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        '\n",
      " 'ChatPromptTemplate.from_template(            \"Fix the following '\n",
      " 'text:\\\\n\\\\n```text\\\\n{input}\\\\n```\\\\nError: {error}\"            \" Don\\'t '\n",
      " 'narrate, just respond with the fixed data.\"        )        | '\n",
      " 'ChatOpenAI()        | StrOutputParser()    )    for _ in range(3):        '\n",
      " 'try:            return json.loads(text)        except Exception as '\n",
      " 'e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, '\n",
      " 'config)    return \"Failed to parse\"from langchain_community.callbacks import '\n",
      " 'get_openai_callbackwith get_openai_callback() as cb:    output = '\n",
      " 'RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": '\n",
      " '[\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb){\\'foo\\': '\n",
      " \"'bar'}Tokens Used: 62    Prompt Tokens: 56    Completion Tokens: 6Successful \"\n",
      " 'Requests: 1Total Cost (USD): $9.6e-05StreamingYou can use generator '\n",
      " 'functions (ie. functions that use the yield keyword, and behave like '\n",
      " 'iterators) in a LCEL pipeline.The signature of these generators should be '\n",
      " 'Iterator[Input] -> Iterator[Output]. Or for async generators: '\n",
      " 'AsyncIterator[Input] -> AsyncIterator[Output].These are useful '\n",
      " 'for:implementing a custom output parsermodifying the output of a previous '\n",
      " \"step, while preserving streaming capabilitiesHere's an example of a custom \"\n",
      " 'output parser for comma-separated lists:from typing import Iterator, '\n",
      " 'Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated '\n",
      " 'list of 5 animals similar to: {animal}. Do not include numbers\")model = '\n",
      " 'ChatOpenAI(temperature=0.0)str_chain = prompt | model | StrOutputParser()for '\n",
      " 'chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", '\n",
      " 'flush=True)lion, tiger, wolf, gorilla, pandastr_chain.invoke({\"animal\": '\n",
      " '\"bear\"})\\'lion, tiger, wolf, gorilla, panda\\'# This is a custom parser that '\n",
      " 'splits an iterator of llm tokens# into a list of strings separated by '\n",
      " 'commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # '\n",
      " 'hold partial input until we get a comma    buffer = \"\"    for chunk in '\n",
      " 'input:        # add current chunk to buffer        buffer += chunk        # '\n",
      " 'while there are commas in the buffer        while \",\" in buffer:            '\n",
      " '# split buffer on comma            comma_index = '\n",
      " 'buffer.index(\",\")            # yield everything before the comma            '\n",
      " 'yield [buffer[:comma_index].strip()]            # save the rest for the next '\n",
      " 'iteration            buffer = buffer[comma_index + 1 :]    # yield the last '\n",
      " 'chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor '\n",
      " 'chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, '\n",
      " 'flush=True)[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']list_chain.invoke({\"animal\": '\n",
      " '\"bear\"})[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'elephant\\']Async '\n",
      " 'version\\u200bfrom typing import AsyncIteratorasync def asplit_into_list(    '\n",
      " 'input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    '\n",
      " 'buffer = \"\"    async for (        chunk    ) in input:  # `input` is a '\n",
      " '`async_generator` object, so use `async for`        buffer += chunk        '\n",
      " 'while \",\" in buffer:            comma_index = buffer.index(\",\")            '\n",
      " 'yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index '\n",
      " '+ 1 :]    yield [buffer.strip()]list_chain = str_chain | '\n",
      " 'asplit_into_listasync for chunk in list_chain.astream({\"animal\": '\n",
      " '\"bear\"}):    print(chunk, '\n",
      " \"flush=True)['lion']['tiger']['wolf']['gorilla']['panda']await \"\n",
      " 'list_chain.ainvoke({\"animal\": \"bear\"})[\\'lion\\', \\'tiger\\', \\'wolf\\', '\n",
      " \"'gorilla', 'panda']Help us out by providing feedback on this documentation \"\n",
      " 'page:PreviousBinding: Attach runtime argsNextPassthrough: Pass through '\n",
      " 'inputsAccepting a Runnable ConfigAsync '\n",
      " 'versionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n\\n\\n\\n\\n\\n\\n\\nConfigure runtime chain internals | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesConfigure runtime chain '\n",
      " 'internalsOn this pageConfigure chain internals at runtimeOftentimes you may '\n",
      " 'want to experiment with, or even expose to the end user, multiple different '\n",
      " 'ways of doing things.\\n'\n",
      " 'In order to make this experience as easy as possible, we have defined two '\n",
      " 'methods.First, a configurable_fields method.\\n'\n",
      " 'This lets you configure particular fields of a runnable.Second, a '\n",
      " 'configurable_alternatives method.\\n'\n",
      " 'With this method, you can list out alternatives for any particular runnable '\n",
      " 'that can be set during runtime.Configuration Fields\\u200bWith LLMs\\u200bWith '\n",
      " 'LLMs we can configure things like temperature%pip install --upgrade --quiet  '\n",
      " 'langchain langchain-openaifrom langchain_core.prompts import '\n",
      " 'PromptTemplatefrom langchain_core.runnables import ConfigurableFieldfrom '\n",
      " 'langchain_openai import ChatOpenAImodel = '\n",
      " 'ChatOpenAI(temperature=0).configurable_fields(    '\n",
      " 'temperature=ConfigurableField(        id=\"llm_temperature\",        name=\"LLM '\n",
      " 'Temperature\",        description=\"The temperature of the LLM\",    '\n",
      " '))model.invoke(\"pick a random '\n",
      " 'number\")AIMessage(content=\\'7\\')model.with_config(configurable={\"llm_temperature\": '\n",
      " '0.9}).invoke(\"pick a random number\")AIMessage(content=\\'34\\')We can also do '\n",
      " 'this when its used as part of a chainprompt = '\n",
      " 'PromptTemplate.from_template(\"Pick a random number above {x}\")chain = prompt '\n",
      " '| modelchain.invoke({\"x\": '\n",
      " '0})AIMessage(content=\\'57\\')chain.with_config(configurable={\"llm_temperature\": '\n",
      " '0.9}).invoke({\"x\": 0})AIMessage(content=\\'6\\')With HubRunnables\\u200bThis is '\n",
      " 'useful to allow for switching of promptsfrom langchain.runnables.hub import '\n",
      " 'HubRunnableprompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(    '\n",
      " 'owner_repo_commit=ConfigurableField(        id=\"hub_commit\",        '\n",
      " 'name=\"Hub Commit\",        description=\"The Hub commit to pull from\",    '\n",
      " '))prompt.invoke({\"question\": \"foo\", \"context\": '\n",
      " '\"bar\"})ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant '\n",
      " 'for question-answering tasks. Use the following pieces of retrieved context '\n",
      " \"to answer the question. If you don't know the answer, just say that you \"\n",
      " \"don't know. Use three sentences maximum and keep the answer \"\n",
      " 'concise.\\\\nQuestion: foo \\\\nContext: bar '\n",
      " '\\\\nAnswer:\")])prompt.with_config(configurable={\"hub_commit\": '\n",
      " '\"rlm/rag-prompt-llama\"}).invoke(    {\"question\": \"foo\", \"context\": '\n",
      " '\"bar\"})ChatPromptValue(messages=[HumanMessage(content=\"[INST]<<SYS>> You are '\n",
      " 'an assistant for question-answering tasks. Use the following pieces of '\n",
      " \"retrieved context to answer the question. If you don't know the answer, just \"\n",
      " \"say that you don't know. Use three sentences maximum and keep the answer \"\n",
      " 'concise.<</SYS>> \\\\nQuestion: foo \\\\nContext: bar \\\\nAnswer: '\n",
      " '[/INST]\")])Configurable Alternatives\\u200bWith LLMs\\u200bLet\\'s take a look '\n",
      " 'at doing this with LLMsfrom langchain_community.chat_models import '\n",
      " 'ChatAnthropicfrom langchain_core.prompts import PromptTemplatefrom '\n",
      " 'langchain_core.runnables import ConfigurableFieldfrom langchain_openai '\n",
      " 'import ChatOpenAIllm = '\n",
      " 'ChatAnthropic(temperature=0).configurable_alternatives(    # This gives this '\n",
      " 'field an id    # When configuring the end runnable, we can then use this id '\n",
      " 'to configure this field    ConfigurableField(id=\"llm\"),    # This sets a '\n",
      " 'default_key.    # If we specify this key, the default LLM (ChatAnthropic '\n",
      " 'initialized above) will be used    default_key=\"anthropic\",    # This adds a '\n",
      " 'new option, with name `openai` that is equal to `ChatOpenAI()`    '\n",
      " 'openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is '\n",
      " 'equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # '\n",
      " 'You can add more configuration options here)prompt = '\n",
      " 'PromptTemplate.from_template(\"Tell me a joke about {topic}\")chain = prompt | '\n",
      " 'llm# By default it will call Anthropicchain.invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do '\n",
      " 'you call a bear with no teeth?\\\\nA gummy bear!\")# We can use '\n",
      " '`.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to '\n",
      " 'usechain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")# If we '\n",
      " 'use the `default_key` then it uses the '\n",
      " 'defaultchain.with_config(configurable={\"llm\": \"anthropic\"}).invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do '\n",
      " 'you call a bear with no teeth?\\\\nA gummy bear!\")With Prompts\\u200bWe can do '\n",
      " 'a similar thing, but alternate between promptsllm = '\n",
      " 'ChatAnthropic(temperature=0)prompt = PromptTemplate.from_template(    \"Tell '\n",
      " 'me a joke about {topic}\").configurable_alternatives(    # This gives this '\n",
      " 'field an id    # When configuring the end runnable, we can then use this id '\n",
      " 'to configure this field    ConfigurableField(id=\"prompt\"),    # This sets a '\n",
      " 'default_key.    # If we specify this key, the default LLM (ChatAnthropic '\n",
      " 'initialized above) will be used    default_key=\"joke\",    # This adds a new '\n",
      " 'option, with name `poem`    poem=PromptTemplate.from_template(\"Write a short '\n",
      " 'poem about {topic}\"),    # You can add more configuration options here)chain '\n",
      " '= prompt | llm# By default it will write a jokechain.invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do '\n",
      " 'you call a bear with no teeth?\\\\nA gummy bear!\")# We can configure it write '\n",
      " 'a poemchain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\\' Here is a short poem about bears:\\\\n\\\\nThe '\n",
      " 'bears awaken from their sleep\\\\nAnd lumber out into the deep\\\\nForests '\n",
      " 'filled with trees so tall\\\\nForaging for food before nightfall \\\\nTheir '\n",
      " 'furry coats and claws so sharp\\\\nSniffing for berries and fish to '\n",
      " 'nab\\\\nLumbering about without a care\\\\nThe mighty grizzly and black '\n",
      " 'bear\\\\nProud creatures, wild and free\\\\nRuling their domain '\n",
      " 'majestically\\\\nWandering the woods they call their own\\\\nBefore returning to '\n",
      " \"their dens alone')With Prompts and LLMs\\u200bWe can also have multiple \"\n",
      " 'things configurable!',\n",
      " \"Here's an example doing that with both prompts and LLMs.llm = \"\n",
      " 'ChatAnthropic(temperature=0).configurable_alternatives(    # This gives this '\n",
      " 'field an id    # When configuring the end runnable, we can then use this id '\n",
      " 'to configure this field    ConfigurableField(id=\"llm\"),    # This sets a '\n",
      " 'default_key.    # If we specify this key, the default LLM (ChatAnthropic '\n",
      " 'initialized above) will be used    default_key=\"anthropic\",    # This adds a '\n",
      " 'new option, with name `openai` that is equal to `ChatOpenAI()`    '\n",
      " 'openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is '\n",
      " 'equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # '\n",
      " 'You can add more configuration options here)prompt = '\n",
      " 'PromptTemplate.from_template(    \"Tell me a joke about '\n",
      " '{topic}\").configurable_alternatives(    # This gives this field an id    # '\n",
      " 'When configuring the end runnable, we can then use this id to configure this '\n",
      " 'field    ConfigurableField(id=\"prompt\"),    # This sets a default_key.    # '\n",
      " 'If we specify this key, the default LLM (ChatAnthropic initialized above) '\n",
      " 'will be used    default_key=\"joke\",    # This adds a new option, with name '\n",
      " '`poem`    poem=PromptTemplate.from_template(\"Write a short poem about '\n",
      " '{topic}\"),    # You can add more configuration options here)chain = prompt | '\n",
      " 'llm# We can configure it write a poem with '\n",
      " 'OpenAIchain.with_config(configurable={\"prompt\": \"poem\", \"llm\": '\n",
      " '\"openai\"}).invoke(    {\"topic\": \"bears\"})AIMessage(content=\"In the forest, '\n",
      " 'where tall trees sway,\\\\nA creature roams, both fierce and gray.\\\\nWith '\n",
      " 'mighty paws and piercing eyes,\\\\nThe bear, a symbol of strength, '\n",
      " 'defies.\\\\n\\\\nThrough snow-kissed mountains, it does roam,\\\\nA guardian of '\n",
      " 'its woodland home.\\\\nWith fur so thick, a shield of might,\\\\nIt braves the '\n",
      " 'coldest winter night.\\\\n\\\\nA gentle giant, yet wild and free,\\\\nThe bear '\n",
      " 'commands respect, you see.\\\\nWith every step, it leaves a trace,\\\\nOf '\n",
      " \"untamed power and ancient grace.\\\\n\\\\nFrom honeyed feast to salmon's \"\n",
      " \"leap,\\\\nIt takes its place, in nature's keep.\\\\nA symbol of untamed \"\n",
      " 'delight,\\\\nThe bear, a wonder, day and night.\\\\n\\\\nSo let us honor this '\n",
      " 'noble beast,\\\\nIn forests where its soul finds peace.\\\\nFor in its presence, '\n",
      " 'we come to know,\\\\nThe untamed spirit that in us also flows.\")# We can '\n",
      " 'always just configure only one if we '\n",
      " 'wantchain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")Saving '\n",
      " 'configurations\\u200bWe can also easily save configured chains as their own '\n",
      " 'objectsopenai_joke = chain.with_config(configurable={\"llm\": '\n",
      " '\"openai\"})openai_joke.invoke({\"topic\": \"bears\"})AIMessage(content=\"Why '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")Help us out by '\n",
      " 'providing feedback on this documentation page:PreviousAssign: Add values to '\n",
      " 'stateNextPrimitivesConfiguration FieldsWith LLMsWith '\n",
      " 'HubRunnablesConfigurable AlternativesWith LLMsWith PromptsWith Prompts and '\n",
      " 'LLMsSaving '\n",
      " 'configurationsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Binding: Attach runtime args | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesBinding: Attach runtime argsOn '\n",
      " 'this pageBinding: Attach runtime argsSometimes we want to invoke a Runnable '\n",
      " 'within a Runnable sequence with constant arguments that are not part of the '\n",
      " 'output of the preceding Runnable in the sequence, and which are not part of '\n",
      " 'the user input. We can use Runnable.bind() to pass these arguments '\n",
      " 'in.Suppose we have a simple prompt + model sequence:%pip install --upgrade '\n",
      " '--quiet  langchain langchain-openaifrom langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAIprompt = ChatPromptTemplate.from_messages(    [        '\n",
      " '(            \"system\",            \"Write out the following equation using '\n",
      " 'algebraic symbols then solve it. Use the '\n",
      " 'format\\\\n\\\\nEQUATION:...\\\\nSOLUTION:...\\\\n\\\\n\",        ),        (\"human\", '\n",
      " '\"{equation_statement}\"),    ])model = ChatOpenAI(temperature=0)runnable = '\n",
      " '(    {\"equation_statement\": RunnablePassthrough()} | prompt | model | '\n",
      " 'StrOutputParser())print(runnable.invoke(\"x raised to the third plus seven '\n",
      " 'equals 12\"))EQUATION: x^3 + 7 = 12SOLUTION:Subtracting 7 from both sides of '\n",
      " 'the equation, we get:x^3 = 12 - 7x^3 = 5Taking the cube root of both sides, '\n",
      " 'we get:x = ‚àõ5Therefore, the solution to the equation x^3 + 7 = 12 is x = '\n",
      " '‚àõ5.and want to call the model with certain stop words:runnable = (    '\n",
      " '{\"equation_statement\": RunnablePassthrough()}    | prompt    | '\n",
      " 'model.bind(stop=\"SOLUTION\")    | StrOutputParser())print(runnable.invoke(\"x '\n",
      " 'raised to the third plus seven equals 12\"))EQUATION: x^3 + 7 = 12Attaching '\n",
      " 'OpenAI functions\\u200bOne particularly useful application of binding is to '\n",
      " 'attach OpenAI functions to a compatible OpenAI model:function = {    \"name\": '\n",
      " '\"solver\",    \"description\": \"Formulates and solves an equation\",    '\n",
      " '\"parameters\": {        \"type\": \"object\",        \"properties\": {            '\n",
      " '\"equation\": {                \"type\": \"string\",                \"description\": '\n",
      " '\"The algebraic expression of the equation\",            },            '\n",
      " '\"solution\": {                \"type\": \"string\",                \"description\": '\n",
      " '\"The solution to the equation\",            },        },        \"required\": '\n",
      " '[\"equation\", \"solution\"],    },}# Need gpt-4 to solve this one '\n",
      " 'correctlyprompt = ChatPromptTemplate.from_messages(    [        (            '\n",
      " '\"system\",            \"Write out the following equation using algebraic '\n",
      " 'symbols then solve it.\",        ),        (\"human\", '\n",
      " '\"{equation_statement}\"),    ])model = ChatOpenAI(model=\"gpt-4\", '\n",
      " 'temperature=0).bind(    function_call={\"name\": \"solver\"}, '\n",
      " 'functions=[function])runnable = {\"equation_statement\": '\n",
      " 'RunnablePassthrough()} | prompt | modelrunnable.invoke(\"x raised to the '\n",
      " 'third plus seven equals 12\")AIMessage(content=\\'\\', '\n",
      " \"additional_kwargs={'function_call': {'name': 'solver', 'arguments': \"\n",
      " '\\'{\\\\n\"equation\": \"x^3 + 7 = 12\",\\\\n\"solution\": \"x = ‚àõ5\"\\\\n}\\'}}, '\n",
      " 'example=False)Attaching OpenAI tools\\u200btools = [    {        \"type\": '\n",
      " '\"function\",        \"function\": {            \"name\": '\n",
      " '\"get_current_weather\",            \"description\": \"Get the current weather in '\n",
      " 'a given location\",            \"parameters\": {                \"type\": '\n",
      " '\"object\",                \"properties\": {                    \"location\": '\n",
      " '{                        \"type\": \"string\",                        '\n",
      " '\"description\": \"The city and state, e.g. San Francisco, '\n",
      " 'CA\",                    },                    \"unit\": {\"type\": \"string\", '\n",
      " '\"enum\": [\"celsius\", \"fahrenheit\"]},                },                '\n",
      " '\"required\": [\"location\"],            },        },    }]model = '\n",
      " 'ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools)model.invoke(\"What\\'s '\n",
      " 'the weather in SF, NYC and LA?\")AIMessage(content=\\'\\', '\n",
      " \"additional_kwargs={'tool_calls': [{'id': 'call_zHN0ZHwrxM7nZDdqTp6dkPko', \"\n",
      " '\\'function\\': {\\'arguments\\': \\'{\"location\": \"San Francisco, CA\", \"unit\": '\n",
      " '\"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, '\n",
      " \"{'id': 'call_aqdMm9HBSlFW9c9rqxTa7eQv', 'function': {'arguments': \"\n",
      " '\\'{\"location\": \"New York, NY\", \"unit\": \"celsius\"}\\', \\'name\\': '\n",
      " \"'get_current_weather'}, 'type': 'function'}, {'id': \"\n",
      " \"'call_cx8E567zcLzYV2WSWVgO63f1', 'function': {'arguments': \"\n",
      " '\\'{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}\\', \\'name\\': '\n",
      " \"'get_current_weather'}, 'type': 'function'}]})Help us out by providing \"\n",
      " 'feedback on this documentation page:PreviousParallel: Format dataNextLambda: '\n",
      " 'Run custom functionsAttaching OpenAI functionsAttaching OpenAI '\n",
      " 'toolsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' ----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Assign: Add values to state | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesAssign: Add values to stateOn '\n",
      " 'this pageAdding values to chain stateThe RunnablePassthrough.assign(...) '\n",
      " 'static method takes an input value and adds the extra arguments passed to '\n",
      " 'the assign function.This is useful when additively creating a dictionary to '\n",
      " \"use as input to a later step, which is a common LCEL pattern.Here's an \"\n",
      " 'example:%pip install --upgrade --quiet langchain '\n",
      " 'langchain-openai\\x1b[33mWARNING: You are using pip version 22.0.4; however, '\n",
      " 'version 24.0 is available.You should consider upgrading via the '\n",
      " \"'/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade \"\n",
      " \"pip' command.\\x1b[0m\\x1b[33m\\x1b[0mNote: you may need to restart the kernel \"\n",
      " 'to use updated packages.from langchain_core.runnables import '\n",
      " 'RunnableParallel, RunnablePassthroughrunnable = RunnableParallel(    '\n",
      " 'extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),    '\n",
      " 'modified=lambda x: x[\"num\"] + 1,)runnable.invoke({\"num\": 1}){\\'extra\\': '\n",
      " \"{'num': 1, 'mult': 3}, 'modified': 2}Let's break down what's happening \"\n",
      " 'here.The input to the chain is {\"num\": 1}. This is passed into a '\n",
      " 'RunnableParallel, which invokes the runnables it is passed in parallel with '\n",
      " 'that input.The value under the extra key is invoked. '\n",
      " 'RunnablePassthrough.assign() keeps the original keys in the input dict '\n",
      " '({\"num\": 1}), and assigns a new key called mult. The value is lambda x: '\n",
      " 'x[\"num\"] * 3), which is 3. Thus, the result is {\"num\": 1, \"mult\": 3}.{\"num\": '\n",
      " '1, \"mult\": 3} is returned to the RunnableParallel call, and is set as the '\n",
      " 'value to the key extra.At the same time, the modified key is called. The '\n",
      " 'result is 2, since the lambda extracts a key called \"num\" from its input and '\n",
      " \"adds one.Thus, the result is {'extra': {'num': 1, 'mult': 3}, 'modified': \"\n",
      " '2}.Streaming\\u200bOne nice feature of this method is that it allows values '\n",
      " \"to pass through as soon as they are available. To show this off, we'll use \"\n",
      " 'RunnablePassthrough.assign() to immediately return source docs in a '\n",
      " 'retrieval chain:from langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)model = '\n",
      " 'ChatOpenAI()generation_chain = prompt | model | '\n",
      " 'StrOutputParser()retrieval_chain = {    \"context\": retriever,    \"question\": '\n",
      " 'RunnablePassthrough(),} | '\n",
      " 'RunnablePassthrough.assign(output=generation_chain)stream = '\n",
      " 'retrieval_chain.stream(\"where did harrison work?\")for chunk in stream:    '\n",
      " \"print(chunk){'question': 'where did harrison work?'}{'context': \"\n",
      " \"[Document(page_content='harrison worked at kensho')]}{'output': \"\n",
      " \"''}{'output': 'H'}{'output': 'arrison'}{'output': ' worked'}{'output': ' \"\n",
      " \"at'}{'output': ' Kens'}{'output': 'ho'}{'output': '.'}{'output': ''}We can \"\n",
      " 'see that the first chunk contains the original \"question\" since that is '\n",
      " 'immediately available. The second chunk contains \"context\" since the '\n",
      " 'retriever finishes second. Finally, the output from the generation_chain '\n",
      " 'streams in chunks as soon as it is available.Help us out by providing '\n",
      " 'feedback on this documentation page:PreviousPassthrough: Pass through '\n",
      " 'inputsNextConfigure runtime chain '\n",
      " 'internalsStreamingCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' ----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Primitives | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesSequences: Chaining runnablesParallel: Format '\n",
      " 'dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: '\n",
      " 'Pass through inputsAssign: Add values to stateConfigure runtime chain '\n",
      " 'internalsPrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguagePrimitivesPrimitivesIn addition to '\n",
      " 'various components that are usable with LCEL, LangChain also includes '\n",
      " 'various primitives\\n'\n",
      " 'that help pass around and format data, bind arguments, invoke custom logic, '\n",
      " 'and more.This section goes into greater depth on where and how some of these '\n",
      " 'components are useful.üìÑÔ∏è Sequences: Chaining runnablesOne key advantage of '\n",
      " 'the Runnable interface is that any two runnables can be \"chained\" together '\n",
      " \"into sequences. The output of the previous runnable's .invoke() call is \"\n",
      " 'passed as input to the next runnable. This can be done using the pipe '\n",
      " 'operator (|), or the more explicit .pipe() method, which does the same '\n",
      " 'thing. The resulting RunnableSequence is itself a runnable, which means it '\n",
      " 'can be invoked, streamed, or piped just like any other runnable.üìÑÔ∏è Parallel: '\n",
      " 'Format dataThe RunnableParallel primitive is essentially a dict whose values '\n",
      " 'are runnables (or things that can be coerced to runnables, like functions). '\n",
      " 'It runs all of its values in parallel, and each value is called with the '\n",
      " 'overall input of the RunnableParallel. The final return value is a dict with '\n",
      " 'the results of each value under its appropriate key.üìÑÔ∏è Binding: Attach '\n",
      " 'runtime argsSometimes we want to invoke a Runnable within a Runnable '\n",
      " 'sequence with constant arguments that are not part of the output of the '\n",
      " 'preceding Runnable in the sequence, and which are not part of the user '\n",
      " 'input. We can use Runnable.bind() to pass these arguments in.üìÑÔ∏è Lambda: Run '\n",
      " 'custom functionsYou can use arbitrary functions in the pipeline.üìÑÔ∏è '\n",
      " 'Passthrough: Pass through inputsRunnablePassthrough on its own allows you to '\n",
      " 'pass inputs unchanged. This typically is used in conjuction with '\n",
      " 'RunnableParallel to pass data through to a new key in the map.üìÑÔ∏è Assign: Add '\n",
      " 'values to stateThe RunnablePassthrough.assign(...) static method takes an '\n",
      " 'input value and adds the extra arguments passed to the assign function.üìÑÔ∏è '\n",
      " 'Configure runtime chain internalsOftentimes you may want to experiment with, '\n",
      " 'or even expose to the end user, multiple different ways of doing things.Help '\n",
      " 'us out by providing feedback on this documentation page:PreviousRunnable '\n",
      " 'interfaceNextSequences: Chaining '\n",
      " 'runnablesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' -----',\n",
      " 'Runnable interface | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageRunnable interfaceOn this pageRunnable '\n",
      " \"interfaceTo make it as easy as possible to create custom chains, we've \"\n",
      " 'implemented a \"Runnable\" protocol. Many LangChain components implement the '\n",
      " 'Runnable protocol, including chat models, LLMs, output parsers, retrievers, '\n",
      " 'prompt templates, and more. There are also several useful primitives for '\n",
      " 'working with runnables, which you can read about in this section.This is a '\n",
      " 'standard interface, which makes it easy to define custom chains as well as '\n",
      " 'invoke them in a standard way.\\n'\n",
      " 'The standard interface includes:stream: stream back chunks of the '\n",
      " 'responseinvoke: call the chain on an inputbatch: call the chain on a list of '\n",
      " 'inputsThese also have corresponding async methods that should be used with '\n",
      " 'asyncio await syntax for concurrency:astream: stream back chunks of the '\n",
      " 'response asyncainvoke: call the chain on an input asyncabatch: call the '\n",
      " 'chain on a list of inputs asyncastream_log: stream back intermediate steps '\n",
      " 'as they happen, in addition to the final responseastream_events: beta stream '\n",
      " 'events as they happen in the chain (introduced in langchain-core 0.1.14)The '\n",
      " 'input type and output type varies by component:ComponentInput TypeOutput '\n",
      " 'TypePromptDictionaryPromptValueChatModelSingle string, list of chat messages '\n",
      " 'or a PromptValueChatMessageLLMSingle string, list of chat messages or a '\n",
      " 'PromptValueStringOutputParserThe output of an LLM or ChatModelDepends on the '\n",
      " 'parserRetrieverSingle stringList of DocumentsToolSingle string or '\n",
      " 'dictionary, depending on the toolDepends on the toolAll runnables expose '\n",
      " 'input and output schemas to inspect the inputs and outputs:input_schema: an '\n",
      " 'input Pydantic model auto-generated from the structure of the '\n",
      " 'Runnableoutput_schema: an output Pydantic model auto-generated from the '\n",
      " \"structure of the RunnableLet's take a look at these methods. To do so, we'll \"\n",
      " 'create a super simple PromptTemplate + ChatModel chain.%pip install '\n",
      " '--upgrade --quiet  langchain-core langchain-community langchain-openaifrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import '\n",
      " 'ChatOpenAImodel = ChatOpenAI()prompt = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = '\n",
      " 'prompt | modelInput Schema\\u200bA description of the inputs accepted by a '\n",
      " 'Runnable.\\n'\n",
      " 'This is a Pydantic model dynamically generated from the structure of any '\n",
      " 'Runnable.',\n",
      " 'You can call .schema() on it to obtain a JSONSchema representation.# The '\n",
      " 'input schema of the chain is the input schema of its first part, the '\n",
      " \"prompt.chain.input_schema.schema(){'title': 'PromptInput', 'type': 'object', \"\n",
      " \"'properties': {'topic': {'title': 'Topic', 'type': \"\n",
      " \"'string'}}}prompt.input_schema.schema(){'title': 'PromptInput', 'type': \"\n",
      " \"'object', 'properties': {'topic': {'title': 'Topic', 'type': \"\n",
      " \"'string'}}}model.input_schema.schema(){'title': 'ChatOpenAIInput', 'anyOf': \"\n",
      " \"[{'type': 'string'},  {'$ref': '#/definitions/StringPromptValue'},  {'$ref': \"\n",
      " \"'#/definitions/ChatPromptValueConcrete'},  {'type': 'array',   'items': \"\n",
      " \"{'anyOf': [{'$ref': '#/definitions/AIMessage'},     {'$ref': \"\n",
      " \"'#/definitions/HumanMessage'},     {'$ref': \"\n",
      " \"'#/definitions/ChatMessage'},     {'$ref': \"\n",
      " \"'#/definitions/SystemMessage'},     {'$ref': \"\n",
      " \"'#/definitions/FunctionMessage'},     {'$ref': \"\n",
      " \"'#/definitions/ToolMessage'}]}}], 'definitions': {'StringPromptValue': \"\n",
      " \"{'title': 'StringPromptValue',   'description': 'String prompt value.',   \"\n",
      " \"'type': 'object',   'properties': {'text': {'title': 'Text', 'type': \"\n",
      " \"'string'},    'type': {'title': 'Type',     'default': \"\n",
      " \"'StringPromptValue',     'enum': ['StringPromptValue'],     'type': \"\n",
      " \"'string'}},   'required': ['text']},  'AIMessage': {'title': 'AIMessage',   \"\n",
      " \"'description': 'A Message from an AI.',   'type': 'object',   'properties': \"\n",
      " \"{'content': {'title': 'Content',     'anyOf': [{'type': 'string'},      \"\n",
      " \"{'type': 'array',       'items': {'anyOf': [{'type': 'string'}, {'type': \"\n",
      " \"'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', \"\n",
      " \"'type': 'object'},    'type': {'title': 'Type',     'default': 'ai',     \"\n",
      " \"'enum': ['ai'],     'type': 'string'},    'example': {'title': 'Example', \"\n",
      " \"'default': False, 'type': 'boolean'}},   'required': ['content']},  \"\n",
      " \"'HumanMessage': {'title': 'HumanMessage',   'description': 'A Message from a \"\n",
      " \"human.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'human',     'enum': ['human'],     \"\n",
      " \"'type': 'string'},    'example': {'title': 'Example', 'default': False, \"\n",
      " \"'type': 'boolean'}},   'required': ['content']},  'ChatMessage': {'title': \"\n",
      " \"'ChatMessage',   'description': 'A Message that can be assigned an arbitrary \"\n",
      " \"speaker (i.e. role).',   'type': 'object',   'properties': {'content': \"\n",
      " \"{'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': \"\n",
      " \"'array',       'items': {'anyOf': [{'type': 'string'}, {'type': \"\n",
      " \"'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', \"\n",
      " \"'type': 'object'},    'type': {'title': 'Type',     'default': 'chat',     \"\n",
      " \"'enum': ['chat'],     'type': 'string'},    'role': {'title': 'Role', \"\n",
      " \"'type': 'string'}},   'required': ['content', 'role']},  'SystemMessage': \"\n",
      " \"{'title': 'SystemMessage',   'description': 'A Message for priming AI \"\n",
      " 'behavior, usually passed in as the first of a sequence\\\\nof input '\n",
      " \"messages.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'system',     'enum': \"\n",
      " \"['system'],     'type': 'string'}},   'required': ['content']},  \"\n",
      " \"'FunctionMessage': {'title': 'FunctionMessage',   'description': 'A Message \"\n",
      " \"for passing the result of executing a function back to a model.',   'type': \"\n",
      " \"'object',   'properties': {'content': {'title': 'Content',     'anyOf': \"\n",
      " \"[{'type': 'string'},      {'type': 'array',       'items': {'anyOf': \"\n",
      " \"[{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': \"\n",
      " \"{'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': \"\n",
      " \"'Type',     'default': 'function',     'enum': ['function'],     'type': \"\n",
      " \"'string'},    'name': {'title': 'Name', 'type': 'string'}},   'required': \"\n",
      " \"['content', 'name']},  'ToolMessage': {'title': 'ToolMessage',   \"\n",
      " \"'description': 'A Message for passing the result of executing a tool back to \"\n",
      " \"a model.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'tool',     'enum': ['tool'],     \"\n",
      " \"'type': 'string'},    'tool_call_id': {'title': 'Tool Call Id', 'type': \"\n",
      " \"'string'}},   'required': ['content', 'tool_call_id']},  \"\n",
      " \"'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',   \"\n",
      " \"'description': 'Chat prompt value which explicitly lists out the message \"\n",
      " \"types it accepts.\\\\nFor use in external schemas.',   'type': 'object',   \"\n",
      " \"'properties': {'messages': {'title': 'Messages',     'type': 'array',     \"\n",
      " \"'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},       {'$ref': \"\n",
      " \"'#/definitions/HumanMessage'},       {'$ref': \"\n",
      " \"'#/definitions/ChatMessage'},       {'$ref': \"\n",
      " \"'#/definitions/SystemMessage'},       {'$ref':\",\n",
      " \"'#/definitions/FunctionMessage'},       {'$ref': \"\n",
      " \"'#/definitions/ToolMessage'}]}},    'type': {'title': 'Type',     'default': \"\n",
      " \"'ChatPromptValueConcrete',     'enum': ['ChatPromptValueConcrete'],     \"\n",
      " \"'type': 'string'}},   'required': ['messages']}}}Output Schema\\u200bA \"\n",
      " 'description of the outputs produced by a Runnable.',\n",
      " 'This is a Pydantic model dynamically generated from the structure of any '\n",
      " 'Runnable.',\n",
      " 'You can call .schema() on it to obtain a JSONSchema representation.# The '\n",
      " 'output schema of the chain is the output schema of its last part, in this '\n",
      " 'case a ChatModel, which outputs a '\n",
      " \"ChatMessagechain.output_schema.schema(){'title': 'ChatOpenAIOutput', \"\n",
      " \"'anyOf': [{'$ref': '#/definitions/AIMessage'},  {'$ref': \"\n",
      " \"'#/definitions/HumanMessage'},  {'$ref': '#/definitions/ChatMessage'},  \"\n",
      " \"{'$ref': '#/definitions/SystemMessage'},  {'$ref': \"\n",
      " \"'#/definitions/FunctionMessage'},  {'$ref': '#/definitions/ToolMessage'}], \"\n",
      " \"'definitions': {'AIMessage': {'title': 'AIMessage',   'description': 'A \"\n",
      " \"Message from an AI.',   'type': 'object',   'properties': {'content': \"\n",
      " \"{'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': \"\n",
      " \"'array',       'items': {'anyOf': [{'type': 'string'}, {'type': \"\n",
      " \"'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', \"\n",
      " \"'type': 'object'},    'type': {'title': 'Type',     'default': 'ai',     \"\n",
      " \"'enum': ['ai'],     'type': 'string'},    'example': {'title': 'Example', \"\n",
      " \"'default': False, 'type': 'boolean'}},   'required': ['content']},  \"\n",
      " \"'HumanMessage': {'title': 'HumanMessage',   'description': 'A Message from a \"\n",
      " \"human.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'human',     'enum': ['human'],     \"\n",
      " \"'type': 'string'},    'example': {'title': 'Example', 'default': False, \"\n",
      " \"'type': 'boolean'}},   'required': ['content']},  'ChatMessage': {'title': \"\n",
      " \"'ChatMessage',   'description': 'A Message that can be assigned an arbitrary \"\n",
      " \"speaker (i.e. role).',   'type': 'object',   'properties': {'content': \"\n",
      " \"{'title': 'Content',     'anyOf': [{'type': 'string'},      {'type': \"\n",
      " \"'array',       'items': {'anyOf': [{'type': 'string'}, {'type': \"\n",
      " \"'object'}]}}]},    'additional_kwargs': {'title': 'Additional Kwargs', \"\n",
      " \"'type': 'object'},    'type': {'title': 'Type',     'default': 'chat',     \"\n",
      " \"'enum': ['chat'],     'type': 'string'},    'role': {'title': 'Role', \"\n",
      " \"'type': 'string'}},   'required': ['content', 'role']},  'SystemMessage': \"\n",
      " \"{'title': 'SystemMessage',   'description': 'A Message for priming AI \"\n",
      " 'behavior, usually passed in as the first of a sequence\\\\nof input '\n",
      " \"messages.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'system',     'enum': \"\n",
      " \"['system'],     'type': 'string'}},   'required': ['content']},  \"\n",
      " \"'FunctionMessage': {'title': 'FunctionMessage',   'description': 'A Message \"\n",
      " \"for passing the result of executing a function back to a model.',   'type': \"\n",
      " \"'object',   'properties': {'content': {'title': 'Content',     'anyOf': \"\n",
      " \"[{'type': 'string'},      {'type': 'array',       'items': {'anyOf': \"\n",
      " \"[{'type': 'string'}, {'type': 'object'}]}}]},    'additional_kwargs': \"\n",
      " \"{'title': 'Additional Kwargs', 'type': 'object'},    'type': {'title': \"\n",
      " \"'Type',     'default': 'function',     'enum': ['function'],     'type': \"\n",
      " \"'string'},    'name': {'title': 'Name', 'type': 'string'}},   'required': \"\n",
      " \"['content', 'name']},  'ToolMessage': {'title': 'ToolMessage',   \"\n",
      " \"'description': 'A Message for passing the result of executing a tool back to \"\n",
      " \"a model.',   'type': 'object',   'properties': {'content': {'title': \"\n",
      " \"'Content',     'anyOf': [{'type': 'string'},      {'type': 'array',       \"\n",
      " \"'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},    \"\n",
      " \"'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},    \"\n",
      " \"'type': {'title': 'Type',     'default': 'tool',     'enum': ['tool'],     \"\n",
      " \"'type': 'string'},    'tool_call_id': {'title': 'Tool Call Id', 'type': \"\n",
      " \"'string'}},   'required': ['content', 'tool_call_id']}}}Stream\\u200bfor s in \"\n",
      " 'chain.stream({\"topic\": \"bears\"}):    print(s.content, end=\"\", '\n",
      " \"flush=True)Sure, here's a bear-themed joke for you:Why don't bears wear \"\n",
      " 'shoes?Because they already have bear feet!Invoke\\u200bchain.invoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\"Why don\\'t bears wear shoes? \\\\n\\\\nBecause they '\n",
      " 'have bear feet!\")Batch\\u200bchain.batch([{\"topic\": \"bears\"}, {\"topic\": '\n",
      " '\"cats\"}])[AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\"), '\n",
      " 'AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many '\n",
      " 'cheetahs!\")]You can set the number of concurrent requests by using the '\n",
      " 'max_concurrency parameterchain.batch([{\"topic\": \"bears\"}, {\"topic\": '\n",
      " '\"cats\"}], config={\"max_concurrency\": 5})[AIMessage(content=\"Why don\\'t bears '\n",
      " 'wear shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why '\n",
      " 'don\\'t cats play poker in the wild? Too many cheetahs!\")]Async '\n",
      " 'Stream\\u200basync for s in chain.astream({\"topic\": \"bears\"}):    '\n",
      " 'print(s.content, end=\"\", flush=True)Why don\\'t bears wear shoes?Because they '\n",
      " 'have bear feet!Async Invoke\\u200bawait chain.ainvoke({\"topic\": '\n",
      " '\"bears\"})AIMessage(content=\"Why don\\'t bears ever wear shoes?\\\\n\\\\nBecause '\n",
      " 'they already have bear feet!\")Async Batch\\u200bawait chain.abatch([{\"topic\": '\n",
      " '\"bears\"}])[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they '\n",
      " 'have bear feet!\")]Async Stream Events (beta)\\u200bEvent Streaming is a beta '\n",
      " 'API, and may change a bit based on feedback.Note: Introduced in '\n",
      " 'langchain-core 0.2.0For now, when using the astream_events API, for '\n",
      " 'everything to work properly please:Use async throughout the code (including '\n",
      " 'async tools',\n",
      " 'etc)Propagate callbacks if defining custom functions / runnables. Whenever '\n",
      " 'using runnables without LCEL, make sure to call .astream() on LLMs rather '\n",
      " 'than .ainvoke to force the LLM to stream tokens.Event Reference\\u200bHere is '\n",
      " 'a reference table that shows some events that might be emitted by the '\n",
      " 'various Runnable objects.',\n",
      " 'Definitions for some of the Runnable are included after the table.‚ö†Ô∏è When '\n",
      " 'streaming the inputs for the runnable will not be available until the input '\n",
      " 'stream has been entirely consumed This means that the inputs will be '\n",
      " 'available at for the corresponding end hook rather than start '\n",
      " 'event.eventnamechunkinputoutputon_chat_model_start[model name]{\"messages\": '\n",
      " '[[SystemMessage, HumanMessage]]}on_chat_model_stream[model '\n",
      " 'name]AIMessageChunk(content=\"hello\")on_chat_model_end[model '\n",
      " 'name]{\"messages\": [[SystemMessage, HumanMessage]]}{\"generations\": [...], '\n",
      " '\"llm_output\": None, ...}on_llm_start[model name]{\\'input\\': '\n",
      " \"'hello'}on_llm_stream[model name]'Hello'on_llm_end[model name]'Hello \"\n",
      " 'human!\\'on_chain_startformat_docson_chain_streamformat_docs\"hello world!, '\n",
      " 'goodbye world!\"on_chain_endformat_docs[Document(...)]\"hello world!, goodbye '\n",
      " 'world!\"on_tool_startsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_streamsome_tool{\"x\": '\n",
      " '1, \"y\": \"2\"}on_tool_endsome_tool{\"x\": 1, \"y\": '\n",
      " '\"2\"}on_retriever_start[retriever name]{\"query\": '\n",
      " '\"hello\"}on_retriever_chunk[retriever name]{documents: '\n",
      " '[...]}on_retriever_end[retriever name]{\"query\": \"hello\"}{documents: '\n",
      " '[...]}on_prompt_start[template_name]{\"question\": '\n",
      " '\"hello\"}on_prompt_end[template_name]{\"question\": '\n",
      " '\"hello\"}ChatPromptValue(messages: [SystemMessage, ...])Here are declarations '\n",
      " 'associated with the events shown above:format_docs:def format_docs(docs: '\n",
      " 'List[Document]) -> str:    \\'\\'\\'Format the docs.\\'\\'\\'    return \", '\n",
      " '\".join([doc.page_content for doc in docs])format_docs = '\n",
      " 'RunnableLambda(format_docs)some_tool:@tooldef some_tool(x: int, y: str) -> '\n",
      " 'dict:    \\'\\'\\'Some_tool.\\'\\'\\'    return {\"x\": x, \"y\": y}prompt:template = '\n",
      " 'ChatPromptTemplate.from_messages(    [(\"system\", \"You are Cat Agent 007\"), '\n",
      " '(\"human\", \"{question}\")]).with_config({\"run_name\": \"my_template\", \"tags\": '\n",
      " '[\"my_template\"]})Let\\'s define a new chain to make it more interesting to '\n",
      " 'show off the astream_events interface (and later the astream_log '\n",
      " 'interface).from langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the '\n",
      " 'following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()retrieval_chain = (    {        \"context\": '\n",
      " 'retriever.with_config(run_name=\"Docs\"),        \"question\": '\n",
      " 'RunnablePassthrough(),    }    | prompt    | '\n",
      " 'model.with_config(run_name=\"my_llm\")    | StrOutputParser())Now let\\'s use '\n",
      " 'astream_events to get events from the retriever and the LLM.async for event '\n",
      " 'in retrieval_chain.astream_events(    \"where did harrison work?\", '\n",
      " 'version=\"v1\", include_names=[\"Docs\", \"my_llm\"]):    kind = event[\"event\"]    '\n",
      " 'if kind == \"on_chat_model_stream\":        '\n",
      " 'print(event[\"data\"][\"chunk\"].content, end=\"|\")    elif kind in '\n",
      " '{\"on_chat_model_start\"}:        print()        print(\"Streaming LLM:\")    '\n",
      " 'elif kind in {\"on_chat_model_end\"}:        print()        print(\"Done '\n",
      " 'streaming LLM.\")    elif kind == \"on_retriever_end\":        '\n",
      " 'print(\"--\")        print(\"Retrieved the following documents:\")        '\n",
      " 'print(event[\"data\"][\"output\"][\"documents\"])    elif kind == '\n",
      " '\"on_tool_end\":        print(f\"Ended tool: {event[\\'name\\']}\")    '\n",
      " 'else:        '\n",
      " 'pass/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: '\n",
      " 'LangChainBetaWarning: This API is in beta and may change in the future.  '\n",
      " 'warn_beta(``````output--Retrieved the following '\n",
      " \"documents:[Document(page_content='harrison worked at kensho')]Streaming \"\n",
      " 'LLM:|H|arrison| worked| at| Kens|ho|.||Done streaming LLM.Async Stream '\n",
      " 'Intermediate Steps\\u200bAll runnables also have a method .astream_log() '\n",
      " 'which is used to stream (as they happen) all or part of the intermediate '\n",
      " 'steps of your chain/sequence. This is useful to show progress to the user, '\n",
      " 'to use intermediate results, or to debug your chain.You can stream all steps '\n",
      " '(default) or include/exclude steps by name, tags or metadata.This method '\n",
      " 'yields JSONPatch ops that when applied in the same order as received build '\n",
      " 'up the RunState.class LogEntry(TypedDict):    id: str    \"\"\"ID of the '\n",
      " 'sub-run.\"\"\"    name: str    \"\"\"Name of the object being run.\"\"\"    type: '\n",
      " 'str    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"    '\n",
      " 'tags: List[str]    \"\"\"List of tags for the run.\"\"\"    metadata: Dict[str, '\n",
      " 'Any]    \"\"\"Key-value pairs of metadata for the run.\"\"\"    start_time: str    '\n",
      " '\"\"\"ISO-8601 timestamp of when the run started.\"\"\"    streamed_output_str: '\n",
      " 'List[str]    \"\"\"List of LLM tokens streamed by this run, if '\n",
      " 'applicable.\"\"\"    final_output: Optional[Any]    \"\"\"Final output of this '\n",
      " 'run.    Only available after the run has finished successfully.\"\"\"    '\n",
      " 'end_time: Optional[str]    \"\"\"ISO-8601 timestamp of when the run ended.    '\n",
      " 'Only available after the run has finished.\"\"\"class RunState(TypedDict):    '\n",
      " 'id: str    \"\"\"ID of the run.\"\"\"    streamed_output: List[Any]    \"\"\"List of '\n",
      " 'output chunks streamed by Runnable.stream()\"\"\"    final_output: '\n",
      " 'Optional[Any]    \"\"\"Final output of the run, usually the result of '\n",
      " 'aggregating (`+`) streamed_output.    Only available after the run has '\n",
      " 'finished successfully.\"\"\"    logs: Dict[str, LogEntry]    \"\"\"Map of run '\n",
      " 'names to sub-runs. If filters were supplied, this list will    contain only '\n",
      " 'the runs that matched the filters.\"\"\"Streaming JSONPatch chunks\\u200bThis is '\n",
      " 'useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops '\n",
      " 'on the client to rebuild the run state there. See LangServe for tooling to '\n",
      " 'make it easier to build a webserver from any Runnable.async for chunk in '\n",
      " 'retrieval_chain.astream_log(    \"where did harrison work?\", '\n",
      " 'include_names=[\"Docs\"]):    print(\"-\" * 40)    '\n",
      " \"print(chunk)----------------------------------------RunLogPatch({'op': \"\n",
      " \"'replace',  'path': '',  'value': {'final_output': None,            'id': \"\n",
      " \"'82e9b4b1-3dd6-4732-8db9-90e79c4da48c',            'logs': {},            \"\n",
      " \"'name': 'RunnableSequence',            'streamed_output': [],\",\n",
      " \"'type': 'chain'}})----------------------------------------RunLogPatch({'op': \"\n",
      " \"'add',  'path': '/logs/Docs',  'value': {'end_time': None,            \"\n",
      " \"'final_output': None,            'id': \"\n",
      " \"'9206e94a-57bd-48ee-8c5e-fdd1c52a6da2',            'metadata': \"\n",
      " \"{},            'name': 'Docs',            'start_time': \"\n",
      " \"'2024-01-19T22:33:55.902+00:00',            'streamed_output': \"\n",
      " \"[],            'streamed_output_str': [],            'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],            'type': \"\n",
      " \"'retriever'}})----------------------------------------RunLogPatch({'op': \"\n",
      " \"'add',  'path': '/logs/Docs/final_output',  'value': {'documents': \"\n",
      " \"[Document(page_content='harrison worked at kensho')]}}, {'op': 'add',  \"\n",
      " \"'path': '/logs/Docs/end_time',  'value': \"\n",
      " \"'2024-01-19T22:33:56.064+00:00'})----------------------------------------RunLogPatch({'op': \"\n",
      " \"'add', 'path': '/streamed_output/-', 'value': ''}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': \"\n",
      " \"''})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': 'H'}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': \"\n",
      " \"'H'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': 'arrison'}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': \"\n",
      " \"'Harrison'})----------------------------------------RunLogPatch({'op': \"\n",
      " \"'add', 'path': '/streamed_output/-', 'value': ' worked'}, {'op': 'replace', \"\n",
      " \"'path': '/final_output', 'value': 'Harrison \"\n",
      " \"worked'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': ' at'}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': 'Harrison worked \"\n",
      " \"at'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': ' Kens'}, {'op': 'replace', 'path': \"\n",
      " \"'/final_output', 'value': 'Harrison worked at \"\n",
      " \"Kens'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': 'ho'}, {'op': 'replace',  'path': \"\n",
      " \"'/final_output',  'value': 'Harrison worked at \"\n",
      " \"Kensho'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': '.'}, {'op': 'replace',  'path': \"\n",
      " \"'/final_output',  'value': 'Harrison worked at \"\n",
      " \"Kensho.'})----------------------------------------RunLogPatch({'op': 'add', \"\n",
      " \"'path': '/streamed_output/-', 'value': ''})Streaming the incremental \"\n",
      " 'RunState\\u200bYou can simply pass diff=False to get incremental values of '\n",
      " 'RunState.',\n",
      " 'You get more verbose output with more repetitive parts.async for chunk in '\n",
      " 'retrieval_chain.astream_log(    \"where did harrison work?\", '\n",
      " 'include_names=[\"Docs\"], diff=False):    print(\"-\" * 70)    '\n",
      " \"print(chunk)----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {}, 'name': \"\n",
      " \"'RunnableSequence', 'streamed_output': [], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': None,                   'final_output': None,                   \"\n",
      " \"'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [''], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'H', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H'], \"\n",
      " \"'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': \"\n",
      " \"{'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',\",\n",
      " \"'start_time': '2024-01-19T22:33:56.939+00:00',                   \"\n",
      " \"'streamed_output': [],                   'streamed_output_str': \"\n",
      " \"[],                   'tags': ['map:key:context', 'FAISS', \"\n",
      " \"'OpenAIEmbeddings'],                   'type': 'retriever'}}, 'name': \"\n",
      " \"'RunnableSequence', 'streamed_output': ['', 'H', 'arrison'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': \"\n",
      " \"{'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': \"\n",
      " \"{'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',                   \"\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked', ' at'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at Kens', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', \"\n",
      " \"'logs': {'Docs': {'end_time': \"\n",
      " \"'2024-01-19T22:33:57.120+00:00',                   'final_output': \"\n",
      " \"{'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked', ' at', ' Kens'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at Kensho', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', \"\n",
      " \"'logs': {'Docs': {'end_time': \"\n",
      " \"'2024-01-19T22:33:57.120+00:00',                   'final_output': \"\n",
      " \"{'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked', ' at', ' Kens', 'ho'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at Kensho.', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', \"\n",
      " \"'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00',\",\n",
      " \"'final_output': {'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', \"\n",
      " \"'arrison', ' worked', ' at', ' Kens', 'ho', '.'], 'type': \"\n",
      " \"'chain'})----------------------------------------------------------------------RunLog({'final_output': \"\n",
      " \"'Harrison worked at Kensho.', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', \"\n",
      " \"'logs': {'Docs': {'end_time': \"\n",
      " \"'2024-01-19T22:33:57.120+00:00',                   'final_output': \"\n",
      " \"{'documents': [Document(page_content='harrison worked at \"\n",
      " \"kensho')]},                   'id': \"\n",
      " \"'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e',                   'metadata': \"\n",
      " \"{},                   'name': 'Docs',                   'start_time': \"\n",
      " \"'2024-01-19T22:33:56.939+00:00',                   'streamed_output': \"\n",
      " \"[],                   'streamed_output_str': [],                   'tags': \"\n",
      " \"['map:key:context', 'FAISS', 'OpenAIEmbeddings'],                   'type': \"\n",
      " \"'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': \"\n",
      " \"['',                     'H',                     \"\n",
      " \"'arrison',                     ' worked',                     ' \"\n",
      " \"at',                     ' Kens',                     \"\n",
      " \"'ho',                     '.',                     ''], 'type': \"\n",
      " \"'chain'})Parallelism\\u200bLet's take a look at how LangChain Expression \"\n",
      " 'Language supports parallel requests.',\n",
      " 'For example, when using a RunnableParallel (often written as a dictionary) '\n",
      " 'it executes each element in parallel.from langchain_core.runnables import '\n",
      " 'RunnableParallelchain1 = ChatPromptTemplate.from_template(\"tell me a joke '\n",
      " 'about {topic}\") | modelchain2 = (    ChatPromptTemplate.from_template(\"write '\n",
      " 'a short (2 line) poem about {topic}\")    | model)combined = '\n",
      " 'RunnableParallel(joke=chain1, poem=chain2)%%timechain1.invoke({\"topic\": '\n",
      " '\"bears\"})CPU times: user 18 ms, sys: 1.27 ms, total: 19.3 msWall time: 692 '\n",
      " 'msAIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already '\n",
      " 'have bear feet!\")%%timechain2.invoke({\"topic\": \"bears\"})CPU times: user 10.5 '\n",
      " 'ms, sys: 166 ¬µs, total: 10.7 msWall time: 579 msAIMessage(content=\"In '\n",
      " 'forest\\'s embrace,\\\\nMajestic bears pace.\")%%timecombined.invoke({\"topic\": '\n",
      " '\"bears\"})CPU times: user 32 ms, sys: 2.59 ms, total: 34.6 msWall time: 816 '\n",
      " 'ms{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear-related joke for '\n",
      " 'you:\\\\n\\\\nWhy did the bear bring a ladder to the bar?\\\\n\\\\nBecause he heard '\n",
      " 'the drinks were on the house!\"), \\'poem\\': AIMessage(content=\"In wilderness '\n",
      " 'they roam,\\\\nMajestic strength, nature\\'s throne.\")}Parallelism on '\n",
      " 'batches\\u200bParallelism can be combined with other runnables.\\n'\n",
      " 'Let\\'s try to use parallelism with batches.%%timechain1.batch([{\"topic\": '\n",
      " '\"bears\"}, {\"topic\": \"cats\"}])CPU times: user 17.3 ms, sys: 4.84 ms, total: '\n",
      " '22.2 msWall time: 628 ms[AIMessage(content=\"Why don\\'t bears wear '\n",
      " 'shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why don\\'t '\n",
      " 'cats play poker in the wild?\\\\n\\\\nToo many '\n",
      " 'cheetahs!\")]%%timechain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU '\n",
      " 'times: user 15.8 ms, sys: 3.83 ms, total: 19.7 msWall time: 718 '\n",
      " \"ms[AIMessage(content='In the wild, bears roam,\\\\nMajestic guardians of \"\n",
      " \"ancient home.'), AIMessage(content='Whiskers grace, eyes gleam,\\\\nCats dance \"\n",
      " 'through the moonbeam.\\')]%%timecombined.batch([{\"topic\": \"bears\"}, {\"topic\": '\n",
      " '\"cats\"}])CPU times: user 44.8 ms, sys: 3.17 ms, total: 48 msWall time: 721 '\n",
      " 'ms[{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy '\n",
      " 'don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"),  \\'poem\\': '\n",
      " 'AIMessage(content=\"Majestic bears roam,\\\\nNature\\'s strength, beauty '\n",
      " 'shown.\")}, {\\'joke\\': AIMessage(content=\"Why don\\'t cats play poker in the '\n",
      " 'wild?\\\\n\\\\nToo many cheetahs!\"),  \\'poem\\': AIMessage(content=\"Whiskers '\n",
      " 'dance, eyes aglow,\\\\nCats embrace the night\\'s gentle flow.\")}]Help us out '\n",
      " 'by providing feedback on this documentation page:PreviousGet '\n",
      " 'startedNextPrimitivesInput SchemaOutput SchemaStreamInvokeBatchAsync '\n",
      " 'StreamAsync InvokeAsync BatchAsync Stream Events (beta)Event ReferenceAsync '\n",
      " 'Stream Intermediate StepsStreaming JSONPatch chunksStreaming the incremental '\n",
      " 'RunStateParallelismParallelism on '\n",
      " 'batchesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n\\n\\n\\n\\n\\n\\n\\nRoute logic based on input | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreRoute logic based on inputOn this pageDynamically route logic '\n",
      " 'based on inputThis notebook covers how to do routing in the LangChain '\n",
      " 'Expression Language.Routing allows you to create non-deterministic chains '\n",
      " 'where the output of a previous step defines the next step. Routing helps '\n",
      " 'provide structure and consistency around interactions with LLMs.There are '\n",
      " 'two ways to perform routing:Conditionally return runnables from a '\n",
      " \"RunnableLambda (recommended)Using a RunnableBranch.We'll illustrate both \"\n",
      " 'methods using a two step sequence where the first step classifies an input '\n",
      " 'question as being about LangChain, Anthropic, or Other, then routes to a '\n",
      " \"corresponding prompt chain.Example Setup\\u200bFirst, let's create a chain \"\n",
      " 'that will identify incoming questions as being about LangChain, Anthropic, '\n",
      " 'or Other:from langchain_anthropic import ChatAnthropicfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import PromptTemplatechain = (    '\n",
      " 'PromptTemplate.from_template(        \"\"\"Given the user question below, '\n",
      " 'classify it as either being about `LangChain`, `Anthropic`, or `Other`.Do '\n",
      " 'not respond with more than one '\n",
      " 'word.<question>{question}</question>Classification:\"\"\"    )    | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")    | '\n",
      " 'StrOutputParser())chain.invoke({\"question\": \"how do I call '\n",
      " 'Anthropic?\"})\\'Anthropic\\'Now, let\\'s create three sub '\n",
      " 'chains:langchain_chain = PromptTemplate.from_template(    \"\"\"You are an '\n",
      " 'expert in langchain. \\\\Always answer questions starting with \"As Harrison '\n",
      " 'Chase told me\". \\\\Respond to the following question:Question: '\n",
      " '{question}Answer:\"\"\") | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")anthropic_chain = '\n",
      " 'PromptTemplate.from_template(    \"\"\"You are an expert in anthropic. \\\\Always '\n",
      " 'answer questions starting with \"As Dario Amodei told me\". \\\\Respond to the '\n",
      " 'following question:Question: {question}Answer:\"\"\") | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")general_chain = '\n",
      " 'PromptTemplate.from_template(    \"\"\"Respond to the following '\n",
      " 'question:Question: {question}Answer:\"\"\") | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")Using a custom function '\n",
      " '(Recommended)\\u200bYou can also use a custom function to route between '\n",
      " 'different outputs. Here\\'s an example:def route(info):    if \"anthropic\" in '\n",
      " 'info[\"topic\"].lower():        return anthropic_chain    elif \"langchain\" in '\n",
      " 'info[\"topic\"].lower():        return langchain_chain    else:        return '\n",
      " 'general_chainfrom langchain_core.runnables import RunnableLambdafull_chain = '\n",
      " '{\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(    '\n",
      " 'route)full_chain.invoke({\"question\": \"how do I use '\n",
      " 'Anthropic?\"})AIMessage(content=\"As Dario Amodei told me, to use Anthropic, '\n",
      " \"you can start by exploring the company's website and learning about their \"\n",
      " 'mission, values, and the different services and products they offer. '\n",
      " 'Anthropic is focused on developing safe and ethical AI systems, so they have '\n",
      " 'a strong emphasis on transparency and responsible AI development. '\n",
      " \"\\\\n\\\\nDepending on your specific needs, you can look into Anthropic's AI \"\n",
      " 'research and development services, which cover areas like natural language '\n",
      " 'processing, computer vision, and reinforcement learning. They also offer '\n",
      " 'consulting and advisory services to help organizations navigate the '\n",
      " 'challenges and opportunities of AI integration.\\\\n\\\\nAdditionally, Anthropic '\n",
      " 'has released some open-source AI models and tools that you can explore and '\n",
      " 'experiment with. These can be a great way to get hands-on experience with '\n",
      " \"Anthropic's approach to AI development.\\\\n\\\\nOverall, Anthropic aims to be a \"\n",
      " \"reliable and trustworthy partner in the AI space, so I'd encourage you to \"\n",
      " 'reach out to them directly to discuss how they can best support your '\n",
      " 'specific requirements.\", response_metadata={\\'id\\': '\n",
      " '\\'msg_01CtLFgFSwvTaJomrihE87Ra\\', \\'content\\': [ContentBlock(text=\"As Dario '\n",
      " \"Amodei told me, to use Anthropic, you can start by exploring the company's \"\n",
      " 'website and learning about their mission, values, and the different services '\n",
      " 'and products they offer. Anthropic is focused on developing safe and ethical '\n",
      " 'AI systems, so they have a strong emphasis on transparency and responsible '\n",
      " 'AI development. \\\\n\\\\nDepending on your specific needs, you can look into '\n",
      " \"Anthropic's AI research and development services, which cover areas like \"\n",
      " 'natural language processing, computer vision, and reinforcement learning. '\n",
      " 'They also offer consulting and advisory services to help organizations '\n",
      " 'navigate the challenges and opportunities of AI '\n",
      " 'integration.\\\\n\\\\nAdditionally, Anthropic has released some open-source AI '\n",
      " 'models and tools that you can explore and experiment with. These can be a '\n",
      " \"great way to get hands-on experience with Anthropic's approach to AI \"\n",
      " 'development.\\\\n\\\\nOverall, Anthropic aims to be a reliable and trustworthy '\n",
      " \"partner in the AI space, so I'd encourage you to reach out to them directly \"\n",
      " 'to discuss how they can best support your specific requirements.\", '\n",
      " \"type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', \"\n",
      " \"'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', \"\n",
      " \"'usage': Usage(input_tokens=53, \"\n",
      " 'output_tokens=219)})full_chain.invoke({\"question\": \"how do I use '\n",
      " 'LangChain?\"})AIMessage(content=\"As Harrison Chase told me, using LangChain '\n",
      " 'involves a few key steps:\\\\n\\\\n1. **Set up your environment**: Install the '\n",
      " 'necessary Python packages, including the LangChain library itself, as well '\n",
      " 'as any other dependencies your application might require, such as language '\n",
      " 'models or other integrations.\\\\n\\\\n2. **Understand the core concepts**: '\n",
      " 'LangChain revolves around a few core concepts, like Agents, Chains, and '\n",
      " 'Tools. Familiarize yourself with these concepts and how they work together '\n",
      " 'to build powerful language-based applications.\\\\n\\\\n3. **Identify your use '\n",
      " 'case**: Determine what kind of task or application you want to build using '\n",
      " 'LangChain, such as a chatbot, a question-answering system, or a document '\n",
      " 'summarization tool.\\\\n\\\\n4. **Choose the appropriate components**: Based on '\n",
      " 'your use case, select the right LangChain components, such as agents, '\n",
      " 'chains, and tools, to build your application.\\\\n\\\\n5. **Integrate with '\n",
      " 'language models**: LangChain is designed to work seamlessly with various '\n",
      " \"language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your \"\n",
      " 'chosen language model to your LangChain application.\\\\n\\\\n6. **Implement '\n",
      " \"your application logic**: Use LangChain's building blocks to implement the \"\n",
      " 'specific functionality of your application, such as prompting the language '\n",
      " 'model, processing the response, and integrating with other services or data '\n",
      " 'sources.\\\\n\\\\n7. **Test and iterate**: Thoroughly test your application, '\n",
      " 'gather feedback, and iterate on your design and implementation to improve '\n",
      " 'its performance and user experience.\\\\n\\\\nAs Harrison Chase emphasized, '\n",
      " 'LangChain provides a flexible and powerful framework for building '\n",
      " 'language-based applications, making it easier to leverage the capabilities '\n",
      " 'of modern language models. By following these steps, you can get started '\n",
      " 'with LangChain and create innovative solutions tailored to your specific '\n",
      " 'needs.\", response_metadata={\\'id\\': \\'msg_01H3UXAAHG4TwxJLpxwuuVU7\\', '\n",
      " '\\'content\\': [ContentBlock(text=\"As Harrison Chase told me, using LangChain '\n",
      " 'involves a few key steps:\\\\n\\\\n1. **Set up your environment**: Install the '\n",
      " 'necessary Python packages, including the LangChain library itself, as well '\n",
      " 'as any other dependencies your application might require, such as language '\n",
      " 'models or other integrations.\\\\n\\\\n2. **Understand the core concepts**: '\n",
      " 'LangChain revolves around a few core concepts,',\n",
      " 'like Agents, Chains, and Tools. Familiarize yourself with these concepts and '\n",
      " 'how they work together to build powerful language-based '\n",
      " 'applications.\\\\n\\\\n3. **Identify your use case**: Determine what kind of '\n",
      " 'task or application you want to build using LangChain, such as a chatbot, a '\n",
      " 'question-answering system, or a document summarization tool.\\\\n\\\\n4. '\n",
      " '**Choose the appropriate components**: Based on your use case, select the '\n",
      " 'right LangChain components, such as agents, chains, and tools, to build your '\n",
      " 'application.\\\\n\\\\n5. **Integrate with language models**: LangChain is '\n",
      " \"designed to work seamlessly with various language models, such as OpenAI's \"\n",
      " \"GPT-3 or Anthropic's models. Connect your chosen language model to your \"\n",
      " 'LangChain application.\\\\n\\\\n6. **Implement your application logic**: Use '\n",
      " \"LangChain's building blocks to implement the specific functionality of your \"\n",
      " 'application, such as prompting the language model, processing the response, '\n",
      " 'and integrating with other services or data sources.\\\\n\\\\n7. **Test and '\n",
      " 'iterate**: Thoroughly test your application, gather feedback, and iterate on '\n",
      " 'your design and implementation to improve its performance and user '\n",
      " 'experience.\\\\n\\\\nAs Harrison Chase emphasized, LangChain provides a flexible '\n",
      " 'and powerful framework for building language-based applications, making it '\n",
      " 'easier to leverage the capabilities of modern language models. By following '\n",
      " 'these steps, you can get started with LangChain and create innovative '\n",
      " 'solutions tailored to your specific needs.\", type=\\'text\\')], \\'model\\': '\n",
      " \"'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', \"\n",
      " \"'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, \"\n",
      " 'output_tokens=400)})full_chain.invoke({\"question\": \"whats 2 + '\n",
      " '2\"})AIMessage(content=\\'4\\', response_metadata={\\'id\\': '\n",
      " \"'msg_01UAKP81jTZu9fyiyFYhsbHc', 'content': [ContentBlock(text='4', \"\n",
      " \"type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', \"\n",
      " \"'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', \"\n",
      " \"'usage': Usage(input_tokens=28, output_tokens=5)})Using a \"\n",
      " 'RunnableBranch\\u200bA RunnableBranch is a special type of runnable that '\n",
      " 'allows you to define a set of conditions and runnables to execute based on '\n",
      " \"the input. It does not offer anything that you can't achieve in a custom \"\n",
      " 'function as described above, so we recommend using a custom function '\n",
      " 'instead.A RunnableBranch is initialized with a list of (condition, runnable) '\n",
      " 'pairs and a default runnable. It selects which branch by passing each '\n",
      " \"condition the input it's invoked with. It selects the first condition to \"\n",
      " 'evaluate to True, and runs the corresponding runnable to that condition with '\n",
      " 'the input. If no provided conditions match, it runs the default '\n",
      " \"runnable.Here's an example of what it looks like in action:from \"\n",
      " 'langchain_core.runnables import RunnableBranchbranch = RunnableBranch(    '\n",
      " '(lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),    (lambda '\n",
      " 'x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),    '\n",
      " 'general_chain,)full_chain = {\"topic\": chain, \"question\": lambda x: '\n",
      " 'x[\"question\"]} | branchfull_chain.invoke({\"question\": \"how do I use '\n",
      " 'Anthropic?\"})AIMessage(content=\"As Dario Amodei told me, to use Anthropic, '\n",
      " 'you should first familiarize yourself with our mission and principles. '\n",
      " 'Anthropic is committed to developing safe and beneficial artificial '\n",
      " 'intelligence that can help solve important problems facing humanity. '\n",
      " '\\\\n\\\\nTo get started, I recommend exploring the resources on our website, '\n",
      " 'which cover our research, products, and approach to AI development. You can '\n",
      " \"also reach out to our team to learn more about how Anthropic's technology \"\n",
      " 'and services can support your specific needs.\\\\n\\\\nThe key is to engage with '\n",
      " 'us in a way that aligns with our values of transparency, ethical AI, and a '\n",
      " \"commitment to the wellbeing of humanity. We're here to collaborate and help \"\n",
      " 'you harness the power of advanced AI responsibly.\", '\n",
      " \"response_metadata={'id': 'msg_0187BVnpniPDJnVvwf3M1LdY', 'content': \"\n",
      " '[ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you should '\n",
      " 'first familiarize yourself with our mission and principles. Anthropic is '\n",
      " 'committed to developing safe and beneficial artificial intelligence that can '\n",
      " 'help solve important problems facing humanity. \\\\n\\\\nTo get started, I '\n",
      " 'recommend exploring the resources on our website, which cover our research, '\n",
      " 'products, and approach to AI development. You can also reach out to our team '\n",
      " \"to learn more about how Anthropic's technology and services can support your \"\n",
      " 'specific needs.\\\\n\\\\nThe key is to engage with us in a way that aligns with '\n",
      " 'our values of transparency, ethical AI, and a commitment to the wellbeing of '\n",
      " \"humanity. We're here to collaborate and help you harness the power of \"\n",
      " 'advanced AI responsibly.\", type=\\'text\\')], \\'model\\': '\n",
      " \"'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', \"\n",
      " \"'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, \"\n",
      " 'output_tokens=160)})full_chain.invoke({\"question\": \"how do I use '\n",
      " 'LangChain?\"})AIMessage(content=\"As Harrison Chase told me, using LangChain '\n",
      " \"involves several key steps. First, you'll need to install the LangChain \"\n",
      " \"library and import the necessary modules. Then, you'll want to define your \"\n",
      " 'language model, any data sources you plan to use, and the specific tasks you '\n",
      " 'want to accomplish, such as question answering, text generation, or '\n",
      " 'agent-based reasoning. \\\\n\\\\nLangChain provides a flexible framework for '\n",
      " 'building applications that leverage large language models. It includes '\n",
      " 'abstractions for things like retrievers, prompts, and chains, which allow '\n",
      " 'you to compose different components together to create powerful workflows. '\n",
      " '\\\\n\\\\nThe documentation on the LangChain website is excellent and covers '\n",
      " \"many common use cases in detail. I'd recommend starting there to get a solid \"\n",
      " 'understanding of the core concepts and how to apply them to your specific '\n",
      " 'needs. And of course, feel free to reach out if you have any other questions '\n",
      " \"- I'm always happy to share more insights from my conversations with \"\n",
      " 'Harrison.\", response_metadata={\\'id\\': \\'msg_01T1naS99wGPkEAP4LME8iAv\\', '\n",
      " '\\'content\\': [ContentBlock(text=\"As Harrison Chase told me, using LangChain '\n",
      " \"involves several key steps. First, you'll need to install the LangChain \"\n",
      " \"library and import the necessary modules. Then, you'll want to define your \"\n",
      " 'language model, any data sources you plan to use, and the specific tasks you '\n",
      " 'want to accomplish, such as question answering, text generation, or '\n",
      " 'agent-based reasoning. \\\\n\\\\nLangChain provides a flexible framework for '\n",
      " 'building applications that leverage large language models. It includes '\n",
      " 'abstractions for things like retrievers, prompts, and chains, which allow '\n",
      " 'you to compose different components together to create powerful workflows. '\n",
      " '\\\\n\\\\nThe documentation on the LangChain website is excellent and covers '\n",
      " \"many common use cases in detail. I'd recommend starting there to get a solid \"\n",
      " 'understanding of the core concepts and how to apply them to your specific '\n",
      " 'needs. And of course, feel free to reach out if you have any other questions '\n",
      " \"- I'm always happy to share more insights from my conversations with \"\n",
      " 'Harrison.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', '\n",
      " \"'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, \"\n",
      " \"'type': 'message', 'usage': Usage(input_tokens=50, \"\n",
      " 'output_tokens=205)})full_chain.invoke({\"question\": \"whats 2 + '\n",
      " '2\"})AIMessage(content=\\'4\\', response_metadata={\\'id\\': '\n",
      " \"'msg_01T6T3TS6hRCtU8JayN93QEi', 'content': [ContentBlock(text='4', \"\n",
      " \"type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', \"\n",
      " \"'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', \"\n",
      " \"'usage': Usage(input_tokens=28, output_tokens=5)})Routing by semantic \"\n",
      " 'similarityOne especially useful technique is to use embeddings to route a '\n",
      " \"query to the most relevant prompt. Here's an example.from \"\n",
      " 'langchain.utils.math import cosine_similarityfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import PromptTemplatefrom langchain_core.runnables '\n",
      " 'import RunnableLambda, RunnablePassthroughfrom',\n",
      " 'langchain_openai import OpenAIEmbeddingsphysics_template = \"\"\"You are a very '\n",
      " 'smart physics professor. \\\\You are great at answering questions about '\n",
      " \"physics in a concise and easy to understand manner. \\\\When you don't know \"\n",
      " \"the answer to a question you admit that you don't know.Here is a \"\n",
      " 'question:{query}\"\"\"math_template = \"\"\"You are a very good mathematician. You '\n",
      " 'are great at answering math questions. \\\\You are so good because you are '\n",
      " 'able to break down hard problems into their component parts, \\\\answer the '\n",
      " 'component parts, and then put them together to answer the broader '\n",
      " 'question.Here is a question:{query}\"\"\"embeddings = '\n",
      " 'OpenAIEmbeddings()prompt_templates = [physics_template, '\n",
      " 'math_template]prompt_embeddings = '\n",
      " 'embeddings.embed_documents(prompt_templates)def prompt_router(input):    '\n",
      " 'query_embedding = embeddings.embed_query(input[\"query\"])    similarity = '\n",
      " 'cosine_similarity([query_embedding], prompt_embeddings)[0]    most_similar = '\n",
      " 'prompt_templates[similarity.argmax()]    print(\"Using MATH\" if most_similar '\n",
      " '== math_template else \"Using PHYSICS\")    return '\n",
      " 'PromptTemplate.from_template(most_similar)chain = (    {\"query\": '\n",
      " 'RunnablePassthrough()}    | RunnableLambda(prompt_router)    | '\n",
      " 'ChatAnthropic(model_name=\"claude-3-haiku-20240307\")    | '\n",
      " 'StrOutputParser())print(chain.invoke(\"What\\'s a black hole\"))Using PHYSICSAs '\n",
      " 'a physics professor, I would be happy to provide a concise and '\n",
      " 'easy-to-understand explanation of what a black hole is.A black hole is an '\n",
      " 'incredibly dense region of space-time where the gravitational pull is so '\n",
      " 'strong that nothing, not even light, can escape from it. This means that if '\n",
      " 'you were to get too close to a black hole, you would be pulled in and '\n",
      " 'crushed by the intense gravitational forces.The formation of a black hole '\n",
      " 'occurs when a massive star, much larger than our Sun, reaches the end of its '\n",
      " 'life and collapses in on itself. This collapse causes the matter to become '\n",
      " 'extremely dense, and the gravitational force becomes so strong that it '\n",
      " 'creates a point of no return, known as the event horizon.Beyond the event '\n",
      " 'horizon, the laws of physics as we know them break down, and the intense '\n",
      " 'gravitational forces create a singularity, which is a point of infinite '\n",
      " 'density and curvature in space-time.Black holes are fascinating and '\n",
      " 'mysterious objects, and there is still much to be learned about their '\n",
      " 'properties and behavior. If I were unsure about any specific details or '\n",
      " 'aspects of black holes, I would readily admit that I do not have a complete '\n",
      " 'understanding and would encourage further research and '\n",
      " 'investigation.print(chain.invoke(\"What\\'s a path integral\"))Using MATHA path '\n",
      " 'integral is a powerful mathematical concept in physics, particularly in the '\n",
      " 'field of quantum mechanics. It was developed by the renowned physicist '\n",
      " 'Richard Feynman as an alternative formulation of quantum mechanics.In a path '\n",
      " 'integral, instead of considering a single, definite path that a particle '\n",
      " 'might take from one point to another, as in classical mechanics, the '\n",
      " 'particle is considered to take all possible paths simultaneously. Each path '\n",
      " 'is assigned a complex-valued weight, and the total probability amplitude for '\n",
      " 'the particle to go from one point to another is calculated by summing '\n",
      " '(integrating) over all possible paths.The key ideas behind the path integral '\n",
      " 'formulation are:1. Superposition principle: In quantum mechanics, particles '\n",
      " 'can exist in a superposition of multiple states or paths simultaneously.2. '\n",
      " 'Probability amplitude: The probability amplitude for a particle to go from '\n",
      " 'one point to another is calculated by summing the complex-valued weights of '\n",
      " 'all possible paths.3. Weighting of paths: Each path is assigned a weight '\n",
      " 'based on the action (the time integral of the Lagrangian) along that path. '\n",
      " \"Paths with lower action have a greater weight.4. Feynman's approach: Feynman \"\n",
      " 'developed the path integral formulation as an alternative to the traditional '\n",
      " 'wave function approach in quantum mechanics, providing a more intuitive and '\n",
      " 'conceptual understanding of quantum phenomena.The path integral approach is '\n",
      " 'particularly useful in quantum field theory, where it provides a powerful '\n",
      " 'framework for calculating transition probabilities and understanding the '\n",
      " 'behavior of quantum systems. It has also found applications in various areas '\n",
      " 'of physics, such as condensed matter, statistical mechanics, and even in '\n",
      " 'finance (the path integral approach to option pricing).The mathematical '\n",
      " 'construction of the path integral involves the use of advanced concepts from '\n",
      " 'functional analysis and measure theory, making it a powerful and '\n",
      " \"sophisticated tool in the physicist's arsenal.Help us out by providing \"\n",
      " 'feedback on this documentation page:PreviousAdd message history '\n",
      " '(memory)NextInspect your runnablesExample SetupUsing a custom function '\n",
      " '(Recommended)Using a '\n",
      " 'RunnableBranchCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n\\n\\n\\n\\n\\n\\n\\nAdd message history (memory) | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageAdd message history (memory)On this '\n",
      " 'pageAdd message history (memory)The RunnableWithMessageHistory lets us add '\n",
      " 'message history to certain types of chains. It wraps another Runnable and '\n",
      " 'manages the chat message history for it.Specifically, it can be used for any '\n",
      " 'Runnable that takes as input one ofa sequence of BaseMessagea dict with a '\n",
      " 'key that takes a sequence of BaseMessagea dict with a key that takes the '\n",
      " 'latest message(s) as a string or sequence of BaseMessage, and a separate key '\n",
      " 'that takes historical messagesAnd returns as output one ofa string that can '\n",
      " 'be treated as the contents of an AIMessagea sequence of BaseMessagea dict '\n",
      " \"with a key that contains a sequence of BaseMessageLet's take a look at some \"\n",
      " 'examples to see how it works. First we construct a runnable (which here '\n",
      " 'accepts a dict as input and returns a message as output):from '\n",
      " 'langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom '\n",
      " 'langchain_openai.chat_models import ChatOpenAImodel = ChatOpenAI()prompt = '\n",
      " 'ChatPromptTemplate.from_messages(    [        (            '\n",
      " '\"system\",            \"You\\'re an assistant who\\'s good at {ability}. Respond '\n",
      " 'in 20 words or fewer\",        ),        '\n",
      " 'MessagesPlaceholder(variable_name=\"history\"),        (\"human\", '\n",
      " '\"{input}\"),    ])runnable = prompt | modelTo manage the message history, we '\n",
      " 'will need:This runnable;A callable that returns an instance of '\n",
      " 'BaseChatMessageHistory.Check out the memory integrations page for '\n",
      " 'implementations of chat message histories using Redis and other providers. '\n",
      " 'Here we demonstrate using an in-memory ChatMessageHistory as well as more '\n",
      " 'persistent storage using RedisChatMessageHistory.In-memory\\u200bBelow we '\n",
      " 'show a simple example in which the chat history lives in memory, in this '\n",
      " 'case via a global Python dict.We construct a callable get_session_history '\n",
      " 'that references this dict to return an instance of ChatMessageHistory. The '\n",
      " 'arguments to the callable can be specified by passing a configuration to the '\n",
      " 'RunnableWithMessageHistory at runtime. By default, the configuration '\n",
      " 'parameter is expected to be a single string session_id. This can be adjusted '\n",
      " 'via the history_factory_config kwarg.Using the single-parameter default:from '\n",
      " 'langchain_community.chat_message_histories import ChatMessageHistoryfrom '\n",
      " 'langchain_core.chat_history import BaseChatMessageHistoryfrom '\n",
      " 'langchain_core.runnables.history import RunnableWithMessageHistorystore = '\n",
      " '{}def get_session_history(session_id: str) -> BaseChatMessageHistory:    if '\n",
      " 'session_id not in store:        store[session_id] = ChatMessageHistory()    '\n",
      " 'return store[session_id]with_message_history = '\n",
      " 'RunnableWithMessageHistory(    runnable,    get_session_history,    '\n",
      " 'input_messages_key=\"input\",    history_messages_key=\"history\",)Note that '\n",
      " \"we've specified input_messages_key (the key to be treated as the latest \"\n",
      " 'input message) and history_messages_key (the key to add historical messages '\n",
      " 'to).When invoking this new runnable, we specify the corresponding chat '\n",
      " 'history via a configuration parameter:with_message_history.invoke(    '\n",
      " '{\"ability\": \"math\", \"input\": \"What does cosine mean?\"},    '\n",
      " 'config={\"configurable\": {\"session_id\": '\n",
      " '\"abc123\"}},)AIMessage(content=\\'Cosine is a trigonometric function that '\n",
      " 'calculates the ratio of the adjacent side to the hypotenuse of a right '\n",
      " 'triangle.\\')# Rememberswith_message_history.invoke(    {\"ability\": \"math\", '\n",
      " '\"input\": \"What?\"},    config={\"configurable\": {\"session_id\": '\n",
      " '\"abc123\"}},)AIMessage(content=\\'Cosine is a mathematical function used to '\n",
      " \"calculate the length of a side in a right triangle.')# New session_id --> \"\n",
      " 'does not remember.with_message_history.invoke(    {\"ability\": \"math\", '\n",
      " '\"input\": \"What?\"},    config={\"configurable\": {\"session_id\": '\n",
      " '\"def234\"}},)AIMessage(content=\\'I can help with math problems. What do you '\n",
      " \"need assistance with?')The configuration parameters by which we track \"\n",
      " 'message histories can be customized by passing in a list of '\n",
      " 'ConfigurableFieldSpec objects to the history_factory_config parameter. '\n",
      " 'Below, we use two parameters: a user_id and conversation_id.from '\n",
      " 'langchain_core.runnables import ConfigurableFieldSpecstore = {}def '\n",
      " 'get_session_history(user_id: str, conversation_id: str) -> '\n",
      " 'BaseChatMessageHistory:    if (user_id, conversation_id) not in '\n",
      " 'store:        store[(user_id, conversation_id)] = ChatMessageHistory()    '\n",
      " 'return store[(user_id, conversation_id)]with_message_history = '\n",
      " 'RunnableWithMessageHistory(    runnable,    get_session_history,    '\n",
      " 'input_messages_key=\"input\",    history_messages_key=\"history\",    '\n",
      " 'history_factory_config=[        ConfigurableFieldSpec(            '\n",
      " 'id=\"user_id\",            annotation=str,            name=\"User '\n",
      " 'ID\",            description=\"Unique identifier for the user.\",            '\n",
      " 'default=\"\",            is_shared=True,        ),        '\n",
      " 'ConfigurableFieldSpec(            id=\"conversation_id\",            '\n",
      " 'annotation=str,            name=\"Conversation ID\",            '\n",
      " 'description=\"Unique identifier for the conversation.\",            '\n",
      " 'default=\"\",            is_shared=True,        ),    '\n",
      " '],)with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"Hello\"},    '\n",
      " 'config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": '\n",
      " '\"1\"}},)Examples with runnables of different signatures\\u200bThe above '\n",
      " 'runnable takes a dict as input and returns a BaseMessage. Below we show some '\n",
      " 'alternatives.Messages input, dict output\\u200bfrom langchain_core.messages '\n",
      " 'import HumanMessagefrom langchain_core.runnables import '\n",
      " 'RunnableParallelchain = RunnableParallel({\"output_message\": '\n",
      " 'ChatOpenAI()})def get_session_history(session_id: str) -> '\n",
      " 'BaseChatMessageHistory:    if session_id not in store:        '\n",
      " 'store[session_id] = ChatMessageHistory()    return '\n",
      " 'store[session_id]with_message_history = RunnableWithMessageHistory(    '\n",
      " 'chain,    get_session_history,    '\n",
      " 'output_messages_key=\"output_message\",)with_message_history.invoke(    '\n",
      " '[HumanMessage(content=\"What did Simone de Beauvoir believe about free '\n",
      " 'will\")],    config={\"configurable\": {\"session_id\": '\n",
      " '\"baz\"}},){\\'output_message\\': AIMessage(content=\"Simone de Beauvoir believed '\n",
      " 'in the existence of free will. She argued that individuals have the ability '\n",
      " 'to make choices and determine their own actions, even in the face of social '\n",
      " 'and cultural constraints. She rejected the idea that individuals are purely '\n",
      " 'products of their environment or predetermined by biology or destiny. '\n",
      " 'Instead, she emphasized the importance of personal responsibility and the '\n",
      " 'need for individuals to actively engage in creating their own lives and '\n",
      " 'defining their own existence. De Beauvoir believed that freedom and agency '\n",
      " \"come from recognizing one's own freedom and actively exercising it\",\n",
      " 'in the pursuit of personal and collective '\n",
      " 'liberation.\")}with_message_history.invoke(    [HumanMessage(content=\"How did '\n",
      " 'this compare to Sartre\")],    config={\"configurable\": {\"session_id\": '\n",
      " '\"baz\"}},){\\'output_message\\': AIMessage(content=\\'Simone de Beauvoir\\\\\\'s '\n",
      " 'views on free will were closely aligned with those of her contemporary and '\n",
      " 'partner Jean-Paul Sartre. Both de Beauvoir and Sartre were existentialist '\n",
      " 'philosophers who emphasized the importance of individual freedom and the '\n",
      " 'rejection of determinism. They believed that human beings have the capacity '\n",
      " 'to transcend their circumstances and create their own meaning and '\n",
      " 'values.\\\\n\\\\nSartre, in his famous work \"Being and Nothingness,\" argued that '\n",
      " 'human beings are condemned to be free, meaning that we are burdened with the '\n",
      " 'responsibility of making choices and defining ourselves in a world that '\n",
      " 'lacks inherent meaning. Like de Beauvoir, Sartre believed that individuals '\n",
      " 'have the ability to exercise their freedom and make choices in the face of '\n",
      " 'external and internal constraints.\\\\n\\\\nWhile there may be some nuanced '\n",
      " 'differences in their philosophical writings, overall, de Beauvoir and Sartre '\n",
      " 'shared a similar belief in the existence of free will and the importance of '\n",
      " \"individual agency in shaping one\\\\'s own life.')}Messages input, messages \"\n",
      " 'output\\u200bRunnableWithMessageHistory(    ChatOpenAI(),    '\n",
      " 'get_session_history,)Dict with single key for all messages input, messages '\n",
      " 'output\\u200bfrom operator import itemgetterRunnableWithMessageHistory(    '\n",
      " 'itemgetter(\"input_messages\") | ChatOpenAI(),    get_session_history,    '\n",
      " 'input_messages_key=\"input_messages\",)Persistent storage\\u200bIn many cases '\n",
      " 'it is preferable to persist conversation histories. '\n",
      " 'RunnableWithMessageHistory is agnostic as to how the get_session_history '\n",
      " 'callable retrieves its chat message histories. See here for an example using '\n",
      " 'a local filesystem. Below we demonstrate how one could use Redis. Check out '\n",
      " 'the memory integrations page for implementations of chat message histories '\n",
      " \"using other providers.Setup\\u200bWe'll need to install Redis if it's not \"\n",
      " 'installed already:%pip install --upgrade --quiet redisStart a local Redis '\n",
      " \"Stack server if we don't have an existing Redis deployment to connect \"\n",
      " 'to:docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latestREDIS_URL '\n",
      " '= \"redis://localhost:6379/0\"LangSmith\\u200bLangSmith is especially useful '\n",
      " 'for something like message history injection, where it can be hard to '\n",
      " 'otherwise understand what the inputs are to various parts of the chain.Note '\n",
      " 'that LangSmith is not needed, but it is helpful.',\n",
      " 'If you do want to use LangSmith, after you sign up at the link above, make '\n",
      " 'sure to uncoment the below and set your environment variables to start '\n",
      " 'logging traces:# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"# '\n",
      " 'os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()Updating the message '\n",
      " 'history implementation just requires us to define a new callable, this time '\n",
      " 'returning an instance of RedisChatMessageHistory:from '\n",
      " 'langchain_community.chat_message_histories import RedisChatMessageHistorydef '\n",
      " 'get_message_history(session_id: str) -> RedisChatMessageHistory:    return '\n",
      " 'RedisChatMessageHistory(session_id, url=REDIS_URL)with_message_history = '\n",
      " 'RunnableWithMessageHistory(    runnable,    get_message_history,    '\n",
      " 'input_messages_key=\"input\",    history_messages_key=\"history\",)We can invoke '\n",
      " 'as before:with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What '\n",
      " 'does cosine mean?\"},    config={\"configurable\": {\"session_id\": '\n",
      " '\"foobar\"}},)AIMessage(content=\\'Cosine is a trigonometric function that '\n",
      " 'represents the ratio of the adjacent side to the hypotenuse in a right '\n",
      " 'triangle.\\')with_message_history.invoke(    {\"ability\": \"math\", \"input\": '\n",
      " '\"What\\'s its inverse\"},    config={\"configurable\": {\"session_id\": '\n",
      " '\"foobar\"}},)AIMessage(content=\\'The inverse of cosine is the arccosine '\n",
      " 'function, denoted as acos or cos^-1, which gives the angle corresponding to '\n",
      " \"a given cosine value.')tipLangsmith traceLooking at the Langsmith trace for \"\n",
      " 'the second call, we can see that when constructing the prompt, a \"history\" '\n",
      " 'variable has been injected which is a list of two messages (our first input '\n",
      " 'and first output).Help us out by providing feedback on this documentation '\n",
      " 'page:PreviousStreamingNextRoute logic based on inputIn-memoryExamples with '\n",
      " 'runnables of different signaturesPersistent '\n",
      " 'storageSetupLangSmithCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n\\n\\n\\n\\n\\n\\n\\nInspect your runnables | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreInspect your runnablesOn this pageInspect your runnablesOnce you '\n",
      " 'create a runnable with LCEL, you may often want to inspect it to get a '\n",
      " 'better sense for what is going on. This notebook covers some methods for '\n",
      " \"doing so.First, let's create an example LCEL. We will create one that does \"\n",
      " 'retrieval%pip install --upgrade --quiet  langchain langchain-openai '\n",
      " 'faiss-cpu tiktokenfrom langchain_community.vectorstores import FAISSfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnablePassthroughfrom langchain_openai '\n",
      " 'import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    '\n",
      " '[\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)model = ChatOpenAI()chain = (    '\n",
      " '{\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | '\n",
      " 'model    | StrOutputParser())Get a graph\\u200bYou can get a graph of the '\n",
      " 'runnablechain.get_graph()Print a graph\\u200bWhile that is not super legible, '\n",
      " \"you can print it to get a display that's easier to \"\n",
      " 'understandchain.get_graph().print_ascii()           '\n",
      " '+---------------------------------+                    | '\n",
      " 'Parallel<context,question>Input |                    '\n",
      " '+---------------------------------+                             '\n",
      " '**               **                                 ***                   '\n",
      " '***                            **                         **           '\n",
      " '+----------------------+              +-------------+  | '\n",
      " 'VectorStoreRetriever |              | Passthrough |  '\n",
      " '+----------------------+              +-------------+                      '\n",
      " '**               **                                      ***         '\n",
      " '***                                           **     '\n",
      " '**                                '\n",
      " '+----------------------------------+                   | '\n",
      " 'Parallel<context,question>Output |                   '\n",
      " '+----------------------------------+                                     '\n",
      " '*                                                      '\n",
      " '*                                                      '\n",
      " '*                                           '\n",
      " '+--------------------+                                 | ChatPromptTemplate '\n",
      " '|                                 '\n",
      " '+--------------------+                                            '\n",
      " '*                                                      '\n",
      " '*                                                      '\n",
      " '*                                               '\n",
      " '+------------+                                         | ChatOpenAI '\n",
      " '|                                         '\n",
      " '+------------+                                                '\n",
      " '*                                                      '\n",
      " '*                                                      '\n",
      " '*                                            '\n",
      " '+-----------------+                                    | StrOutputParser '\n",
      " '|                                    '\n",
      " '+-----------------+                                              '\n",
      " '*                                                      *',\n",
      " '*                                         '\n",
      " '+-----------------------+                              | '\n",
      " 'StrOutputParserOutput |                              '\n",
      " '+-----------------------+Get the prompts\\u200bAn important part of every '\n",
      " 'chain is the prompts that are used. You can get the prompts present in the '\n",
      " \"chain:chain.get_prompts()[ChatPromptTemplate(input_variables=['context', \"\n",
      " \"'question'], \"\n",
      " \"messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', \"\n",
      " \"'question'], template='Answer the question based only on the following \"\n",
      " \"context:\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\n'))])]Help us out by \"\n",
      " 'providing feedback on this documentation page:PreviousRoute logic based on '\n",
      " 'inputNextCreate a runnable with the @chain decoratorGet a graphPrint a '\n",
      " 'graphGet the '\n",
      " 'promptsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Create a runnable with the @chain decorator | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreCreate a runnable with the @chain decoratorCreate a runnable '\n",
      " 'with the @chain decoratorYou can also turn an arbitrary function into a '\n",
      " 'chain by adding a @chain decorator. This is functionaly equivalent to '\n",
      " 'wrapping in a RunnableLambda.This will have the benefit of improved '\n",
      " 'observability by tracing your chain correctly. Any calls to runnables inside '\n",
      " 'this function will be traced as nested childen.It will also allow you to use '\n",
      " \"this as any other runnable, compose it in chain, etc.Let's take a look at \"\n",
      " 'this in action!%pip install --upgrade --quiet  langchain '\n",
      " 'langchain-openaifrom langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import chainfrom langchain_openai import '\n",
      " 'ChatOpenAIprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about '\n",
      " '{topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of '\n",
      " 'this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = '\n",
      " 'prompt1.invoke({\"topic\": text})    output1 = '\n",
      " 'ChatOpenAI().invoke(prompt_val1)    parsed_output1 = '\n",
      " 'StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | '\n",
      " 'StrOutputParser()    return chain2.invoke({\"joke\": '\n",
      " 'parsed_output1})custom_chain is now a runnable, meaning you will need to use '\n",
      " 'invokecustom_chain.invoke(\"bears\")\\'The subject of this joke is bears.\\'If '\n",
      " 'you check out your LangSmith traces, you should see a custom_chain trace in '\n",
      " 'there, with the calls to OpenAI nested underneathHelp us out by providing '\n",
      " 'feedback on this documentation page:PreviousInspect your '\n",
      " 'runnablesNextManaging prompt '\n",
      " 'sizeCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' ----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Get started | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageGet startedOn this pageGet startedLCEL '\n",
      " 'makes it easy to build complex chains from basic components, and supports '\n",
      " 'out of the box functionality such as streaming, parallelism, and '\n",
      " 'logging.Basic example: prompt + model + output parser\\u200bThe most basic '\n",
      " 'and common use case is chaining a prompt template and a model together. To '\n",
      " \"see how this works, let's create a chain that takes a topic and generates a \"\n",
      " 'joke:%pip install --upgrade --quiet  langchain-core langchain-community '\n",
      " 'langchain-openaiOpenAIAnthropicGoogleCohereFireworksAIMistralAITogetherAIInstall '\n",
      " 'dependenciespip install -qU langchain-openaiSet environment variablesimport '\n",
      " 'getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4\")Install '\n",
      " 'dependenciespip install -qU langchain-anthropicSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_anthropic import ChatAnthropicmodel = '\n",
      " 'ChatAnthropic(model=\"claude-3-sonnet-20240229\")Install dependenciespip '\n",
      " 'install -qU langchain-google-vertexaiSet environment variablesimport '\n",
      " 'getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_google_vertexai import ChatVertexAImodel = '\n",
      " 'ChatVertexAI(model=\"gemini-pro\")Install dependenciespip install -qU '\n",
      " 'langchain-cohereSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere '\n",
      " 'import ChatCoheremodel = ChatCohere(model=\"command-r\")Install '\n",
      " 'dependenciespip install -qU langchain-fireworksSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_fireworks import ChatFireworksmodel = '\n",
      " 'ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")Install '\n",
      " 'dependenciespip install -qU langchain-mistralaiSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_mistralai import ChatMistralAImodel = '\n",
      " 'ChatMistralAI(model=\"mistral-large-latest\")Install dependenciespip install '\n",
      " '-qU langchain-openaiSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai '\n",
      " 'import ChatOpenAImodel = ChatOpenAI(    '\n",
      " 'base_url=\"https://api.together.xyz/v1\",    '\n",
      " 'api_key=os.environ[\"TOGETHER_API_KEY\"],    '\n",
      " 'model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)# | output: false# | echo: '\n",
      " 'falsefrom langchain_openai import ChatOpenAImodel = '\n",
      " 'ChatOpenAI(model=\"gpt-4\")from langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = '\n",
      " 'ChatPromptTemplate.from_template(\"tell me a short joke about '\n",
      " '{topic}\")output_parser = StrOutputParser()chain = prompt | model | '\n",
      " 'output_parserchain.invoke({\"topic\": \"ice cream\"})\"Why don\\'t ice creams ever '\n",
      " 'get invited to parties?\\\\n\\\\nBecause they always drip when things heat '\n",
      " 'up!\"Notice this line of the code, where we piece together these different '\n",
      " 'components into a single chain using LCEL:chain = prompt | model | '\n",
      " 'output_parserThe | symbol is similar to a unix pipe operator, which chains '\n",
      " 'together the different components, feeding the output from one component as '\n",
      " 'input into the next component. In this chain the user input is passed to the '\n",
      " 'prompt template, then the prompt template output is passed to the model, '\n",
      " \"then the model output is passed to the output parser. Let's take a look at \"\n",
      " \"each component individually to really understand what's going on.1. \"\n",
      " 'Prompt\\u200bprompt is a BasePromptTemplate, which means it takes in a '\n",
      " 'dictionary of template variables and produces a PromptValue. A PromptValue '\n",
      " 'is a wrapper around a completed prompt that can be passed to either an LLM '\n",
      " '(which takes a string as input) or ChatModel (which takes a sequence of '\n",
      " 'messages as input). It can work with either language model type because it '\n",
      " 'defines logic both for producing BaseMessages and for producing a '\n",
      " 'string.prompt_value = prompt.invoke({\"topic\": \"ice '\n",
      " 'cream\"})prompt_valueChatPromptValue(messages=[HumanMessage(content=\\'tell me '\n",
      " 'a short joke about ice '\n",
      " \"cream')])prompt_value.to_messages()[HumanMessage(content='tell me a short \"\n",
      " \"joke about ice cream')]prompt_value.to_string()'Human: tell me a short joke \"\n",
      " \"about ice cream'2. Model\\u200bThe PromptValue is then passed to model. In \"\n",
      " 'this case our model is a ChatModel, meaning it will output a '\n",
      " 'BaseMessage.message = '\n",
      " 'model.invoke(prompt_value)messageAIMessage(content=\"Why don\\'t ice creams '\n",
      " 'ever get invited to parties?\\\\n\\\\nBecause they always bring a melt down!\")If '\n",
      " 'our model was an LLM, it would output a string.from langchain_openai import '\n",
      " 'OpenAIllm = '\n",
      " 'OpenAI(model=\"gpt-3.5-turbo-instruct\")llm.invoke(prompt_value)\\'\\\\n\\\\nRobot: '\n",
      " \"Why did the ice cream truck break down? Because it had a meltdown!'3. Output \"\n",
      " 'parser\\u200bAnd lastly we pass our model output to the output_parser, which '\n",
      " 'is a BaseOutputParser meaning it takes either a string or a',\n",
      " 'BaseMessage as input. The specific StrOutputParser simply converts any input '\n",
      " 'into a string.output_parser.invoke(message)\"Why did the ice cream go to '\n",
      " \"therapy? \\\\n\\\\nBecause it had too many toppings and couldn't find its \"\n",
      " 'cone-fidence!\"4. Entire Pipeline\\u200bTo follow the steps along:We pass in '\n",
      " 'user input on the desired topic as {\"topic\": \"ice cream\"}The prompt '\n",
      " 'component takes the user input, which is then used to construct a '\n",
      " 'PromptValue after using the topic to construct the prompt. The model '\n",
      " 'component takes the generated prompt, and passes into the OpenAI LLM model '\n",
      " 'for evaluation. The generated output from the model is a ChatMessage object. '\n",
      " 'Finally, the output_parser component takes in a ChatMessage, and transforms '\n",
      " 'this into a Python string, which is returned from the invoke method. '\n",
      " 'infoNote that if you‚Äôre curious about the output of any components, you can '\n",
      " 'always test out a smaller version of the chain such as prompt or prompt | '\n",
      " 'model to see the intermediate results:input = {\"topic\": \"ice '\n",
      " 'cream\"}prompt.invoke(input)# > '\n",
      " \"ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about \"\n",
      " 'ice cream\\')])(prompt | model).invoke(input)# > AIMessage(content=\"Why did '\n",
      " \"the ice cream go to therapy?\\\\nBecause it had too many toppings and couldn't \"\n",
      " 'cone-trol itself!\")RAG Search Example\\u200bFor our next example, we want to '\n",
      " 'run a retrieval-augmented generation chain to add some context when '\n",
      " 'responding to '\n",
      " 'questions.OpenAIAnthropicGoogleCohereFireworksAIMistralAITogetherAIInstall '\n",
      " 'dependenciespip install -qU langchain-openaiSet environment variablesimport '\n",
      " 'getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_openai import ChatOpenAImodel = '\n",
      " 'ChatOpenAI(model=\"gpt-3.5-turbo-0125\")Install dependenciespip install -qU '\n",
      " 'langchain-anthropicSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_anthropic import ChatAnthropicmodel = '\n",
      " 'ChatAnthropic(model=\"claude-3-sonnet-20240229\")Install dependenciespip '\n",
      " 'install -qU langchain-google-vertexaiSet environment variablesimport '\n",
      " 'getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from '\n",
      " 'langchain_google_vertexai import ChatVertexAImodel = '\n",
      " 'ChatVertexAI(model=\"gemini-pro\")Install dependenciespip install -qU '\n",
      " 'langchain-cohereSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere '\n",
      " 'import ChatCoheremodel = ChatCohere(model=\"command-r\")Install '\n",
      " 'dependenciespip install -qU langchain-fireworksSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_fireworks import ChatFireworksmodel = '\n",
      " 'ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")Install '\n",
      " 'dependenciespip install -qU langchain-mistralaiSet environment '\n",
      " 'variablesimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = '\n",
      " 'getpass.getpass()from langchain_mistralai import ChatMistralAImodel = '\n",
      " 'ChatMistralAI(model=\"mistral-large-latest\")Install dependenciespip install '\n",
      " '-qU langchain-openaiSet environment variablesimport getpassimport '\n",
      " 'osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai '\n",
      " 'import ChatOpenAImodel = ChatOpenAI(    '\n",
      " 'base_url=\"https://api.together.xyz/v1\",    '\n",
      " 'api_key=os.environ[\"TOGETHER_API_KEY\"],    '\n",
      " 'model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)# Requires:# pip install '\n",
      " 'langchain docarray tiktokenfrom langchain_community.vectorstores import '\n",
      " 'DocArrayInMemorySearchfrom langchain_core.output_parsers import '\n",
      " 'StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom '\n",
      " 'langchain_core.runnables import RunnableParallel, RunnablePassthroughfrom '\n",
      " 'langchain_openai import OpenAIEmbeddingsvectorstore = '\n",
      " 'DocArrayInMemorySearch.from_texts(    [\"harrison worked at kensho\", \"bears '\n",
      " 'like to eat honey\"],    embedding=OpenAIEmbeddings(),)retriever = '\n",
      " 'vectorstore.as_retriever()template = \"\"\"Answer the question based only on '\n",
      " 'the following context:{context}Question: {question}\"\"\"prompt = '\n",
      " 'ChatPromptTemplate.from_template(template)output_parser = '\n",
      " 'StrOutputParser()setup_and_retrieval = RunnableParallel(    {\"context\": '\n",
      " 'retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | '\n",
      " 'prompt | model | output_parserchain.invoke(\"where did harrison work?\")In '\n",
      " 'this case, the composed chain is: chain = setup_and_retrieval | prompt | '\n",
      " 'model | output_parserTo explain this, we first can see that the prompt '\n",
      " 'template above takes in context and question as values to be substituted in '\n",
      " 'the prompt. Before building the prompt template, we want to retrieve '\n",
      " 'relevant documents to the search and include them as part of the context. As '\n",
      " 'a preliminary step, we‚Äôve setup the retriever using an in memory store, '\n",
      " 'which can retrieve documents based on a query. This is a runnable component '\n",
      " 'as well that can be chained together with other components, but you can also '\n",
      " 'try to run it separately:retriever.invoke(\"where did harrison work?\")We then '\n",
      " 'use the RunnableParallel to prepare the expected inputs into the prompt by '\n",
      " 'using the entries for the retrieved documents as well as the original user '\n",
      " 'question, using the retriever for document search, and RunnablePassthrough '\n",
      " 'to pass the user‚Äôs question:setup_and_retrieval = RunnableParallel(    '\n",
      " '{\"context\": retriever, \"question\": RunnablePassthrough()})To review, the '\n",
      " 'complete chain is:setup_and_retrieval = RunnableParallel(    {\"context\": '\n",
      " 'retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | '\n",
      " 'prompt | model | output_parserWith the flow being:The first steps create a '\n",
      " 'RunnableParallel object with two entries.  The first entry, context will '\n",
      " 'include the document results fetched by the retriever. The second entry, '\n",
      " 'question will contain the user‚Äôs original question. To pass on the question, '\n",
      " 'we use RunnablePassthrough to copy this entry. Feed the dictionary from the '\n",
      " 'step above to the prompt component. It then takes the user input which is '\n",
      " 'question as well as the retrieved document which is context to construct a '\n",
      " 'prompt and output a PromptValue. The model component takes the generated '\n",
      " 'prompt, and passes into the OpenAI LLM model for evaluation. The generated '\n",
      " 'output from the model is a ChatMessage object. Finally, the output_parser '\n",
      " 'component takes in a ChatMessage, and transforms this into a Python string, '\n",
      " 'which is returned from the invoke method.Next steps\\u200bWe recommend '\n",
      " 'reading our Advantages of LCEL section next to see a side-by-side comparison '\n",
      " 'of the code needed to produce common functionality with and without '\n",
      " 'LCEL.Help us out by providing feedback on this documentation '\n",
      " 'page:PreviousLangChain Expression Language (LCEL)NextRunnable interfaceBasic '\n",
      " 'example: prompt + model + output parser1. Prompt2. Model3. Output parser4. '\n",
      " 'Entire PipelineRAG Search ExampleNext '\n",
      " 'stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n\\n\\n\\n\\n\\n\\n\\nManaging prompt size | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreManaging prompt sizeManaging prompt sizeAgents dynamically call '\n",
      " 'tools. The results of those tool calls are added back to the prompt, so that '\n",
      " 'the agent can plan the next action. Depending on what tools are being used '\n",
      " \"and how they're being called, the agent prompt can easily grow larger than \"\n",
      " \"the model context window.With LCEL, it's easy to add custom functionality \"\n",
      " \"for managing the size of prompts within your chain or agent. Let's look at \"\n",
      " 'simple agent example that can search Wikipedia for information.%pip install '\n",
      " '--upgrade --quiet  langchain langchain-openai wikipediafrom operator import '\n",
      " 'itemgetterfrom langchain.agents import AgentExecutor, load_toolsfrom '\n",
      " 'langchain.agents.format_scratchpad import '\n",
      " 'format_to_openai_function_messagesfrom langchain.agents.output_parsers '\n",
      " 'import OpenAIFunctionsAgentOutputParserfrom langchain_community.tools import '\n",
      " 'WikipediaQueryRunfrom langchain_community.utilities import '\n",
      " 'WikipediaAPIWrapperfrom langchain_core.prompt_values import '\n",
      " 'ChatPromptValuefrom langchain_core.prompts import ChatPromptTemplate, '\n",
      " 'MessagesPlaceholderfrom langchain_openai import ChatOpenAIwiki = '\n",
      " 'WikipediaQueryRun(    api_wrapper=WikipediaAPIWrapper(top_k_results=5, '\n",
      " 'doc_content_chars_max=10_000))tools = [wiki]prompt = '\n",
      " 'ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful '\n",
      " 'assistant\"),        (\"user\", \"{input}\"),        '\n",
      " 'MessagesPlaceholder(variable_name=\"agent_scratchpad\"),    ])llm = '\n",
      " 'ChatOpenAI(model=\"gpt-3.5-turbo\")Let\\'s try a many-step question without any '\n",
      " 'prompt size handling:agent = (    {        \"input\": '\n",
      " 'itemgetter(\"input\"),        \"agent_scratchpad\": lambda x: '\n",
      " 'format_to_openai_function_messages(            '\n",
      " 'x[\"intermediate_steps\"]        ),    }    | prompt    | '\n",
      " 'llm.bind_functions(tools)    | '\n",
      " 'OpenAIFunctionsAgentOutputParser())agent_executor = '\n",
      " 'AgentExecutor(agent=agent, tools=tools, '\n",
      " 'verbose=True)agent_executor.invoke(    {        \"input\": \"Who is the current '\n",
      " \"US president? What's their home state? What's their home state's bird? \"\n",
      " 'What\\'s that bird\\'s scientific name?\"    })\\x1b[1m> Entering new '\n",
      " 'AgentExecutor chain...\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with '\n",
      " '`List of presidents of the United States`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: '\n",
      " 'List of presidents of the United StatesSummary: The president of the United '\n",
      " 'States is the head of state and head of government of the United States, '\n",
      " 'indirectly elected to a four-year term via the Electoral College. The '\n",
      " 'officeholder leads the executive branch of the federal government and is the '\n",
      " 'commander-in-chief of the United States Armed Forces. Since the office was '\n",
      " 'established in 1789, 45 men have served in 46 presidencies. The first '\n",
      " 'president, George Washington, won a unanimous vote of the Electoral College. '\n",
      " 'Grover Cleveland served two non-consecutive terms and is therefore counted '\n",
      " 'as the 22nd and 24th president of the United States, giving rise to the '\n",
      " 'discrepancy between the number of presidencies and the number of individuals '\n",
      " 'who have served as president. The incumbent president is Joe Biden.The '\n",
      " 'presidency of William Henry Harrison, who died 31 days after taking office '\n",
      " 'in 1841, was the shortest in American history. Franklin D. Roosevelt served '\n",
      " 'the longest, over twelve years, before dying early in his fourth term in '\n",
      " '1945. He is the only U.S. president to have served more than two terms. '\n",
      " 'Since the ratification of the Twenty-second Amendment to the United States '\n",
      " 'Constitution in 1951, no person may be elected president more than twice, '\n",
      " 'and no one who has served more than two years of a term to which someone '\n",
      " 'else was elected may be elected more than once.Four presidents died in '\n",
      " 'office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. '\n",
      " 'Harding, and Franklin D. Roosevelt), four were assassinated (Abraham '\n",
      " 'Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one '\n",
      " 'resigned (Richard Nixon, facing impeachment and removal from office). John '\n",
      " 'Tyler was the first vice president to assume the presidency during a '\n",
      " 'presidential term, and set the precedent that a vice president who does so '\n",
      " 'becomes the fully functioning president with his presidency.Throughout most '\n",
      " 'of its history, American politics has been dominated by political parties. '\n",
      " 'The Constitution is silent on the issue of political parties, and at the '\n",
      " 'time it came into force in 1789, no organized parties existed. Soon after '\n",
      " 'the 1st Congress convened, political factions began rallying around dominant '\n",
      " 'Washington administration officials, such as Alexander Hamilton and Thomas '\n",
      " 'Jefferson. Concerned about the capacity of political parties to destroy the '\n",
      " 'fragile unity holding the nation together, Washington remained unaffiliated '\n",
      " 'with any political faction or party throughout his eight-year presidency. He '\n",
      " 'was, and remains, the only U.S. president never affiliated with a political '\n",
      " 'party.Page: List of presidents of the United States by ageSummary: In this '\n",
      " 'list of presidents of the United States by age, the first table charts the '\n",
      " 'age of each president of the United States at the time of presidential '\n",
      " 'inauguration (first inauguration if elected to multiple and consecutive '\n",
      " 'terms), upon leaving office, and at the time of death. Where the president '\n",
      " 'is still living, their lifespan and post-presidency timespan are calculated '\n",
      " 'up to January 25, 2024.Page: List of vice presidents of the United '\n",
      " 'StatesSummary: There have been 49 vice presidents of the United States since '\n",
      " 'the office was created in 1789. Originally, the vice president was the '\n",
      " 'person who received the second-most votes for president in the Electoral '\n",
      " 'College. But after the election of 1800 produced a tie between Thomas '\n",
      " 'Jefferson and Aaron Burr, requiring the House of Representatives to choose '\n",
      " 'between them, lawmakers acted to prevent such a situation from recurring. '\n",
      " 'The Twelfth Amendment was added to the Constitution in 1804, creating the '\n",
      " 'current system where electors cast a separate ballot for the vice '\n",
      " 'presidency.The vice president is the first person in the presidential line '\n",
      " 'of succession‚Äîthat is, they assume the presidency if the president dies, '\n",
      " 'resigns, or is impeached and removed from office. Nine vice presidents have '\n",
      " 'ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, '\n",
      " 'Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, '\n",
      " \"Harry S. Truman, and Lyndon B. Johnson) through the president's death and \"\n",
      " \"one (Gerald Ford) through the president's resignation. The vice president \"\n",
      " 'also serves as the president of the Senate and may choose to cast a '\n",
      " 'tie-breaking vote on decisions made by the Senate. Vice presidents have '\n",
      " 'exercised this latter power to varying extents over the years.Before '\n",
      " 'adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the '\n",
      " 'office of the vice president could not be filled until the next '\n",
      " 'post-election inauguration. Several such vacancies occurred: seven vice '\n",
      " 'presidents died, one resigned and eight succeeded to the presidency. This '\n",
      " 'amendment allowed for a vacancy to be filled through appointment by the '\n",
      " 'president and confirmation by both chambers of the Congress. Since its '\n",
      " 'ratification, the vice presidency has been vacant twice (both in the context '\n",
      " 'of scandals surrounding the Nixon administration) and was filled both times '\n",
      " \"through this process, namely in 1973 following Spiro Agnew's resignation, \"\n",
      " 'and again in 1974 after Gerald Ford succeeded to the presidency. The '\n",
      " 'amendment also established a procedure whereby a vice president may, if the '\n",
      " 'president is unable to discharge the powers and duties of the office, '\n",
      " 'temporarily assume the powers and duties of the office as acting president. '\n",
      " 'Three vice presidents have briefly acted as president under the 25th '\n",
      " 'Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, '\n",
      " 'and on July 21, 2007; and Kamala Harris on November 19, 2021.The persons who '\n",
      " 'have served as vice president were born in or primarily affiliated with 27 '\n",
      " 'states plus the District of Columbia. New York has produced the most of any '\n",
      " 'state as eight have been born there and three others considered it their '\n",
      " 'home state. Most vice presidents have been in their 50s or 60s and had '\n",
      " 'political experience before assuming the office. Two vice presidents‚ÄîGeorge '\n",
      " 'Clinton and John',\n",
      " 'C. Calhoun‚Äîserved under more than one president. Ill with tuberculosis and '\n",
      " 'recovering in Cuba on Inauguration Day in 1853, William R. King, by an Act '\n",
      " 'of Congress, was allowed to take the oath outside the United States. He is '\n",
      " 'the only vice president to take his oath of office in a foreign '\n",
      " 'country.Page: List of presidents of the United States by net worthSummary: '\n",
      " 'The list of presidents of the United States by net worth at peak varies '\n",
      " \"greatly. Debt and depreciation often means that presidents' net worth is \"\n",
      " 'less than $0 at the time of death. Most presidents before 1845 were '\n",
      " 'extremely wealthy, especially Andrew Jackson and George Washington.    '\n",
      " 'Presidents since 1929, when Herbert Hoover took office, have generally been '\n",
      " 'wealthier than presidents of the late nineteenth and early twentieth '\n",
      " 'centuries; with the exception of Harry S. Truman, all presidents since this '\n",
      " 'time have been millionaires. These presidents have often received income '\n",
      " 'from autobiographies and other writing. Except for Franklin D. Roosevelt and '\n",
      " 'John F. Kennedy (both of whom died while in office), all presidents '\n",
      " 'beginning with Calvin Coolidge have written autobiographies. In addition, '\n",
      " 'many presidents‚Äîincluding Bill Clinton‚Äîhave earned considerable income from '\n",
      " 'public speaking after leaving office.The richest president in history may be '\n",
      " 'Donald Trump. However, his net worth is not precisely known because the '\n",
      " 'Trump Organization is privately held.Truman was among the poorest U.S. '\n",
      " 'presidents, with a net worth considerably less than $1 million. His '\n",
      " 'financial situation contributed to the doubling of the presidential salary '\n",
      " 'to $100,000 in 1949. In addition, the presidential pension was created in '\n",
      " '1958 when Truman was again experiencing financial difficulties. Harry and '\n",
      " 'Bess Truman received the first Medicare cards in 1966 via the Social '\n",
      " 'Security Act of 1965.Page: List of presidents of the United States by home '\n",
      " 'stateSummary: These lists give the states of primary affiliation and of '\n",
      " 'birth for each president of the United '\n",
      " 'States.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Joe '\n",
      " 'Biden`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Joe BidenSummary: Joseph Robinette '\n",
      " 'Biden Jr. (  BY-d…ôn; born November 20, 1942) is an American politician who '\n",
      " 'is the 46th and current president of the United States. A member of the '\n",
      " 'Democratic Party, he previously served as the 47th vice president from 2009 '\n",
      " 'to 2017 under President Barack Obama and represented Delaware in the United '\n",
      " 'States Senate from 1973 to 2009.Born in Scranton, Pennsylvania, Biden moved '\n",
      " 'with his family to Delaware in 1953. He graduated from the University of '\n",
      " 'Delaware before earning his law degree from Syracuse University. He was '\n",
      " 'elected to the New Castle County Council in 1970 and to the U.S. Senate in '\n",
      " '1972. As a senator, Biden drafted and led the effort to pass the Violent '\n",
      " 'Crime Control and Law Enforcement Act and the Violence Against Women Act. He '\n",
      " 'also oversaw six U.S. Supreme Court confirmation hearings, including the '\n",
      " 'contentious hearings for Robert Bork and Clarence Thomas. Biden ran '\n",
      " 'unsuccessfully for the Democratic presidential nomination in 1988 and 2008. '\n",
      " 'In 2008, Obama chose Biden as his running mate, and he was a close counselor '\n",
      " 'to Obama during his two terms as vice president. In the 2020 presidential '\n",
      " 'election, Biden and his running mate, Kamala Harris, defeated incumbents '\n",
      " 'Donald Trump and Mike Pence. He became the oldest president in U.S. history, '\n",
      " 'and the first to have a female vice president.As president, Biden signed the '\n",
      " 'American Rescue Plan Act in response to the COVID-19 pandemic and subsequent '\n",
      " 'recession. He signed bipartisan bills on infrastructure and manufacturing. '\n",
      " 'He proposed the Build Back Better Act, which failed in Congress, but aspects '\n",
      " 'of which were incorporated into the Inflation Reduction Act that he signed '\n",
      " 'into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme '\n",
      " 'Court. He worked with congressional Republicans to resolve the 2023 United '\n",
      " 'States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. '\n",
      " \"In foreign policy, Biden restored America's membership in the Paris \"\n",
      " 'Agreement. He oversaw the complete withdrawal of U.S. troops from '\n",
      " 'Afghanistan that ended the war in Afghanistan, during which the Afghan '\n",
      " 'government collapsed and the Taliban seized control. He responded to the '\n",
      " 'Russian invasion of Ukraine by imposing sanctions on Russia and authorizing '\n",
      " 'civilian and military aid to Ukraine. During the Israel‚ÄìHamas war, Biden '\n",
      " 'announced military support for Israel, and condemned the actions of Hamas '\n",
      " 'and other Palestinian militants as terrorism. In April 2023, Biden announced '\n",
      " 'his candidacy for the Democratic nomination in the 2024 presidential '\n",
      " \"election.Page: Presidency of Joe BidenSummary: Joe Biden's tenure as the \"\n",
      " '46th president of the United States began with his inauguration on January '\n",
      " '20, 2021. Biden, a Democrat from Delaware who previously served as vice '\n",
      " 'president for two terms under president Barack Obama, took office following '\n",
      " 'his victory in the 2020 presidential election over Republican incumbent '\n",
      " 'president Donald Trump. Biden won the presidency with a popular vote of over '\n",
      " '81 million, the highest number of votes cast for a single United States '\n",
      " 'presidential candidate. Upon his inauguration, he became the oldest '\n",
      " 'president in American history, breaking the record set by his predecessor '\n",
      " 'Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, '\n",
      " 'and increased political polarization.On the first day of his presidency, '\n",
      " \"Biden made an effort to revert President Trump's energy policy by restoring \"\n",
      " 'U.S. participation in the Paris Agreement and revoking the permit for the '\n",
      " \"Keystone XL pipeline. He also halted funding for Trump's border wall, an \"\n",
      " 'expansion of the Mexican border wall. On his second day, he issued a series '\n",
      " 'of executive orders to reduce the impact of COVID-19, including invoking the '\n",
      " 'Defense Production Act of 1950, and set an early goal of achieving one '\n",
      " 'hundred million COVID-19 vaccinations in the United States in his first 100 '\n",
      " 'days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 '\n",
      " 'trillion stimulus bill that temporarily established expanded unemployment '\n",
      " 'insurance and sent $1,400 stimulus checks to most Americans in response to '\n",
      " 'continued economic pressure from COVID-19. He signed the bipartisan '\n",
      " 'Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden '\n",
      " 'alongside Democrats and Republicans in Congress, to invest in American '\n",
      " 'roads, bridges, public transit, ports and broadband access. Biden signed the '\n",
      " 'Juneteenth National Independence Day Act, making Juneteenth a federal '\n",
      " 'holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. '\n",
      " 'Supreme Court‚Äîthe first Black woman to serve on the court. After The Supreme '\n",
      " 'Court overturned Roe v. Wade, Biden took executive actions, such as the '\n",
      " \"signing of Executive Order 14076, to preserve and protect women's health \"\n",
      " 'rights nationwide, against abortion bans in Republican led states. Biden '\n",
      " 'proposed a significant expansion of the U.S. social safety net through the '\n",
      " 'Build Back Better Act, but those efforts, along with voting rights '\n",
      " 'legislation, failed in Congress. However, in August 2022, Biden signed the '\n",
      " 'Inflation Reduction Act of 2022, a domestic appropriations bill that '\n",
      " 'included some of the provisions of the Build Back Better Act after the '\n",
      " 'entire bill failed to pass. It included significant federal investment in '\n",
      " 'climate and domestic clean energy production, tax credits for solar panels, '\n",
      " 'electric cars and other home energy programs as well as a three-year '\n",
      " \"extension of Affordable Care Act subsidies. The administration's economic \"\n",
      " 'policies, known as \"Bidenomics\", were inspired and designed by Trickle-up '\n",
      " 'economics. Described as growing the economy from the middle out and bottom '\n",
      " 'up and growing the middle class. Biden signed the CHIPS and Science Act, '\n",
      " 'bolstering the semiconductor and manufacturing industry, the Honoring our '\n",
      " 'PACT Act, expanding health care for US veterans, the Bipartisan Safer '\n",
      " 'Communities Act and the Electoral Count Reform and Presidential Transition '\n",
      " 'Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, '\n",
      " 'which repealed the Defense of Marriage Act and codified same-sex and '\n",
      " 'interracial marriage in the United States. In response to the debt-ceiling '\n",
      " 'crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of '\n",
      " '2023, which restrains federal spending for fiscal years 2024 and 2025, '\n",
      " 'implements minor changes to SNAP and TANF, includes energy permitting '\n",
      " 'reform, claws back some IRS funding and unspent money for COVID-19, and '\n",
      " 'suspends the debt ceiling to January 1, 2025. Biden established the American '\n",
      " 'Climate Corps and created the first ever White House Office of Gun Violence '\n",
      " 'Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers '\n",
      " 'picket line during the 2023 United Auto Workers strike, making him the first '\n",
      " 'US president to visit one.The foreign policy goal of the Biden '\n",
      " 'administration is to restore the US to a \"position of trusted leadership\" '\n",
      " 'among global democracies in order to address the challenges posed by Russia '\n",
      " 'and China. In foreign policy, Biden completed the withdrawal of U.S. '\n",
      " 'military forces from Afghanistan, declaring an end to nation-building '\n",
      " 'efforts and shifting U.S. foreign policy toward strategic competition with '\n",
      " 'China and, to a lesser extent, Russia. However, during the withdrawal, the '\n",
      " 'Afghan government collapsed and the Taliban seized control, leading to Biden '\n",
      " 'receiving bipartisan criticism. He responded to the Russian invasion of '\n",
      " 'Ukraine by imposing sanctions on Russia as well as providing Ukraine with '\n",
      " 'over $100 billion in combined military, economic, and humanitarian aid. '\n",
      " 'Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi '\n",
      " 'al-Qurashi, the leader of the Islamic State, and approved a drone strike '\n",
      " 'which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed and created '\n",
      " 'AUKUS, an international security alliance, together with Australia and the '\n",
      " 'United Kingdom. Biden called for the expansion of NATO with the addition of '\n",
      " 'Finland and Sweden, and rallied NATO allies in support of Ukraine. During '\n",
      " 'the 2023 Israel‚ÄìHamas war, Biden condemned Hamas and other Palestinian '\n",
      " 'militants as terrorism and announced American military',\n",
      " 'support for Israel; Biden also showed his support and sympathy towards '\n",
      " 'Palestinians affected by the war, sent humanitarian aid, and brokered a '\n",
      " 'four-day temporary pause and hostage exchange.Page: Family of Joe '\n",
      " 'BidenSummary: Joe Biden, the 46th and current president of the United '\n",
      " 'States, has family members who are prominent in law, education, activism and '\n",
      " \"politics. Biden's immediate family became the first family of the United \"\n",
      " 'States on his inauguration on January 20, 2021. His immediate family circle '\n",
      " 'was also the second family of the United States from 2009 to 2017, when '\n",
      " \"Biden was vice president. Biden's family is mostly descended from the \"\n",
      " 'British Isles, with most of their ancestors coming from Ireland and England, '\n",
      " \"and a smaller number descending from the French.Of Joe Biden's sixteen \"\n",
      " 'great-great-grandparents, ten were born in Ireland. He is descended from the '\n",
      " \"Blewitts of County Mayo and the Finnegans of County Louth. One of Biden's \"\n",
      " 'great-great-great-grandfathers was born in Sussex, England, and emigrated to '\n",
      " 'Maryland in the United States by 1820.Page: Inauguration of Joe '\n",
      " 'BidenSummary: The inauguration of Joe Biden as the 46th president of the '\n",
      " 'United States took place on Wednesday, January 20, 2021, marking the start '\n",
      " 'of the four-year term of Joe Biden as president and Kamala Harris as vice '\n",
      " 'president. The 59th presidential inauguration took place on the West Front '\n",
      " 'of the United States Capitol in Washington, D.C. Biden took the presidential '\n",
      " 'oath of office, before which Harris took the vice presidential oath of '\n",
      " 'office.The inauguration took place amidst extraordinary political, public '\n",
      " 'health, economic, and national security crises, including the ongoing '\n",
      " \"COVID-19 pandemic; outgoing President Donald Trump's attempts to overturn \"\n",
      " 'the 2020 United States presidential election, which provoked an attack on '\n",
      " 'the United States Capitol on January 6; '\n",
      " \"Trump'\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with \"\n",
      " '`Delaware`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: DelawareSummary: Delaware (  '\n",
      " 'DEL-…ô-wair) is a state in the northeast and Mid-Atlantic regions of the '\n",
      " 'United States. It borders Maryland to its south and west, Pennsylvania to '\n",
      " 'its north, New Jersey to its northeast, and the Atlantic Ocean to its east. '\n",
      " \"The state's name derives from the adjacent Delaware Bay, which in turn was \"\n",
      " 'named after Thomas West, 3rd Baron De La Warr, an English nobleman and the '\n",
      " \"Colony of Virginia's first colonial-era governor.Delaware occupies the \"\n",
      " 'northeastern portion of the Delmarva Peninsula, and some islands and '\n",
      " 'territory within the Delaware River. It is the 2nd smallest and 6th least '\n",
      " \"populous state, but also the 6th most densely populated. Delaware's most \"\n",
      " \"populous city is Wilmington, and the state's capital is Dover, the 2nd most \"\n",
      " 'populous city in Delaware. The state is divided into three counties, the '\n",
      " 'fewest number of counties of any of the 50 U.S. states; from north to south, '\n",
      " 'the three counties are: New Castle County, Kent County, and Sussex '\n",
      " 'County.The southern two counties, Kent and Sussex counties, historically '\n",
      " 'have been predominantly agrarian economies. New Castle is more urbanized and '\n",
      " 'is considered part of the Delaware Valley metropolitan statistical area that '\n",
      " \"surrounds and includes Philadelphia, the nation's 6th most populous city. \"\n",
      " 'Delaware is considered part of the Southern United States by the U.S. Census '\n",
      " \"Bureau, but the state's geography, culture, and history are a hybrid of the \"\n",
      " 'Mid-Atlantic and Northeastern regions of the country.Before Delaware '\n",
      " 'coastline was explored and developed by Europeans in the 16th century, the '\n",
      " 'state was inhabited by several Native Americans tribes, including the Lenape '\n",
      " 'in the north and Nanticoke in the south. The state was first colonized by '\n",
      " 'Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in '\n",
      " '1631.Delaware was one of the Thirteen Colonies that participated in the '\n",
      " 'American Revolution and American Revolutionary War, in which the American '\n",
      " 'Continental Army, led by George Washington, defeated the British, ended '\n",
      " 'British colonization and establishing the United States as a sovereign and '\n",
      " 'independent nation.On December 7, 1787, Delaware was the first state to '\n",
      " 'ratify the Constitution of the United States, earning it the nickname \"The '\n",
      " 'First State\".Since the turn of the 20th century, Delaware has become an '\n",
      " 'onshore corporate haven whose corporate laws are deemed appealing to '\n",
      " 'corporations; over half of all New York Stock Exchange-listed corporations '\n",
      " 'and over three-fifths of the Fortune 500 is legally incorporated in the '\n",
      " 'state.Page: Delaware City, DelawareSummary: Delaware City is a city in New '\n",
      " 'Castle County, Delaware, United States. The population was 1,885 as of 2020. '\n",
      " 'It is a small port town on the eastern terminus of the Chesapeake and '\n",
      " 'Delaware Canal and is the location of the Forts Ferry Crossing to Fort '\n",
      " 'Delaware on Pea Patch Island.Page: Delaware RiverSummary: The Delaware River '\n",
      " 'is a major river in the Mid-Atlantic region of the United States and is the '\n",
      " 'longest free-flowing (undammed) river in the Eastern United States. From the '\n",
      " 'meeting of its branches in Hancock, New York, the river flows for 282 miles '\n",
      " '(454 km) along the borders of New York, Pennsylvania, New Jersey, and '\n",
      " 'Delaware, before emptying into Delaware Bay.The river has been recognized by '\n",
      " \"the National Wildlife Federation as one of the country's Great Waters and \"\n",
      " 'has been called the \"Lifeblood of the Northeast\" by American Rivers. Its '\n",
      " 'watershed drains an area of 13,539 square miles (35,070 km2) and provides '\n",
      " 'drinking water for 17 million people, including half of New York City via '\n",
      " 'the Delaware Aqueduct.The Delaware River has two branches that rise in the '\n",
      " 'Catskill Mountains of New York: the West Branch at Mount Jefferson in '\n",
      " 'Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware '\n",
      " 'County. The branches merge to form the main Delaware River at Hancock, New '\n",
      " 'York. Flowing south, the river remains relatively undeveloped, with 152 '\n",
      " 'miles (245 km) protected as the Upper, Middle, and Lower Delaware National '\n",
      " 'Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, '\n",
      " 'navigable, and significantly more industrial. This section forms the '\n",
      " 'backbone of the Delaware Valley metropolitan area, serving the port cities '\n",
      " 'of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river '\n",
      " 'flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the '\n",
      " \"bay's outlet to the Atlantic Ocean between Cape May and Cape Henlopen.Before \"\n",
      " 'the arrival of European settlers, the river was the homeland of the Lenape '\n",
      " 'native people. They called the river Lenapewihittuk, or Lenape River, and '\n",
      " 'Kithanne, meaning the largest river in this part of the country.In 1609, the '\n",
      " 'river was visited by a Dutch East India Company expedition led by Henry '\n",
      " 'Hudson. Hudson, an English navigator, was hired to find a western route to '\n",
      " 'Cathay (China), but his encounters set the stage for Dutch colonization of '\n",
      " 'North America in the 17th century. Early Dutch and Swedish settlements were '\n",
      " 'established along the lower section of the river and Delaware Bay. Both '\n",
      " 'colonial powers called the river the South River (Zuidrivier), compared to '\n",
      " 'the Hudson River, which was known as the North River. After the English '\n",
      " 'expelled the Dutch and took control of the New Netherland colony in 1664, '\n",
      " 'the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, '\n",
      " \"an English nobleman and the Virginia colony's first royal governor, who \"\n",
      " 'defended the colony during the First Anglo-Powhatan War.Page: University of '\n",
      " 'DelawareSummary: The University of Delaware (colloquially known as UD or '\n",
      " 'Delaware) is a privately governed, state-assisted land-grant research '\n",
      " 'university located in Newark, Delaware. UD is the largest university in '\n",
      " \"Delaware. It offers three associate's programs, 148 bachelor's programs, 121 \"\n",
      " \"master's programs (with 13 joint degrees), and 55 doctoral programs across \"\n",
      " 'its eight colleges. The main campus is in Newark, with satellite campuses in '\n",
      " 'Dover, Wilmington, Lewes, and Georgetown. It is considered a large '\n",
      " 'institution with approximately 18,200 undergraduate and 4,200 graduate '\n",
      " 'students. It is a privately governed university which receives public '\n",
      " 'funding for being a land-grant, sea-grant, and space-grant state-supported '\n",
      " 'research institution.UD is classified among \"R1: Doctoral Universities ‚Äì '\n",
      " 'Very high research activity\". According to the National Science Foundation, '\n",
      " 'UD spent $186 million on research and development in 2018, ranking it 119th '\n",
      " 'in the nation.  It is recognized with the Community Engagement '\n",
      " 'Classification by the Carnegie Foundation for the Advancement of Teaching.UD '\n",
      " 'students, alumni, and sports teams are known as the \"Fightin\\' Blue Hens\", '\n",
      " 'more commonly shortened to \"Blue Hens\", and the school colors are Delaware '\n",
      " \"blue and gold. UD sponsors 21 men's and women's NCAA Division-I sports teams \"\n",
      " 'and have competed in the Colonial Athletic Association (CAA) since '\n",
      " '2001.Page: LenapeSummary: The Lenape (English: , , ; Lenape languages: '\n",
      " '[l…ônaÀêpe]), also called the Lenni Lenape and Delaware people, are an '\n",
      " 'Indigenous people of the Northeastern Woodlands, who live in the United '\n",
      " \"States and Canada.The Lenape's historical territory includes present-day \"\n",
      " 'northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions '\n",
      " 'of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, '\n",
      " 'western Long Island, and the lower Hudson Valley in New York state. Today '\n",
      " 'they are based in Oklahoma, Wisconsin, and Ontario.During the last decades '\n",
      " 'of the 18th century, European settlers and the effects of the American '\n",
      " 'Revolutionary War displaced',\n",
      " 'most Lenape from their homelands and pushed them north and west. In the '\n",
      " '1860s, under the Indian removal policy, the U.S. federal government '\n",
      " 'relocated most Lenape remaining in the Eastern United States to the Indian '\n",
      " 'Territory and surrounding regions. Lenape people currently belong to the '\n",
      " 'Delaware Nation and Delaware Tribe of Indians in Oklahoma, the '\n",
      " 'Stockbridge‚ÄìMunsee Community in Wisconsin, and the Munsee-Delaware Nation, '\n",
      " 'Moravian of the Thames First Nation, and Delaware of Six Nations in '\n",
      " 'Ontario.\\x1b[0m---------------------------------------------------------------------------``````outputBadRequestError                           '\n",
      " 'Traceback (most recent call last)``````outputCell In[11], line 14      1 '\n",
      " 'agent = (      2     {      3         \"input\": itemgetter(\"input\"),   '\n",
      " '(...)     10     | OpenAIFunctionsAgentOutputParser()     11 )     13 '\n",
      " 'agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)---> '\n",
      " '14 agent_executor.invoke(     15     {     16         \"input\": \"Who is the '\n",
      " \"current US president? What's their home state? What's their home state's \"\n",
      " 'bird? What\\'s that bird\\'s scientific name?\"     17     }     18 '\n",
      " ')``````outputFile ~/langchain/libs/langchain/langchain/chains/base.py:162, '\n",
      " 'in Chain.invoke(self, input, config, **kwargs)    160 except BaseException '\n",
      " 'as e:    161     run_manager.on_chain_error(e)--> 162     raise e    163 '\n",
      " 'run_manager.on_chain_end(outputs)    164 final_outputs: Dict[str, Any] = '\n",
      " 'self.prep_outputs(    165     inputs, outputs, return_only_outputs    166 '\n",
      " ')``````outputFile ~/langchain/libs/langchain/langchain/chains/base.py:156, '\n",
      " 'in Chain.invoke(self, input, config, **kwargs)    149 run_manager = '\n",
      " 'callback_manager.on_chain_start(    150     dumpd(self),    151     '\n",
      " 'inputs,    152     name=run_name,    153 )    154 try:    155     outputs = '\n",
      " '(--> 156         self._call(inputs, run_manager=run_manager)    157         '\n",
      " 'if new_arg_supported    158         else self._call(inputs)    159     )    '\n",
      " '160 except BaseException as e:    161     '\n",
      " 'run_manager.on_chain_error(e)``````outputFile '\n",
      " '~/langchain/libs/langchain/langchain/agents/agent.py:1391, in '\n",
      " 'AgentExecutor._call(self, inputs, run_manager)   1389 # We now enter the '\n",
      " 'agent loop (until it returns something).   1390 while '\n",
      " 'self._should_continue(iterations, time_elapsed):-> 1391     next_step_output '\n",
      " '= self._take_next_step(   1392         name_to_tool_map,   1393         '\n",
      " 'color_mapping,   1394         inputs,   1395         intermediate_steps,   '\n",
      " '1396         run_manager=run_manager,   1397     )   1398     if '\n",
      " 'isinstance(next_step_output, AgentFinish):   1399         return '\n",
      " 'self._return(   1400             next_step_output, intermediate_steps, '\n",
      " 'run_manager=run_manager   1401         )``````outputFile '\n",
      " '~/langchain/libs/langchain/langchain/agents/agent.py:1097, in '\n",
      " 'AgentExecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, '\n",
      " 'intermediate_steps, run_manager)   1088 def _take_next_step(   1089     '\n",
      " 'self,   1090     name_to_tool_map: Dict[str, BaseTool],   (...)   1094     '\n",
      " 'run_manager: Optional[CallbackManagerForChainRun] = None,   1095 ) -> '\n",
      " 'Union[AgentFinish, List[Tuple[AgentAction, str]]]:   1096     return '\n",
      " 'self._consume_next_step(-> 1097         [   1098             a   '\n",
      " '1099             for a in self._iter_next_step(   1100                 '\n",
      " 'name_to_tool_map,   1101                 color_mapping,   '\n",
      " '1102                 inputs,   1103                 intermediate_steps,   '\n",
      " '1104                 run_manager,   1105             )   1106         ]   '\n",
      " '1107     )``````outputFile '\n",
      " '~/langchain/libs/langchain/langchain/agents/agent.py:1097, in '\n",
      " '<listcomp>(.0)   1088 def _take_next_step(   1089     self,   1090     '\n",
      " 'name_to_tool_map: Dict[str, BaseTool],   (...)   1094     run_manager: '\n",
      " 'Optional[CallbackManagerForChainRun] = None,   1095 ) -> Union[AgentFinish, '\n",
      " 'List[Tuple[AgentAction, str]]]:   1096     return self._consume_next_step(-> '\n",
      " '1097         [   1098             a   1099             for a in '\n",
      " 'self._iter_next_step(   1100                 name_to_tool_map,   '\n",
      " '1101                 color_mapping,   1102                 inputs,   '\n",
      " '1103                 intermediate_steps,   1104                 '\n",
      " 'run_manager,   1105             )   1106         ]   1107     '\n",
      " ')``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1125, '\n",
      " 'in AgentExecutor._iter_next_step(self, name_to_tool_map, color_mapping, '\n",
      " 'inputs, intermediate_steps, run_manager)   1122     intermediate_steps = '\n",
      " 'self._prepare_intermediate_steps(intermediate_steps)   1124     # Call the '\n",
      " 'LLM to see what to do.-> 1125     output = self.agent.plan(   1126         '\n",
      " 'intermediate_steps,   1127         callbacks=run_manager.get_child() if '\n",
      " 'run_manager else None,   1128         **inputs,   1129     )   1130 except '\n",
      " 'OutputParserException as e:   1131     if '\n",
      " 'isinstance(self.handle_parsing_errors, bool):``````outputFile '\n",
      " '~/langchain/libs/langchain/langchain/agents/agent.py:387, in '\n",
      " 'RunnableAgent.plan(self,',\n",
      " 'intermediate_steps, callbacks, **kwargs)    381 # Use streaming to make sure '\n",
      " 'that the underlying LLM is invoked in a streaming    382 # fashion to make '\n",
      " 'it possible to get access to the individual LLM tokens    383 # when using '\n",
      " 'stream_log with the Agent Executor.    384 # Because the response from the '\n",
      " 'plan is not a generator, we need to    385 # accumulate the output into '\n",
      " 'final output and return that.    386 final_output: Any = None--> 387 for '\n",
      " 'chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):    '\n",
      " '388     if final_output is None:    389         final_output = '\n",
      " 'chunk``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:2424, in '\n",
      " 'RunnableSequence.stream(self, input, config, **kwargs)   2418 def stream(   '\n",
      " '2419     self,   2420     input: Input,   2421     config: '\n",
      " 'Optional[RunnableConfig] = None,   2422     **kwargs: Optional[Any],   2423 '\n",
      " ') -> Iterator[Output]:-> 2424     yield from self.transform(iter([input]), '\n",
      " 'config, **kwargs)``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:2411, in '\n",
      " 'RunnableSequence.transform(self, input, config, **kwargs)   2405 def '\n",
      " 'transform(   2406     self,   2407     input: Iterator[Input],   2408     '\n",
      " 'config: Optional[RunnableConfig] = None,   2409     **kwargs: '\n",
      " 'Optional[Any],   2410 ) -> Iterator[Output]:-> 2411     yield from '\n",
      " 'self._transform_stream_with_config(   2412         input,   2413         '\n",
      " 'self._transform,   2414         patch_config(config, run_name=(config or '\n",
      " '{}).get(\"run_name\") or self.name),   2415         **kwargs,   2416     '\n",
      " ')``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:1497, in '\n",
      " 'Runnable._transform_stream_with_config(self, input, transformer, config, '\n",
      " 'run_type, **kwargs)   1495 try:   1496     while True:-> 1497         chunk: '\n",
      " 'Output = context.run(next, iterator)  # type: ignore   1498         yield '\n",
      " 'chunk   1499         if final_output_supported:``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:2375, in '\n",
      " 'RunnableSequence._transform(self, input, run_manager, config)   2366 for '\n",
      " 'step in steps:   2367     final_pipeline = step.transform(   2368         '\n",
      " 'final_pipeline,   2369         patch_config(   (...)   2372         ),   '\n",
      " '2373     )-> 2375 for output in final_pipeline:   2376     yield '\n",
      " 'output``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:1035, in '\n",
      " 'Runnable.transform(self, input, config, **kwargs)   1032 final: Input   1033 '\n",
      " 'got_first_val = False-> 1035 for chunk in input:   1036     if not '\n",
      " 'got_first_val:   1037         final = chunk``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:3991, in '\n",
      " 'RunnableBindingBase.transform(self, input, config, **kwargs)   3985 def '\n",
      " 'transform(   3986     self,   3987     input: Iterator[Input],   3988     '\n",
      " 'config: Optional[RunnableConfig] = None,   3989     **kwargs: Any,   3990 ) '\n",
      " '-> Iterator[Output]:-> 3991     yield from self.bound.transform(   '\n",
      " '3992         input,   3993         self._merge_configs(config),   '\n",
      " '3994         **{**self.kwargs, **kwargs},   3995     )``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/runnables/base.py:1045, in '\n",
      " 'Runnable.transform(self, input, config, **kwargs)   1042         final = '\n",
      " 'final + chunk  # type: ignore[operator]   1044 if got_first_val:-> 1045     '\n",
      " 'yield from self.stream(final, config, **kwargs)``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/language_models/chat_models.py:249, in '\n",
      " 'BaseChatModel.stream(self, input, config, stop, **kwargs)    242 except '\n",
      " 'BaseException as e:    243     run_manager.on_llm_error(    244         '\n",
      " 'e,    245         response=LLMResult(    246             '\n",
      " 'generations=[[generation]] if generation else []    247         ),    '\n",
      " '248     )--> 249     raise e    250 else:    251     '\n",
      " 'run_manager.on_llm_end(LLMResult(generations=[[generation]]))``````outputFile '\n",
      " '~/langchain/libs/core/langchain_core/language_models/chat_models.py:233, in '\n",
      " 'BaseChatModel.stream(self, input, config, stop, **kwargs)    231 generation: '\n",
      " 'Optional[ChatGenerationChunk] = None    232 try:--> 233     for chunk in '\n",
      " 'self._stream(    234         messages, stop=stop, run_manager=run_manager, '\n",
      " '**kwargs    235     ):    236         yield chunk.message    237         if '\n",
      " 'generation is None:``````outputFile '\n",
      " '~/langchain/libs/partners/openai/langchain_openai/chat_models/base.py:403, '\n",
      " 'in ChatOpenAI._stream(self, messages, stop, run_manager, **kwargs)    400 '\n",
      " 'params = {**params, **kwargs, \"stream\": True}    402 default_chunk_class = '\n",
      " 'AIMessageChunk--> 403 for chunk in '\n",
      " 'self.client.create(messages=message_dicts, **params):    404     if not '\n",
      " 'isinstance(chunk, dict):    405         chunk = chunk.dict()``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/_utils/_utils.py:271, '\n",
      " 'in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)    '\n",
      " '269             msg = f\"Missing required argument: {quote(missing[0])}\"    '\n",
      " '270     raise TypeError(msg)--> 271 return func(*args, '\n",
      " '**kwargs)``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/resources/chat/completions.py:648, '\n",
      " 'in Completions.create(self, messages, model, frequency_penalty, '\n",
      " 'function_call, functions, logit_bias, logprobs, max_tokens, n, '\n",
      " 'presence_penalty, response_format, seed, stop, stream, temperature, '\n",
      " 'tool_choice, tools, top_logprobs, top_p, user, extra_headers,',\n",
      " 'extra_query, extra_body, timeout)    599 @required_args([\"messages\", '\n",
      " '\"model\"], [\"messages\", \"model\", \"stream\"])    600 def create(    601     '\n",
      " 'self,   (...)    646     timeout: float | httpx.Timeout | None | NotGiven = '\n",
      " 'NOT_GIVEN,    647 ) -> ChatCompletion | Stream[ChatCompletionChunk]:--> '\n",
      " '648     return self._post(    649         \"/chat/completions\",    '\n",
      " '650         body=maybe_transform(    651             {    '\n",
      " '652                 \"messages\": messages,    653                 \"model\": '\n",
      " 'model,    654                 \"frequency_penalty\": frequency_penalty,    '\n",
      " '655                 \"function_call\": function_call,    656                 '\n",
      " '\"functions\": functions,    657                 \"logit_bias\": logit_bias,    '\n",
      " '658                 \"logprobs\": logprobs,    659                 '\n",
      " '\"max_tokens\": max_tokens,    660                 \"n\": n,    '\n",
      " '661                 \"presence_penalty\": presence_penalty,    '\n",
      " '662                 \"response_format\": response_format,    '\n",
      " '663                 \"seed\": seed,    664                 \"stop\": stop,    '\n",
      " '665                 \"stream\": stream,    666                 \"temperature\": '\n",
      " 'temperature,    667                 \"tool_choice\": tool_choice,    '\n",
      " '668                 \"tools\": tools,    669                 \"top_logprobs\": '\n",
      " 'top_logprobs,    670                 \"top_p\": top_p,    671                 '\n",
      " '\"user\": user,    672             },    673             '\n",
      " 'completion_create_params.CompletionCreateParams,    674         ),    '\n",
      " '675         options=make_request_options(    676             '\n",
      " 'extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, '\n",
      " 'timeout=timeout    677         ),    678         cast_to=ChatCompletion,    '\n",
      " '679         stream=stream or False,    680         '\n",
      " 'stream_cls=Stream[ChatCompletionChunk],    681     )``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:1179, '\n",
      " 'in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, '\n",
      " 'stream_cls)   1165 def post(   1166     self,   1167     path: str,   '\n",
      " '(...)   1174     stream_cls: type[_StreamT] | None = None,   1175 ) -> '\n",
      " 'ResponseT | _StreamT:   1176     opts = FinalRequestOptions.construct(   '\n",
      " '1177         method=\"post\", url=path, json_data=body, '\n",
      " 'files=to_httpx_files(files), **options   1178     )-> 1179     return '\n",
      " 'cast(ResponseT, self.request(cast_to, opts, stream=stream, '\n",
      " 'stream_cls=stream_cls))``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:868, in '\n",
      " 'SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, '\n",
      " 'stream_cls)    859 def request(    860     self,    861     cast_to: '\n",
      " 'Type[ResponseT],   (...)    866     stream_cls: type[_StreamT] | None = '\n",
      " 'None,    867 ) -> ResponseT | _StreamT:--> 868     return self._request(    '\n",
      " '869         cast_to=cast_to,    870         options=options,    871         '\n",
      " 'stream=stream,    872         stream_cls=stream_cls,    873         '\n",
      " 'remaining_retries=remaining_retries,    874     )``````outputFile '\n",
      " '~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:959, in '\n",
      " 'SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, '\n",
      " 'stream_cls)    956         err.response.read()    958     '\n",
      " 'log.debug(\"Re-raising status error\")--> 959     raise '\n",
      " 'self._make_status_error_from_response(err.response) from None    961 return '\n",
      " 'self._process_response(    962     cast_to=cast_to,    963     '\n",
      " 'options=options,   (...)    966     stream_cls=stream_cls,    967 '\n",
      " \")``````outputBadRequestError: Error code: 400 - {'error': {'message': \"\n",
      " '\"This model\\'s maximum context length is 4097 tokens. However, your messages '\n",
      " 'resulted in 5487 tokens (5419 in the messages, 68 in the functions). Please '\n",
      " 'reduce the length of the messages or functions.\", \\'type\\': '\n",
      " \"'invalid_request_error', 'param': 'messages', 'code': \"\n",
      " \"'context_length_exceeded'}}tipLangSmith traceUnfortunately we run out of \"\n",
      " \"space in our model's context window before we the agent can get to the final \"\n",
      " \"answer. Now let's add some prompt handling logic. To keep things simple, if \"\n",
      " \"our messages have too many tokens we'll start dropping the earliest AI, \"\n",
      " 'Function message pairs (this is the model tool invocation message and the '\n",
      " 'subsequent tool output message) in the chat history.def '\n",
      " 'condense_prompt(prompt: ChatPromptValue) -> ChatPromptValue:    messages = '\n",
      " 'prompt.to_messages()    num_tokens = '\n",
      " 'llm.get_num_tokens_from_messages(messages)    ai_function_messages = '\n",
      " 'messages[2:]    while num_tokens > 4_000:        ai_function_messages = '\n",
      " 'ai_function_messages[2:]        num_tokens = '\n",
      " 'llm.get_num_tokens_from_messages(            messages[:2]',\n",
      " '+ ai_function_messages        )    messages = messages[:2] + '\n",
      " 'ai_function_messages    return ChatPromptValue(messages=messages)agent = '\n",
      " '(    {        \"input\": itemgetter(\"input\"),        \"agent_scratchpad\": '\n",
      " 'lambda x: format_to_openai_function_messages(            '\n",
      " 'x[\"intermediate_steps\"]        ),    }    | prompt    | condense_prompt    | '\n",
      " 'llm.bind_functions(tools)    | '\n",
      " 'OpenAIFunctionsAgentOutputParser())agent_executor = '\n",
      " 'AgentExecutor(agent=agent, tools=tools, '\n",
      " 'verbose=True)agent_executor.invoke(    {        \"input\": \"Who is the current '\n",
      " \"US president? What's their home state? What's their home state's bird? \"\n",
      " 'What\\'s that bird\\'s scientific name?\"    })\\x1b[1m> Entering new '\n",
      " 'AgentExecutor chain...\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with '\n",
      " '`List of presidents of the United States`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: '\n",
      " 'List of presidents of the United StatesSummary: The president of the United '\n",
      " 'States is the head of state and head of government of the United States, '\n",
      " 'indirectly elected to a four-year term via the Electoral College. The '\n",
      " 'officeholder leads the executive branch of the federal government and is the '\n",
      " 'commander-in-chief of the United States Armed Forces. Since the office was '\n",
      " 'established in 1789, 45 men have served in 46 presidencies. The first '\n",
      " 'president, George Washington, won a unanimous vote of the Electoral College. '\n",
      " 'Grover Cleveland served two non-consecutive terms and is therefore counted '\n",
      " 'as the 22nd and 24th president of the United States, giving rise to the '\n",
      " 'discrepancy between the number of presidencies and the number of individuals '\n",
      " 'who have served as president. The incumbent president is Joe Biden.The '\n",
      " 'presidency of William Henry Harrison, who died 31 days after taking office '\n",
      " 'in 1841, was the shortest in American history. Franklin D. Roosevelt served '\n",
      " 'the longest, over twelve years, before dying early in his fourth term in '\n",
      " '1945. He is the only U.S. president to have served more than two terms. '\n",
      " 'Since the ratification of the Twenty-second Amendment to the United States '\n",
      " 'Constitution in 1951, no person may be elected president more than twice, '\n",
      " 'and no one who has served more than two years of a term to which someone '\n",
      " 'else was elected may be elected more than once.Four presidents died in '\n",
      " 'office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. '\n",
      " 'Harding, and Franklin D. Roosevelt), four were assassinated (Abraham '\n",
      " 'Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one '\n",
      " 'resigned (Richard Nixon, facing impeachment and removal from office). John '\n",
      " 'Tyler was the first vice president to assume the presidency during a '\n",
      " 'presidential term, and set the precedent that a vice president who does so '\n",
      " 'becomes the fully functioning president with his presidency.Throughout most '\n",
      " 'of its history, American politics has been dominated by political parties. '\n",
      " 'The Constitution is silent on the issue of political parties, and at the '\n",
      " 'time it came into force in 1789, no organized parties existed. Soon after '\n",
      " 'the 1st Congress convened, political factions began rallying around dominant '\n",
      " 'Washington administration officials, such as Alexander Hamilton and Thomas '\n",
      " 'Jefferson. Concerned about the capacity of political parties to destroy the '\n",
      " 'fragile unity holding the nation together, Washington remained unaffiliated '\n",
      " 'with any political faction or party throughout his eight-year presidency. He '\n",
      " 'was, and remains, the only U.S. president never affiliated with a political '\n",
      " 'party.Page: List of presidents of the United States by ageSummary: In this '\n",
      " 'list of presidents of the United States by age, the first table charts the '\n",
      " 'age of each president of the United States at the time of presidential '\n",
      " 'inauguration (first inauguration if elected to multiple and consecutive '\n",
      " 'terms), upon leaving office, and at the time of death. Where the president '\n",
      " 'is still living, their lifespan and post-presidency timespan are calculated '\n",
      " 'up to January 25, 2024.Page: List of vice presidents of the United '\n",
      " 'StatesSummary: There have been 49 vice presidents of the United States since '\n",
      " 'the office was created in 1789. Originally, the vice president was the '\n",
      " 'person who received the second-most votes for president in the Electoral '\n",
      " 'College. But after the election of 1800 produced a tie between Thomas '\n",
      " 'Jefferson and Aaron Burr, requiring the House of Representatives to choose '\n",
      " 'between them, lawmakers acted to prevent such a situation from recurring. '\n",
      " 'The Twelfth Amendment was added to the Constitution in 1804, creating the '\n",
      " 'current system where electors cast a separate ballot for the vice '\n",
      " 'presidency.The vice president is the first person in the presidential line '\n",
      " 'of succession‚Äîthat is, they assume the presidency if the president dies, '\n",
      " 'resigns, or is impeached and removed from office. Nine vice presidents have '\n",
      " 'ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, '\n",
      " 'Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, '\n",
      " \"Harry S. Truman, and Lyndon B. Johnson) through the president's death and \"\n",
      " \"one (Gerald Ford) through the president's resignation. The vice president \"\n",
      " 'also serves as the president of the Senate and may choose to cast a '\n",
      " 'tie-breaking vote on decisions made by the Senate. Vice presidents have '\n",
      " 'exercised this latter power to varying extents over the years.Before '\n",
      " 'adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the '\n",
      " 'office of the vice president could not be filled until the next '\n",
      " 'post-election inauguration. Several such vacancies occurred: seven vice '\n",
      " 'presidents died, one resigned and eight succeeded to the presidency. This '\n",
      " 'amendment allowed for a vacancy to be filled through appointment by the '\n",
      " 'president and confirmation by both chambers of the Congress. Since its '\n",
      " 'ratification, the vice presidency has been vacant twice (both in the context '\n",
      " 'of scandals surrounding the Nixon administration) and was filled both times '\n",
      " \"through this process, namely in 1973 following Spiro Agnew's resignation, \"\n",
      " 'and again in 1974 after Gerald Ford succeeded to the presidency. The '\n",
      " 'amendment also established a procedure whereby a vice president may, if the '\n",
      " 'president is unable to discharge the powers and duties of the office, '\n",
      " 'temporarily assume the powers and duties of the office as acting president. '\n",
      " 'Three vice presidents have briefly acted as president under the 25th '\n",
      " 'Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, '\n",
      " 'and on July 21, 2007; and Kamala Harris on November 19, 2021.The persons who '\n",
      " 'have served as vice president were born in or primarily affiliated with 27 '\n",
      " 'states plus the District of Columbia. New York has produced the most of any '\n",
      " 'state as eight have been born there and three others considered it their '\n",
      " 'home state. Most vice presidents have been in their 50s or 60s and had '\n",
      " 'political experience before assuming the office. Two vice presidents‚ÄîGeorge '\n",
      " 'Clinton and John C. Calhoun‚Äîserved under more than one president. Ill with '\n",
      " 'tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. '\n",
      " 'King, by an Act of Congress, was allowed to take the oath outside the United '\n",
      " 'States. He is the only vice president to take his oath of office in a '\n",
      " 'foreign country.Page: List of presidents of the United States by net '\n",
      " 'worthSummary: The list of presidents of the United States by net worth at '\n",
      " \"peak varies greatly. Debt and depreciation often means that presidents' net \"\n",
      " 'worth is less than $0 at the time of death. Most presidents before 1845 were '\n",
      " 'extremely wealthy, especially Andrew Jackson and George Washington.    '\n",
      " 'Presidents since 1929, when Herbert Hoover took office, have generally been '\n",
      " 'wealthier than presidents of the late nineteenth and early twentieth '\n",
      " 'centuries; with the exception of Harry S. Truman, all presidents since this '\n",
      " 'time have been millionaires. These presidents have often received income '\n",
      " 'from autobiographies and other writing. Except for Franklin D. Roosevelt and '\n",
      " 'John F. Kennedy (both of whom died while in office), all presidents '\n",
      " 'beginning with Calvin Coolidge have written autobiographies. In addition, '\n",
      " 'many presidents‚Äîincluding Bill Clinton‚Äîhave earned considerable income from '\n",
      " 'public speaking after leaving office.The richest president in history may be '\n",
      " 'Donald Trump. However, his net worth is not precisely known because the '\n",
      " 'Trump Organization is privately held.Truman was among the poorest U.S. '\n",
      " 'presidents, with a net worth considerably less than $1 million. His '\n",
      " 'financial situation contributed to the doubling of the presidential salary '\n",
      " 'to $100,000 in 1949. In addition, the presidential pension was created in '\n",
      " '1958 when Truman was again experiencing financial difficulties. Harry and '\n",
      " 'Bess Truman received the first Medicare cards in 1966 via the Social '\n",
      " 'Security Act of 1965.Page: List of presidents of the United States by home '\n",
      " 'stateSummary: These lists give the states of primary affiliation and of '\n",
      " 'birth for each president of the United '\n",
      " 'States.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Joe '\n",
      " 'Biden`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Joe BidenSummary: Joseph Robinette '\n",
      " 'Biden Jr. (  BY-d…ôn; born November 20, 1942) is an American politician who '\n",
      " 'is the 46th and current president of the United States. A member of the '\n",
      " 'Democratic Party, he previously served as the 47th vice president from 2009 '\n",
      " 'to 2017 under President Barack Obama and represented Delaware in the United '\n",
      " 'States Senate from 1973 to 2009.Born in Scranton, Pennsylvania, Biden moved '\n",
      " 'with his family to Delaware in 1953. He graduated from the',\n",
      " 'University of Delaware before earning his law degree from Syracuse '\n",
      " 'University. He was elected to the New Castle County Council in 1970 and to '\n",
      " 'the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to '\n",
      " 'pass the Violent Crime Control and Law Enforcement Act and the Violence '\n",
      " 'Against Women Act. He also oversaw six U.S. Supreme Court confirmation '\n",
      " 'hearings, including the contentious hearings for Robert Bork and Clarence '\n",
      " 'Thomas. Biden ran unsuccessfully for the Democratic presidential nomination '\n",
      " 'in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was '\n",
      " 'a close counselor to Obama during his two terms as vice president. In the '\n",
      " '2020 presidential election, Biden and his running mate, Kamala Harris, '\n",
      " 'defeated incumbents Donald Trump and Mike Pence. He became the oldest '\n",
      " 'president in U.S. history, and the first to have a female vice president.As '\n",
      " 'president, Biden signed the American Rescue Plan Act in response to the '\n",
      " 'COVID-19 pandemic and subsequent recession. He signed bipartisan bills on '\n",
      " 'infrastructure and manufacturing. He proposed the Build Back Better Act, '\n",
      " 'which failed in Congress, but aspects of which were incorporated into the '\n",
      " 'Inflation Reduction Act that he signed into law in 2022. Biden appointed '\n",
      " 'Ketanji Brown Jackson to the Supreme Court. He worked with congressional '\n",
      " 'Republicans to resolve the 2023 United States debt-ceiling crisis by '\n",
      " 'negotiating a deal to raise the debt ceiling. In foreign policy, Biden '\n",
      " \"restored America's membership in the Paris Agreement. He oversaw the \"\n",
      " 'complete withdrawal of U.S. troops from Afghanistan that ended the war in '\n",
      " 'Afghanistan, during which the Afghan government collapsed and the Taliban '\n",
      " 'seized control. He responded to the Russian invasion of Ukraine by imposing '\n",
      " 'sanctions on Russia and authorizing civilian and military aid to Ukraine. '\n",
      " 'During the Israel‚ÄìHamas war, Biden announced military support for Israel, '\n",
      " 'and condemned the actions of Hamas and other Palestinian militants as '\n",
      " 'terrorism. In April 2023, Biden announced his candidacy for the Democratic '\n",
      " 'nomination in the 2024 presidential election.Page: Presidency of Joe '\n",
      " \"BidenSummary: Joe Biden's tenure as the 46th president of the United States \"\n",
      " 'began with his inauguration on January 20, 2021. Biden, a Democrat from '\n",
      " 'Delaware who previously served as vice president for two terms under '\n",
      " 'president Barack Obama, took office following his victory in the 2020 '\n",
      " 'presidential election over Republican incumbent president Donald Trump. '\n",
      " 'Biden won the presidency with a popular vote of over 81 million, the highest '\n",
      " 'number of votes cast for a single United States presidential candidate. Upon '\n",
      " 'his inauguration, he became the oldest president in American history, '\n",
      " 'breaking the record set by his predecessor Trump. Biden entered office amid '\n",
      " 'the COVID-19 pandemic, an economic crisis, and increased political '\n",
      " 'polarization.On the first day of his presidency, Biden made an effort to '\n",
      " \"revert President Trump's energy policy by restoring U.S. participation in \"\n",
      " 'the Paris Agreement and revoking the permit for the Keystone XL pipeline. He '\n",
      " \"also halted funding for Trump's border wall, an expansion of the Mexican \"\n",
      " 'border wall. On his second day, he issued a series of executive orders to '\n",
      " 'reduce the impact of COVID-19, including invoking the Defense Production Act '\n",
      " 'of 1950, and set an early goal of achieving one hundred million COVID-19 '\n",
      " 'vaccinations in the United States in his first 100 days.Biden signed into '\n",
      " 'law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that '\n",
      " 'temporarily established expanded unemployment insurance and sent $1,400 '\n",
      " 'stimulus checks to most Americans in response to continued economic pressure '\n",
      " 'from COVID-19. He signed the bipartisan Infrastructure Investment and Jobs '\n",
      " 'Act; a ten-year plan brokered by Biden alongside Democrats and Republicans '\n",
      " 'in Congress, to invest in American roads, bridges, public transit, ports and '\n",
      " 'broadband access. Biden signed the Juneteenth National Independence Day Act, '\n",
      " 'making Juneteenth a federal holiday in the United States. He appointed '\n",
      " 'Ketanji Brown Jackson to the U.S. Supreme Court‚Äîthe first Black woman to '\n",
      " 'serve on the court. After The Supreme Court overturned Roe v. Wade, Biden '\n",
      " 'took executive actions, such as the signing of Executive Order 14076, to '\n",
      " \"preserve and protect women's health rights nationwide, against abortion bans \"\n",
      " 'in Republican led states. Biden proposed a significant expansion of the U.S. '\n",
      " 'social safety net through the Build Back Better Act, but those efforts, '\n",
      " 'along with voting rights legislation, failed in Congress. However, in August '\n",
      " '2022, Biden signed the Inflation Reduction Act of 2022, a domestic '\n",
      " 'appropriations bill that included some of the provisions of the Build Back '\n",
      " 'Better Act after the entire bill failed to pass. It included significant '\n",
      " 'federal investment in climate and domestic clean energy production, tax '\n",
      " 'credits for solar panels, electric cars and other home energy programs as '\n",
      " 'well as a three-year extension of Affordable Care Act subsidies. The '\n",
      " 'administration\\'s economic policies, known as \"Bidenomics\", were inspired '\n",
      " 'and designed by Trickle-up economics. Described as growing the economy from '\n",
      " 'the middle out and bottom up and growing the middle class. Biden signed the '\n",
      " 'CHIPS and Science Act, bolstering the semiconductor and manufacturing '\n",
      " 'industry, the Honoring our PACT Act, expanding health care for US veterans, '\n",
      " 'the Bipartisan Safer Communities Act and the Electoral Count Reform and '\n",
      " 'Presidential Transition Improvement Act. In late 2022, Biden signed the '\n",
      " 'Respect for Marriage Act, which repealed the Defense of Marriage Act and '\n",
      " 'codified same-sex and interracial marriage in the United States. In response '\n",
      " 'to the debt-ceiling crisis of 2023, Biden negotiated and signed the Fiscal '\n",
      " 'Responsibility Act of 2023, which restrains federal spending for fiscal '\n",
      " 'years 2024 and 2025, implements minor changes to SNAP and TANF, includes '\n",
      " 'energy permitting reform, claws back some IRS funding and unspent money for '\n",
      " 'COVID-19, and suspends the debt ceiling to January 1, 2025. Biden '\n",
      " 'established the American Climate Corps and created the first ever White '\n",
      " 'House Office of Gun Violence Prevention. On September 26, 2023, Joe Biden '\n",
      " 'visited a United Auto Workers picket line during the 2023 United Auto '\n",
      " 'Workers strike, making him the first US president to visit one.The foreign '\n",
      " 'policy goal of the Biden administration is to restore the US to a \"position '\n",
      " 'of trusted leadership\" among global democracies in order to address the '\n",
      " 'challenges posed by Russia and China. In foreign policy, Biden completed the '\n",
      " 'withdrawal of U.S. military forces from Afghanistan, declaring an end to '\n",
      " 'nation-building efforts and shifting U.S. foreign policy toward strategic '\n",
      " 'competition with China and, to a lesser extent, Russia. However, during the '\n",
      " 'withdrawal, the Afghan government collapsed and the Taliban seized control, '\n",
      " 'leading to Biden receiving bipartisan criticism. He responded to the Russian '\n",
      " 'invasion of Ukraine by imposing sanctions on Russia as well as providing '\n",
      " 'Ukraine with over $100 billion in combined military, economic, and '\n",
      " 'humanitarian aid. Biden also approved a raid which led to the death of Abu '\n",
      " 'Ibrahim al-Hashimi al-Qurashi, the leader of the Islamic State, and approved '\n",
      " 'a drone strike which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden '\n",
      " 'signed and created AUKUS, an international security alliance, together with '\n",
      " 'Australia and the United Kingdom. Biden called for the expansion of NATO '\n",
      " 'with the addition of Finland and Sweden, and rallied NATO allies in support '\n",
      " 'of Ukraine. During the 2023 Israel‚ÄìHamas war, Biden condemned Hamas and '\n",
      " 'other Palestinian militants as terrorism and announced American military '\n",
      " 'support for Israel; Biden also showed his support and sympathy towards '\n",
      " 'Palestinians affected by the war, sent humanitarian aid, and brokered a '\n",
      " 'four-day temporary pause and hostage exchange.Page: Family of Joe '\n",
      " 'BidenSummary: Joe Biden, the 46th and current president of the United '\n",
      " 'States, has family members who are prominent in law, education, activism and '\n",
      " \"politics. Biden's immediate family became the first family of the United \"\n",
      " 'States on his inauguration on January 20, 2021. His immediate family circle '\n",
      " 'was also the second family of the United States from 2009 to 2017, when '\n",
      " \"Biden was vice president. Biden's family is mostly descended from the \"\n",
      " 'British Isles, with most of their ancestors coming from Ireland and England, '\n",
      " \"and a smaller number descending from the French.Of Joe Biden's sixteen \"\n",
      " 'great-great-grandparents, ten were born in Ireland. He is descended from the '\n",
      " \"Blewitts of County Mayo and the Finnegans of County Louth. One of Biden's \"\n",
      " 'great-great-great-grandfathers was born in Sussex, England, and emigrated to '\n",
      " 'Maryland in the United States by 1820.Page: Inauguration of Joe '\n",
      " 'BidenSummary: The inauguration of Joe Biden as the 46th president of the '\n",
      " 'United States took place on Wednesday, January 20, 2021, marking the start '\n",
      " 'of the four-year term of Joe Biden as president and Kamala Harris as vice '\n",
      " 'president. The 59th presidential inauguration took place on the West Front '\n",
      " 'of the United States Capitol in Washington, D.C. Biden took the presidential '\n",
      " 'oath of office, before which Harris took the vice presidential oath of '\n",
      " 'office.The inauguration took place amidst extraordinary political, public '\n",
      " 'health, economic, and national security crises, including the ongoing '\n",
      " \"COVID-19 pandemic; outgoing President Donald Trump's attempts to overturn \"\n",
      " 'the 2020 United States presidential election, which provoked an attack on '\n",
      " 'the United States Capitol on January 6; '\n",
      " \"Trump'\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with \"\n",
      " '`Delaware`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: DelawareSummary: Delaware (  '\n",
      " 'DEL-…ô-wair) is a state in the northeast and Mid-Atlantic regions of the '\n",
      " 'United States. It borders Maryland to its south and west, Pennsylvania to '\n",
      " 'its north, New Jersey to its northeast, and the Atlantic Ocean to its east. '\n",
      " \"The state's name derives from the adjacent Delaware Bay, which in turn was \"\n",
      " 'named after Thomas West, 3rd Baron De La Warr, an English nobleman and the '\n",
      " \"Colony of Virginia's first colonial-era governor.Delaware occupies the \"\n",
      " 'northeastern portion of the',\n",
      " 'Delmarva Peninsula, and some islands and territory within the Delaware '\n",
      " 'River. It is the 2nd smallest and 6th least populous state, but also the 6th '\n",
      " \"most densely populated. Delaware's most populous city is Wilmington, and the \"\n",
      " \"state's capital is Dover, the 2nd most populous city in Delaware. The state \"\n",
      " 'is divided into three counties, the fewest number of counties of any of the '\n",
      " '50 U.S. states; from north to south, the three counties are: New Castle '\n",
      " 'County, Kent County, and Sussex County.The southern two counties, Kent and '\n",
      " 'Sussex counties, historically have been predominantly agrarian economies. '\n",
      " 'New Castle is more urbanized and is considered part of the Delaware Valley '\n",
      " 'metropolitan statistical area that surrounds and includes Philadelphia, the '\n",
      " \"nation's 6th most populous city. Delaware is considered part of the Southern \"\n",
      " \"United States by the U.S. Census Bureau, but the state's geography, culture, \"\n",
      " 'and history are a hybrid of the Mid-Atlantic and Northeastern regions of the '\n",
      " 'country.Before Delaware coastline was explored and developed by Europeans in '\n",
      " 'the 16th century, the state was inhabited by several Native Americans '\n",
      " 'tribes, including the Lenape in the north and Nanticoke in the south. The '\n",
      " 'state was first colonized by Dutch traders at Zwaanendael, near present-day '\n",
      " 'Lewes, Delaware, in 1631.Delaware was one of the Thirteen Colonies that '\n",
      " 'participated in the American Revolution and American Revolutionary War, in '\n",
      " 'which the American Continental Army, led by George Washington, defeated the '\n",
      " 'British, ended British colonization and establishing the United States as a '\n",
      " 'sovereign and independent nation.On December 7, 1787, Delaware was the first '\n",
      " 'state to ratify the Constitution of the United States, earning it the '\n",
      " 'nickname \"The First State\".Since the turn of the 20th century, Delaware has '\n",
      " 'become an onshore corporate haven whose corporate laws are deemed appealing '\n",
      " 'to corporations; over half of all New York Stock Exchange-listed '\n",
      " 'corporations and over three-fifths of the Fortune 500 is legally '\n",
      " 'incorporated in the state.Page: Delaware City, DelawareSummary: Delaware '\n",
      " 'City is a city in New Castle County, Delaware, United States. The population '\n",
      " 'was 1,885 as of 2020. It is a small port town on the eastern terminus of the '\n",
      " 'Chesapeake and Delaware Canal and is the location of the Forts Ferry '\n",
      " 'Crossing to Fort Delaware on Pea Patch Island.Page: Delaware RiverSummary: '\n",
      " 'The Delaware River is a major river in the Mid-Atlantic region of the United '\n",
      " 'States and is the longest free-flowing (undammed) river in the Eastern '\n",
      " 'United States. From the meeting of its branches in Hancock, New York, the '\n",
      " 'river flows for 282 miles (454 km) along the borders of New York, '\n",
      " 'Pennsylvania, New Jersey, and Delaware, before emptying into Delaware '\n",
      " 'Bay.The river has been recognized by the National Wildlife Federation as one '\n",
      " 'of the country\\'s Great Waters and has been called the \"Lifeblood of the '\n",
      " 'Northeast\" by American Rivers. Its watershed drains an area of 13,539 square '\n",
      " 'miles (35,070 km2) and provides drinking water for 17 million people, '\n",
      " 'including half of New York City via the Delaware Aqueduct.The Delaware River '\n",
      " 'has two branches that rise in the Catskill Mountains of New York: the West '\n",
      " 'Branch at Mount Jefferson in Jefferson, Schoharie County, and the East '\n",
      " 'Branch at Grand Gorge, Delaware County. The branches merge to form the main '\n",
      " 'Delaware River at Hancock, New York. Flowing south, the river remains '\n",
      " 'relatively undeveloped, with 152 miles (245 km) protected as the Upper, '\n",
      " 'Middle, and Lower Delaware National Scenic Rivers. At Trenton, New Jersey, '\n",
      " 'the Delaware becomes tidal, navigable, and significantly more industrial. '\n",
      " 'This section forms the backbone of the Delaware Valley metropolitan area, '\n",
      " 'serving the port cities of Philadelphia, Camden, New Jersey, and Wilmington, '\n",
      " 'Delaware. The river flows into Delaware Bay at Liston Point, 48 miles (77 '\n",
      " \"km) upstream of the bay's outlet to the Atlantic Ocean between Cape May and \"\n",
      " 'Cape Henlopen.Before the arrival of European settlers, the river was the '\n",
      " 'homeland of the Lenape native people. They called the river Lenapewihittuk, '\n",
      " 'or Lenape River, and Kithanne, meaning the largest river in this part of the '\n",
      " 'country.In 1609, the river was visited by a Dutch East India Company '\n",
      " 'expedition led by Henry Hudson. Hudson, an English navigator, was hired to '\n",
      " 'find a western route to Cathay (China), but his encounters set the stage for '\n",
      " 'Dutch colonization of North America in the 17th century. Early Dutch and '\n",
      " 'Swedish settlements were established along the lower section of the river '\n",
      " 'and Delaware Bay. Both colonial powers called the river the South River '\n",
      " '(Zuidrivier), compared to the Hudson River, which was known as the North '\n",
      " 'River. After the English expelled the Dutch and took control of the New '\n",
      " 'Netherland colony in 1664, the river was renamed Delaware after Sir Thomas '\n",
      " \"West, 3rd Baron De La Warr, an English nobleman and the Virginia colony's \"\n",
      " 'first royal governor, who defended the colony during the First '\n",
      " 'Anglo-Powhatan War.Page: University of DelawareSummary: The University of '\n",
      " 'Delaware (colloquially known as UD or Delaware) is a privately governed, '\n",
      " 'state-assisted land-grant research university located in Newark, Delaware. '\n",
      " \"UD is the largest university in Delaware. It offers three associate's \"\n",
      " \"programs, 148 bachelor's programs, 121 master's programs (with 13 joint \"\n",
      " 'degrees), and 55 doctoral programs across its eight colleges. The main '\n",
      " 'campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, '\n",
      " 'and Georgetown. It is considered a large institution with approximately '\n",
      " '18,200 undergraduate and 4,200 graduate students. It is a privately governed '\n",
      " 'university which receives public funding for being a land-grant, sea-grant, '\n",
      " 'and space-grant state-supported research institution.UD is classified among '\n",
      " '\"R1: Doctoral Universities ‚Äì Very high research activity\". According to the '\n",
      " 'National Science Foundation, UD spent $186 million on research and '\n",
      " 'development in 2018, ranking it 119th in the nation.  It is recognized with '\n",
      " 'the Community Engagement Classification by the Carnegie Foundation for the '\n",
      " 'Advancement of Teaching.UD students, alumni, and sports teams are known as '\n",
      " 'the \"Fightin\\' Blue Hens\", more commonly shortened to \"Blue Hens\", and the '\n",
      " \"school colors are Delaware blue and gold. UD sponsors 21 men's and women's \"\n",
      " 'NCAA Division-I sports teams and have competed in the Colonial Athletic '\n",
      " 'Association (CAA) since 2001.Page: LenapeSummary: The Lenape (English: , , ; '\n",
      " 'Lenape languages: [l…ônaÀêpe]), also called the Lenni Lenape and Delaware '\n",
      " 'people, are an Indigenous people of the Northeastern Woodlands, who live in '\n",
      " \"the United States and Canada.The Lenape's historical territory includes \"\n",
      " 'present-day northeastern Delaware, all of New Jersey, the eastern '\n",
      " 'Pennsylvania regions of the Lehigh Valley and Northeastern Pennsylvania, and '\n",
      " 'New York Bay, western Long Island, and the lower Hudson Valley in New York '\n",
      " 'state. Today they are based in Oklahoma, Wisconsin, and Ontario.During the '\n",
      " 'last decades of the 18th century, European settlers and the effects of the '\n",
      " 'American Revolutionary War displaced most Lenape from their homelands and '\n",
      " 'pushed them north and west. In the 1860s, under the Indian removal policy, '\n",
      " 'the U.S. federal government relocated most Lenape remaining in the Eastern '\n",
      " 'United States to the Indian Territory and surrounding regions. Lenape people '\n",
      " 'currently belong to the Delaware Nation and Delaware Tribe of Indians in '\n",
      " 'Oklahoma, the Stockbridge‚ÄìMunsee Community in Wisconsin, and the '\n",
      " 'Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of '\n",
      " 'Six Nations in Ontario.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with '\n",
      " '`Blue hen chicken`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Delaware Blue HenSummary: '\n",
      " 'The Delaware Blue Hen or Blue Hen of Delaware is a blue strain of American '\n",
      " 'gamecock. Under the name Blue Hen Chicken it is the official bird of the '\n",
      " 'State of Delaware. It is the emblem or mascot of several institutions in the '\n",
      " 'state, among them the sports teams of the University of Delaware.Page: '\n",
      " \"Delaware Fightin' Blue HensSummary: The Delaware Fightin' Blue Hens are the \"\n",
      " 'athletic teams of the University of Delaware (UD) of Newark, Delaware, in '\n",
      " 'the United States. The Blue Hens compete in the Football Championship '\n",
      " 'Subdivision (FCS) of Division I of the National Collegiate Athletic '\n",
      " 'Association (NCAA) as members of the Coastal Athletic Association and its '\n",
      " 'technically separate football league, CAA Football.On November 28, 2023, UD '\n",
      " 'and Conference USA (CUSA) jointly announced that UD would start a transition '\n",
      " 'to the Division I Football Bowl Subdivision (FBS) in 2024 and join CUSA in '\n",
      " '2025. UD will continue to compete in both sides of the CAA in 2024‚Äì25; it '\n",
      " 'will be ineligible for the FCS playoffs due to NCAA rules for transitioning '\n",
      " 'programs, but will be eligible for all non-football CAA championships. Upon '\n",
      " 'joining CUSA, UD will be eligible for all conference championship events '\n",
      " 'except the football championship game; it will become eligible for that '\n",
      " 'event upon completing the FBS transition in 2026. At the same time, UD also '\n",
      " \"announced it would add one women's sport due to Title IX considerations, and \"\n",
      " 'would also be seeking conference homes for the seven sports that UD sponsors '\n",
      " \"but CUSA does not. The new women's sport would later be announced as ice \"\n",
      " 'hockey; UD will join College Hockey America for its first season of varsity '\n",
      " 'play in 2025‚Äì26.Page: Brahma chickenSummary: The Brahma is an American breed '\n",
      " 'of chicken. It was bred in the United States from birds imported from',\n",
      " 'the Chinese port of Shanghai,:\\u200a78\\u200a and was the principal American '\n",
      " 'meat breed from the 1850s until about 1930.Page: SilkieSummary: The Silkie '\n",
      " '(also known as the Silky or Chinese silk chicken) is a breed of chicken '\n",
      " 'named for its atypically fluffy plumage, which is said to feel like silk and '\n",
      " 'satin. The breed has several other unusual qualities, such as black skin and '\n",
      " 'bones, blue earlobes, and five toes on each foot, whereas most chickens have '\n",
      " 'only four. They are often exhibited in poultry shows, and also appear in '\n",
      " 'various colors. In addition to their distinctive physical characteristics, '\n",
      " 'Silkies are well known for their calm and friendly temperament. It is among '\n",
      " 'the most docile of poultry. Hens are also exceptionally broody, and care for '\n",
      " 'young well. Although they are fair layers themselves, laying only about '\n",
      " 'three eggs a week, they are commonly used to hatch eggs from other breeds '\n",
      " 'and bird species due to their broody nature. Silkie chickens have been bred '\n",
      " 'to have a wide variety of colors which include but are not limited to: '\n",
      " 'Black, Blue, Buff, Partridge, Splash, White, Lavender, Paint and '\n",
      " 'Porcelain.Page: Silverudd BlueSummary: The Silverudd Blue, Swedish: '\n",
      " 'Silverudds Bl√•, is a Swedish breed of chicken. It was developed by Martin '\n",
      " 'Silverudd in Sm√•land, in southern Sweden. Hens lay blue/green eggs, weighing '\n",
      " '50‚Äì65 grams. The flock-book for the breed is kept by the Svenska '\n",
      " 'Kulturh√∂nsf√∂reningen ‚Äì the Swedish Cultural Hen Association. It was '\n",
      " 'initially known by various names including Isbar, Blue Isbar and Svensk '\n",
      " 'Gr√∂nv√§rpare, or \"Swedish green egg layer\"; in 2016 it was renamed to '\n",
      " \"'Silverudd Blue' after its creator.\\x1b[0m\\x1b[32;1m\\x1b[1;3mThe current US \"\n",
      " 'president is Joe Biden. His home state is Delaware. The home state bird of '\n",
      " 'Delaware is the Delaware Blue Hen. The scientific name of the Delaware Blue '\n",
      " 'Hen is Gallus gallus domesticus.\\x1b[0m\\x1b[1m> Finished '\n",
      " 'chain.\\x1b[0m{\\'input\\': \"Who is the current US president? What\\'s their '\n",
      " \"home state? What's their home state's bird? What's that bird's scientific \"\n",
      " 'name?\", \\'output\\': \\'The current US president is Joe Biden. His home state '\n",
      " 'is Delaware. The home state bird of Delaware is the Delaware Blue Hen. The '\n",
      " 'scientific name of the Delaware Blue Hen is Gallus gallus '\n",
      " \"domesticus.'}tipLangSmith traceHelp us out by providing feedback on this \"\n",
      " 'documentation page:PreviousCreate a runnable with the @chain '\n",
      " 'decoratorNextMultiple '\n",
      " 'chainsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.',\n",
      " '----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Multiple chains | ü¶úÔ∏èüîó LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreRoute logic based on inputInspect your runnablesCreate a '\n",
      " 'runnable with the @chain decoratorManaging prompt sizeMultiple '\n",
      " 'chainsEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityExpression '\n",
      " 'LanguageMoreMultiple chainsOn this pageMultiple chainsRunnables can easily '\n",
      " 'be used to string together multiple Chains%pip install --upgrade --quiet  '\n",
      " 'langchain langchain-openaifrom operator import itemgetterfrom '\n",
      " 'langchain_core.output_parsers import StrOutputParserfrom '\n",
      " 'langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import '\n",
      " 'ChatOpenAIprompt1 = ChatPromptTemplate.from_template(\"what is the city '\n",
      " '{person} is from?\")prompt2 = ChatPromptTemplate.from_template(    \"what '\n",
      " 'country is the city {city} in? respond in {language}\")model = '\n",
      " 'ChatOpenAI()chain1 = prompt1 | model | StrOutputParser()chain2 = (    '\n",
      " '{\"city\": chain1, \"language\": itemgetter(\"language\")}    | prompt2    | '\n",
      " 'model    | StrOutputParser())chain2.invoke({\"person\": \"obama\", \"language\": '\n",
      " '\"spanish\"})\\'El pa√≠s donde se encuentra la ciudad de Honolulu, donde naci√≥ '\n",
      " 'Barack Obama, el 44¬∫ Presidente de los Estados Unidos, es Estados Unidos. '\n",
      " \"Honolulu se encuentra en la isla de Oahu, en el estado de Haw√°i.'from \"\n",
      " 'langchain_core.runnables import RunnablePassthroughprompt1 = '\n",
      " 'ChatPromptTemplate.from_template(    \"generate a {attribute} color. Return '\n",
      " 'the name of the color and nothing else:\")prompt2 = '\n",
      " 'ChatPromptTemplate.from_template(    \"what is a fruit of color: {color}. '\n",
      " 'Return the name of the fruit and nothing else:\")prompt3 = '\n",
      " 'ChatPromptTemplate.from_template(    \"what is a country with a flag that has '\n",
      " 'the color: {color}. Return the name of the country and nothing '\n",
      " 'else:\")prompt4 = ChatPromptTemplate.from_template(    \"What is the color of '\n",
      " '{fruit} and the flag of {country}?\")model_parser = model | '\n",
      " 'StrOutputParser()color_generator = (    {\"attribute\": RunnablePassthrough()} '\n",
      " '| prompt1 | {\"color\": model_parser})color_to_fruit = prompt2 | '\n",
      " 'model_parsercolor_to_country = prompt3 | model_parserquestion_generator = '\n",
      " '(    color_generator | {\"fruit\": color_to_fruit, \"country\": '\n",
      " 'color_to_country} | '\n",
      " 'prompt4)question_generator.invoke(\"warm\")ChatPromptValue(messages=[HumanMessage(content=\\'What '\n",
      " \"is the color of strawberry and the flag of China?', additional_kwargs={}, \"\n",
      " 'example=False)])prompt = '\n",
      " 'question_generator.invoke(\"warm\")model.invoke(prompt)AIMessage(content=\\'The '\n",
      " 'color of an apple is typically red or green. The flag of China is '\n",
      " 'predominantly red with a large yellow star in the upper left corner and four '\n",
      " \"smaller yellow stars surrounding it.', additional_kwargs={}, \"\n",
      " 'example=False)Branching and Merging\\u200bYou may want the output of one '\n",
      " 'component to be processed by 2 or more other components. RunnableParallels '\n",
      " 'let you split or fork the chain so multiple components can process the input '\n",
      " 'in parallel. Later, other components can join or merge the results to '\n",
      " 'synthesize a final response. This type of chain creates a computation graph '\n",
      " 'that looks like the following:     Input      / \\\\     /   \\\\ Branch1 '\n",
      " 'Branch2     \\\\   /      \\\\ /      Combineplanner = (    '\n",
      " 'ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")    | '\n",
      " 'ChatOpenAI()    | StrOutputParser()    | {\"base_response\": '\n",
      " 'RunnablePassthrough()})arguments_for = (    '\n",
      " 'ChatPromptTemplate.from_template(        \"List the pros or positive aspects '\n",
      " 'of {base_response}\"    )    | ChatOpenAI()    | '\n",
      " 'StrOutputParser())arguments_against = (    '\n",
      " 'ChatPromptTemplate.from_template(        \"List the cons or negative aspects '\n",
      " 'of {base_response}\"    )    | ChatOpenAI()    | '\n",
      " 'StrOutputParser())final_responder = (    '\n",
      " 'ChatPromptTemplate.from_messages(        [            (\"ai\", '\n",
      " '\"{original_response}\"),            (\"human\", '\n",
      " '\"Pros:\\\\n{results_1}\\\\n\\\\nCons:\\\\n{results_2}\"),            (\"system\", '\n",
      " '\"Generate a final response given the critique\"),        ]    )    | '\n",
      " 'ChatOpenAI()    | StrOutputParser())chain = (    planner    | {        '\n",
      " '\"results_1\": arguments_for,        \"results_2\": arguments_against,        '\n",
      " '\"original_response\": itemgetter(\"base_response\"),    }    | '\n",
      " 'final_responder)chain.invoke({\"input\": \"scrum\"})\\'While Scrum has its '\n",
      " 'potential cons and challenges, many organizations have successfully embraced '\n",
      " 'and implemented this project management framework to great effect. The cons '\n",
      " 'mentioned above can be mitigated or overcome with proper training, support, '\n",
      " 'and a commitment to continuous improvement. It is also important to note '\n",
      " 'that not all cons may be applicable to every organization or '\n",
      " 'project.\\\\n\\\\nFor example, while Scrum may be complex initially, with proper '\n",
      " 'training and guidance, teams can quickly grasp the concepts and practices. '\n",
      " 'The lack of predictability can be mitigated by implementing techniques such '\n",
      " 'as velocity tracking and release planning. The limited documentation can be '\n",
      " 'addressed by maintaining a balance between lightweight documentation and '\n",
      " 'clear communication among team members. The dependency on team collaboration '\n",
      " 'can be improved through effective communication channels and regular '\n",
      " 'team-building activities.\\\\n\\\\nScrum can be scaled and adapted to larger '\n",
      " 'projects by using frameworks like Scrum of Scrums or LeSS (Large Scale '\n",
      " 'Scrum). Concerns about speed versus quality can be addressed by '\n",
      " 'incorporating quality assurance practices, such as continuous integration '\n",
      " 'and automated testing, into the Scrum process. Scope creep can be managed by '\n",
      " 'having a well-defined and prioritized product backlog, and a strong product '\n",
      " 'owner can be developed through training and mentorship.\\\\n\\\\nResistance to '\n",
      " 'change can be overcome by providing proper education and communication to '\n",
      " 'stakeholders and involving them in the decision-making process. Ultimately, '\n",
      " 'the cons of Scrum can be seen as opportunities for growth and improvement, '\n",
      " 'and with the right mindset and support, they can be effectively '\n",
      " 'managed.\\\\n\\\\nIn conclusion, while Scrum may have its challenges and '\n",
      " 'potential cons, the benefits and advantages it offers in terms of '\n",
      " 'collaboration, flexibility, adaptability, transparency, and customer '\n",
      " 'satisfaction make it a widely adopted and successful project management '\n",
      " 'framework. With proper implementation and continuous improvement, '\n",
      " 'organizations can leverage Scrum to drive innovation, efficiency, and '\n",
      " \"project success.'Help us out by providing feedback on this documentation \"\n",
      " 'page:PreviousManaging prompt sizeNextü¶úüõ†Ô∏è LangSmithBranching and '\n",
      " 'MergingCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' ----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'LangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain',\n",
      " 'Skip to main contentComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì '\n",
      " 'LangServeSecurityExpression LanguageLangChain Expression Language '\n",
      " '(LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily '\n",
      " 'compose chains together.\\n'\n",
      " 'LCEL was designed from day 1 to support putting prototypes in production, '\n",
      " 'with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most '\n",
      " 'complex chains (we‚Äôve seen folks successfully run LCEL chains with 100s of '\n",
      " 'steps in production). To highlight a few of the reasons you might want to '\n",
      " 'use LCEL:First-class streaming support\\n'\n",
      " 'When you build your chains with LCEL you get the best possible '\n",
      " 'time-to-first-token (time elapsed until the first chunk of output comes '\n",
      " 'out). For some chains this means eg. we stream tokens straight from an LLM '\n",
      " 'to a streaming output parser, and you get back parsed, incremental chunks of '\n",
      " 'output at the same rate as the LLM provider outputs the raw tokens.Async '\n",
      " 'support\\n'\n",
      " 'Any chain built with LCEL can be called both with the synchronous API (eg. '\n",
      " 'in your Jupyter notebook while prototyping) as well as with the asynchronous '\n",
      " 'API (eg. in a LangServe server). This enables using the same code for '\n",
      " 'prototypes and in production, with great performance, and the ability to '\n",
      " 'handle many concurrent requests in the same server.Optimized parallel '\n",
      " 'execution\\n'\n",
      " 'Whenever your LCEL chains have steps that can be executed in parallel (eg if '\n",
      " 'you fetch documents from multiple retrievers) we automatically do it, both '\n",
      " 'in the sync and the async interfaces, for the smallest possible '\n",
      " 'latency.Retries and fallbacks\\n'\n",
      " 'Configure retries and fallbacks for any part of your LCEL chain. This is a '\n",
      " 'great way to make your chains more reliable at scale. We‚Äôre currently '\n",
      " 'working on adding streaming support for retries/fallbacks, so you can get '\n",
      " 'the added reliability without any latency cost.Access intermediate results\\n'\n",
      " 'For more complex chains it‚Äôs often very useful to access the results of '\n",
      " 'intermediate steps even before the final output is produced. This can be '\n",
      " 'used to let end-users know something is happening, or even just to debug '\n",
      " 'your chain. You can stream intermediate results, and it‚Äôs available on every '\n",
      " 'LangServe server.Input and output schemas\\n'\n",
      " 'Input and output schemas give every LCEL chain Pydantic and JSONSchema '\n",
      " 'schemas inferred from the structure of your chain. This can be used for '\n",
      " 'validation of inputs and outputs, and is an integral part of '\n",
      " 'LangServe.Seamless LangSmith tracing\\n'\n",
      " 'As your chains get more and more complex, it becomes increasingly important '\n",
      " 'to understand what exactly is happening at every step.\\n'\n",
      " 'With LCEL, all steps are automatically logged to LangSmith for maximum '\n",
      " 'observability and debuggability.Seamless LangServe deployment\\n'\n",
      " 'Any chain created with LCEL can be easily deployed using LangServe.Help us '\n",
      " 'out by providing feedback on this documentation page:PreviousWeb '\n",
      " 'scrapingNextGet '\n",
      " 'startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '¬© 2024 LangChain, Inc.']\n"
     ]
    }
   ],
   "source": [
    "pprint(text_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd= OpenAIEmbeddings()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model= ChatOpenAI(temperature=0, model= 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree construction\n",
    "The clustering approach is following few algorithms\n",
    "\n",
    "#### GMM (Gaussian Mixture Model)\n",
    "* Model the distribution of points across different clusters\n",
    "* Optimal number of clusters by evaluating model's Bayesian Information Criterion (IBC)\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "* Reduces the dimensionality of high-dimensional data\n",
    "* supports clustering\n",
    "* Helps to hilight the natural group of data points based on their similarities\n",
    "\n",
    "### Local and global clustering\n",
    "* Used to analyse data and different scales\n",
    "* Both fine-grained and border patterns within the data points based on siilarities\n",
    "\n",
    "### Thresholding\n",
    "* Apply in the context of GMM to determine cluster membership\n",
    "* Based on the probability distribution (assingment of data points >= 1 cluster)\n",
    "---\n",
    "Code for GMM and thresholding is from Sarthi et al, as noted in the below two sources:\n",
    "* [original repo](https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py)\n",
    "* [Minor Tweaks](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/clustering.py)\n",
    "\n",
    "Full credit to both the authors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/tqdm-4.66.2-py3.10.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "RANDOM_SEED= 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "* It is basically combining multiple docs of same intention and clubbing them.\n",
    "* Then create a summary out of those docs\n",
    "* We will get one more layer of summary\n",
    "* Combine summaries with same intention\n",
    "* Create summary from those summary and create one more level.\n",
    "* Continue untill you get single summary or the same summary\n",
    "\n",
    "- Use **GMM** as clustering technique\n",
    "- When using GM you might face slowness due to higher dimensionality\n",
    "- Hence we use UMAP to reduce dimension and considerably less loss compared to other algos such as PCA, t-SNE\n",
    "- To Determine optimal number of clusters we use **BIC** as it will penalise model complexity and reward goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim:int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str=\"cosine\",\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function definition:\n",
    "        Perform global dimensionality reduction on the embeddings using UMAP\n",
    "    Parameters:\n",
    "    - Embeddings: input embeddings\n",
    "    - dim: The target dimensionality for reduced space\n",
    "    - n_neighbors: Optional: Number of neighbors consider for each points.\n",
    "                             If not specified, it defaults to square root of number of embeddings\n",
    "    - metric: The distance metric to use for UMAP\n",
    "    \n",
    "    returns:\n",
    "    - A numpy array of the embbedings reduced to the specified dimensionality.        \n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors= int((len(embeddings)-1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors= n_neighbors, n_components= dim, metric= metric\n",
    "    ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    num_neighbors: int = 10,\n",
    "    metric: str = \"cosine\"\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Funcion definition:\n",
    "    Perform local dimensionality reduction. Basically creates local clustering inside global clustering\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: input embeddings as numpy array\n",
    "    - dim: Targer dimensionality for reduced space (to how much dimension do we have to reduce)\n",
    "    - num_neighbors: Number of neighbors to consider for each point\n",
    "    - metric: Distance to be used for UMAP\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of embeddings reduced to specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(n_neighbors=num_neighbors, n_components=dim,\n",
    "                    metric= metric).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int= RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"Determine the optimal number of clusters using Bayesian Information Criterion (BIC) with a \n",
    "       Gaussian Mixture Model\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): input embeddings of numpy array\n",
    "        max_clusters (int, optional): maximum number of clusters to consider. Defaults to 50.\n",
    "        random_state (int, optional): seed for reproducibility. Defaults to RANDOM_SEED.\n",
    "\n",
    "    Returns:\n",
    "        int: result with optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters= min(max_clusters, len(embeddings))\n",
    "    n_clusters= np.arange(1, max_clusters)\n",
    "    bics= []\n",
    "    for n in n_clusters:\n",
    "        gm= GaussianMixture(n_components=n, random_state= random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "        return n_clusters[np.argmin(bics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GMM Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float,\n",
    "                random_state: int = RANDOM_SEED) -> Tuple:\n",
    "    \"\"\"Cluster embedding using GMM based on probability threshold.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): input embeddings as numpy array\n",
    "        threshold (float): the probability threshold for assigning an embedding to a cluster\n",
    "        random_state (int, optional): Seed for reproducibility. Defaults to RANDOM_SEED.\n",
    "    Returns:\n",
    "            - A tuple containing cluster labels and number of clusters determined.\n",
    "    \"\"\"\n",
    "    ## Get optimal number of clusters using GMM\n",
    "    n_clusters= get_optimal_clusters(embeddings)\n",
    "    ##Create labels and clusters using GMM again\n",
    "    gm= GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs= gm.predict_proba(embeddings)\n",
    "    labels= [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAPTOR Overview](../RAPTOR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: umap-learn in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (0.5.6)\n",
      "Requirement already satisfied: scikit-learn in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: langchain_community in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: langchain-openai in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (0.1.7)\n",
      "Requirement already satisfied: langchainhub in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (0.1.15)\n",
      "Requirement already satisfied: chromadb in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: langchain-anthropic in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (0.1.13)\n",
      "Requirement already satisfied: matplotlib in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (3.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/PyYAML-6.0.1-py3.10-macosx-11.1-arm64.egg (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/SQLAlchemy-2.0.28-py3.10.egg (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/aiohttp-3.9.4rc0-py3.10-macosx-11.1-arm64.egg (from langchain) (3.9.4rc0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/async_timeout-4.0.3-py3.10.egg (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/dataclasses_json-0.6.4-py3.10.egg (from langchain) (0.6.4)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from langchain) (0.2.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from langchain) (0.2.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/langsmith-0.1.26-py3.10.egg (from langchain) (0.1.26)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/numpy-1.26.4-py3.10-macosx-11.1-arm64.egg (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/pydantic-2.6.4-py3.10.egg (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/requests-2.31.0-py3.10.egg (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/tenacity-8.2.3-py3.10.egg (from langchain) (8.2.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from umap-learn) (1.13.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from umap-learn) (0.59.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from umap-learn) (0.5.12)\n",
      "Requirement already satisfied: tqdm in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/tqdm-4.66.2-py3.10.egg (from umap-learn) (4.66.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from langchain-openai) (1.26.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from langchainhub) (2.31.0.20240406)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.28.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (4.10.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (1.17.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (1.62.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/orjson-3.9.15-py3.10-macosx-11.1-arm64.egg (from chromadb) (3.9.15)\n",
      "Requirement already satisfied: anthropic<1,>=0.26.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from langchain-anthropic) (0.26.0)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from langchain-anthropic) (0.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/pillow-10.2.0-py3.10-macosx-11.1-arm64.egg (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/aiosignal-1.3.1-py3.10.egg (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/attrs-23.2.0-py3.10.egg (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/frozenlist-1.4.1-py3.10-macosx-11.1-arm64.egg (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/multidict-6.0.5-py3.10.egg (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/yarl-1.9.4-py3.10-macosx-11.1-arm64.egg (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/anyio-4.3.0-py3.10.egg (from anthropic<1,>=0.26.0->langchain-anthropic) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/distro-1.9.0-py3.10.egg (from anthropic<1,>=0.26.0->langchain-anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/httpx-0.27.0-py3.10.egg (from anthropic<1,>=0.26.0->langchain-anthropic) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/sniffio-1.3.1-py3.10.egg (from anthropic<1,>=0.26.0->langchain-anthropic) (1.3.1)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/marshmallow-3.21.1-py3.10.egg (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/typing_inspect-0.9.0-py3.10.egg (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.36.3)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/certifi-2024.2.2-py3.10.egg (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/urllib3-2.2.1-py3.10.egg (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/jsonpatch-1.33-py3.10.egg (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.42.0)\n",
      "Requirement already satisfied: coloredlogs in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.7)\n",
      "Requirement already satisfied: protobuf in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/protobuf-4.25.3-py3.10.egg (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.24.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.45b0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (68.2.2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/annotated_types-0.6.0-py3.10.egg (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/pydantic_core-2.16.3-py3.10-macosx-11.1-arm64.egg (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/charset_normalizer-3.3.2-py3.10-macosx-11.1-arm64.egg (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/idna-3.6-py3.10.egg (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/click-8.1.7-py3.10.egg (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/h11-0.14.0-py3.10.egg (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/python_dotenv-1.0.1-py3.10.egg (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.26.0->langchain-anthropic) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/cachetools-5.3.3-py3.10.egg (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/httpcore-1.0.4-py3.10.egg (from httpx<1,>=0.23.0->anthropic<1,>=0.26.0->langchain-anthropic) (1.0.4)\n",
      "Requirement already satisfied: filelock in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/jsonpointer-2.4-py3.10.egg (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/mypy_extensions-1.0.0-py3.10.egg (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that return number of tokens from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name:str) -> int:\n",
    "    ## We will use tiktoken to get the token details\n",
    "    encoding= tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens= len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"Hello, I am Vikas\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Langchain docs LCEL docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader= RecursiveUrlLoader(\n",
    "    url= url,\n",
    "    max_depth= 20,\n",
    "    extractor= lambda x:Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️LangGraph🦜️🏓 LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment\\nAny chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousWeb scrapingNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'content_type': 'text/html; charset=utf-8', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 LangChain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL with pydantic output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader= RecursiveUrlLoader(\n",
    "    url= url,\n",
    "    max_depth=1,\n",
    "    extractor= lambda x: Soup(x, \"html.parser\").text\n",
    "\n",
    ")\n",
    "docs_pydantic= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nQuickstart | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput ParsersCustom Output ParserstypesRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel I/OOutput parsersQuickstartOn this pageQuickstartLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.And then one optional one:\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.Get started\\u200bBelow we go over the main type of output parser, the PydanticOutputParser.from langchain.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_core.pydantic_v1 import BaseModel, Field, validatorfrom langchain_openai import OpenAImodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @validator(\"setup\")    def question_ends_with_question_mark(cls, field):        if field[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return field# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParserPromptTemplateOpenAIJoke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')LCEL\\u200bOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.Output parsers accept a string or BaseMessage as input and can return an arbitrary type.parser.invoke(output)Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')Instead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:chain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')While all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.The SimpleJsonOutputParser for example can stream through partial outputs:from langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI Reference:SimpleJsonOutputParserlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]While the PydanticOutputParser cannot:list(chain.stream({\"query\": \"Tell me a joke.\"}))[Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')]Help us out by providing feedback on this documentation page:PreviousOutput ParsersNextOutput ParsersGet startedLCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start', 'content_type': 'text/html; charset=utf-8', 'title': 'Quickstart | 🦜️🔗 LangChain', 'description': 'Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.', 'language': 'en'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LCEL with self query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader= RecursiveUrlLoader(\n",
    "    url= url,\n",
    "    max_depth=1,\n",
    "    extractor= lambda x: Soup(x, \"html.parser\").text\n",
    "\n",
    ")\n",
    "docs_sq= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nSelf-querying | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversVector store-backed retrieverRetrieversMultiQueryRetrieverContextual compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn this pageSelf-queryinginfoHead to Integrations for documentation on vector stores with built-in support for self-querying.A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.Get started\\u200bFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.Note: The self-query retriever requires you to have lark package installed.%pip install --upgrade --quiet  lark langchain-chromafrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our self-querying retriever\\u200bNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.from langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out\\u200bAnd now we can actually try using our retriever!# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Filter k\\u200bWe can also use the self query retriever to specify k: the number of documents to fetch.We can do this by passing enable_limit=True to the constructor.retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Constructing from scratch with LCEL\\u200bTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.First, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.from langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParserget_query_constructor_promptLet\\'s look at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:```json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparatorattr (string):  name of attribute to apply the comparison toval (string): is the comparison valueA logical operation statement takes the form op(statement1, statement2, ...):op (and | or | not): logical operatorstatement1, statement2, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format YYYY-MM-DD when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}<< Example 2. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs that were not published on SpotifyStructured Request:{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\nData Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}User Query:\\ndummy questionStructured Request:And what our full chain produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })StructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)The query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.from langchain.retrievers.self_query.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Help us out by providing feedback on this documentation page:PreviousParent Document RetrieverNextTime-weighted vector store retrieverGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/', 'content_type': 'text/html; charset=utf-8', 'title': 'Self-querying | 🦜️🔗 LangChain', 'description': 'Head to Integrations for documentation on vector stores with built-in support for self-querying.', 'language': 'en'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append all the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts= [d.page_content for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'LangChain Expression Language (LCEL) | 🦜️🔗 LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old '\n",
      " 'v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️LangGraph🦜️🏓 '\n",
      " 'LangServeSecurityExpression LanguageLangChain Expression Language '\n",
      " '(LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily '\n",
      " 'compose chains together.\\n'\n",
      " 'LCEL was designed from day 1 to support putting prototypes in production, '\n",
      " 'with no code changes, from the simplest “prompt + LLM” chain to the most '\n",
      " 'complex chains (we’ve seen folks successfully run LCEL chains with 100s of '\n",
      " 'steps in production). To highlight a few of the reasons you might want to '\n",
      " 'use LCEL:First-class streaming support\\n'\n",
      " 'When you build your chains with LCEL you get the best possible '\n",
      " 'time-to-first-token (time elapsed until the first chunk of output comes '\n",
      " 'out). For some chains this means eg. we stream tokens straight from an LLM '\n",
      " 'to a streaming output parser, and you get back parsed, incremental chunks of '\n",
      " 'output at the same rate as the LLM provider outputs the raw tokens.Async '\n",
      " 'support\\n'\n",
      " 'Any chain built with LCEL can be called both with the synchronous API (eg. '\n",
      " 'in your Jupyter notebook while prototyping) as well as with the asynchronous '\n",
      " 'API (eg. in a LangServe server). This enables using the same code for '\n",
      " 'prototypes and in production, with great performance, and the ability to '\n",
      " 'handle many concurrent requests in the same server.Optimized parallel '\n",
      " 'execution\\n'\n",
      " 'Whenever your LCEL chains have steps that can be executed in parallel (eg if '\n",
      " 'you fetch documents from multiple retrievers) we automatically do it, both '\n",
      " 'in the sync and the async interfaces, for the smallest possible '\n",
      " 'latency.Retries and fallbacks\\n'\n",
      " 'Configure retries and fallbacks for any part of your LCEL chain. This is a '\n",
      " 'great way to make your chains more reliable at scale. We’re currently '\n",
      " 'working on adding streaming support for retries/fallbacks, so you can get '\n",
      " 'the added reliability without any latency cost.Access intermediate results\\n'\n",
      " 'For more complex chains it’s often very useful to access the results of '\n",
      " 'intermediate steps even before the final output is produced. This can be '\n",
      " 'used to let end-users know something is happening, or even just to debug '\n",
      " 'your chain. You can stream intermediate results, and it’s available on every '\n",
      " 'LangServe server.Input and output schemas\\n'\n",
      " 'Input and output schemas give every LCEL chain Pydantic and JSONSchema '\n",
      " 'schemas inferred from the structure of your chain. This can be used for '\n",
      " 'validation of inputs and outputs, and is an integral part of '\n",
      " 'LangServe.Seamless LangSmith tracing\\n'\n",
      " 'As your chains get more and more complex, it becomes increasingly important '\n",
      " 'to understand what exactly is happening at every step.\\n'\n",
      " 'With LCEL, all steps are automatically logged to LangSmith for maximum '\n",
      " 'observability and debuggability.Seamless LangServe deployment\\n'\n",
      " 'Any chain created with LCEL can be easily deployed using LangServe.Help us '\n",
      " 'out by providing feedback on this documentation page:PreviousWeb '\n",
      " 'scrapingNextGet '\n",
      " 'startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Quickstart | 🦜️🔗 LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old '\n",
      " 'v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput '\n",
      " 'ParsersCustom Output ParserstypesRetrievalDocument loadersText '\n",
      " 'splittersEmbedding modelsVector '\n",
      " 'storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel '\n",
      " 'I/OOutput parsersQuickstartOn this pageQuickstartLanguage models output '\n",
      " 'text. But many times you may want to get more structured information than '\n",
      " 'just text back. This is where output parsers come in.Output parsers are '\n",
      " 'classes that help structure language model responses. There are two main '\n",
      " 'methods an output parser must implement:\"Get format instructions\": A method '\n",
      " 'which returns a string containing instructions for how the output of a '\n",
      " 'language model should be formatted.\"Parse\": A method which takes in a string '\n",
      " '(assumed to be the response from a language model) and parses it into some '\n",
      " 'structure.And then one optional one:\"Parse with prompt\": A method which '\n",
      " 'takes in a string (assumed to be the response from a language model) and a '\n",
      " 'prompt (assumed to be the prompt that generated such a response) and parses '\n",
      " 'it into some structure. The prompt is largely provided in the event the '\n",
      " 'OutputParser wants to retry or fix the output in some way, and needs '\n",
      " 'information from the prompt to do so.Get started\\u200bBelow we go over the '\n",
      " 'main type of output parser, the PydanticOutputParser.from '\n",
      " 'langchain.output_parsers import PydanticOutputParserfrom '\n",
      " 'langchain_core.prompts import PromptTemplatefrom langchain_core.pydantic_v1 '\n",
      " 'import BaseModel, Field, validatorfrom langchain_openai import OpenAImodel = '\n",
      " 'OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your '\n",
      " 'desired data structure.class Joke(BaseModel):    setup: str = '\n",
      " 'Field(description=\"question to set up a joke\")    punchline: str = '\n",
      " 'Field(description=\"answer to resolve the joke\")    # You can add custom '\n",
      " 'validation logic easily with Pydantic.    @validator(\"setup\")    def '\n",
      " 'question_ends_with_question_mark(cls, field):        if field[-1] != '\n",
      " '\"?\":            raise ValueError(\"Badly formed question!\")        return '\n",
      " 'field# Set up a parser + inject instructions into the prompt template.parser '\n",
      " '= PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    '\n",
      " 'template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    '\n",
      " 'input_variables=[\"query\"],    partial_variables={\"format_instructions\": '\n",
      " 'parser.get_format_instructions()},)# And a query intended to prompt a '\n",
      " 'language model to populate the data structure.prompt_and_model = prompt | '\n",
      " 'modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a '\n",
      " 'joke.\"})parser.invoke(output)API '\n",
      " \"Reference:PydanticOutputParserPromptTemplateOpenAIJoke(setup='Why did the \"\n",
      " \"chicken cross the road?', punchline='To get to the other \"\n",
      " \"side!')LCEL\\u200bOutput parsers implement the Runnable interface, the basic \"\n",
      " 'building block of the LangChain Expression Language (LCEL). This means they '\n",
      " 'support invoke, ainvoke, stream, astream, batch, abatch, astream_log '\n",
      " 'calls.Output parsers accept a string or BaseMessage as input and can return '\n",
      " \"an arbitrary type.parser.invoke(output)Joke(setup='Why did the chicken cross \"\n",
      " \"the road?', punchline='To get to the other side!')Instead of manually \"\n",
      " \"invoking the parser, we also could've just added it to our Runnable \"\n",
      " 'sequence:chain = prompt | model | parserchain.invoke({\"query\": \"Tell me a '\n",
      " 'joke.\"})Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To '\n",
      " \"get to the other side!')While all parsers support the streaming interface, \"\n",
      " 'only certain parsers can stream through partially parsed objects, since this '\n",
      " 'is highly dependent on the output type. Parsers which cannot construct '\n",
      " 'partial objects will simply yield the fully parsed output.The '\n",
      " 'SimpleJsonOutputParser for example can stream through partial outputs:from '\n",
      " 'langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = '\n",
      " 'PromptTemplate.from_template(    \"Return a JSON object with an `answer` key '\n",
      " 'that answers the following question: {question}\")json_parser = '\n",
      " 'SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI '\n",
      " 'Reference:SimpleJsonOutputParserlist(json_chain.stream({\"question\": \"Who '\n",
      " 'invented the microscope?\"}))[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, '\n",
      " \"{'answer': 'Anton'}, {'answer': 'Antonie'}, {'answer': 'Antonie van'}, \"\n",
      " \"{'answer': 'Antonie van Lee'}, {'answer': 'Antonie van Leeu'}, {'answer': \"\n",
      " \"'Antonie van Leeuwen'}, {'answer': 'Antonie van Leeuwenho'}, {'answer': \"\n",
      " \"'Antonie van Leeuwenhoek'}]While the PydanticOutputParser \"\n",
      " 'cannot:list(chain.stream({\"query\": \"Tell me a joke.\"}))[Joke(setup=\\'Why did '\n",
      " \"the chicken cross the road?', punchline='To get to the other side!')]Help us \"\n",
      " 'out by providing feedback on this documentation page:PreviousOutput '\n",
      " 'ParsersNextOutput ParsersGet '\n",
      " 'startedLCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Self-querying | 🦜️🔗 LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old '\n",
      " 'v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument '\n",
      " 'loadersText splittersEmbedding modelsVector storesRetrieversVector '\n",
      " 'store-backed retrieverRetrieversMultiQueryRetrieverContextual '\n",
      " 'compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector '\n",
      " 'RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store '\n",
      " 'retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn '\n",
      " 'this pageSelf-queryinginfoHead to Integrations for documentation on vector '\n",
      " 'stores with built-in support for self-querying.A self-querying retriever is '\n",
      " 'one that, as the name suggests, has the ability to query itself. '\n",
      " 'Specifically, given any natural language query, the retriever uses a '\n",
      " 'query-constructing LLM chain to write a structured query and then applies '\n",
      " 'that structured query to its underlying VectorStore. This allows the '\n",
      " 'retriever to not only use the user-input query for semantic similarity '\n",
      " 'comparison with the contents of stored documents but to also extract filters '\n",
      " 'from the user query on the metadata of stored documents and to execute those '\n",
      " \"filters.Get started\\u200bFor demonstration purposes we'll use a Chroma \"\n",
      " \"vector store. We've created a small demo set of documents that contain \"\n",
      " 'summaries of movies.Note: The self-query retriever requires you to have lark '\n",
      " 'package installed.%pip install --upgrade --quiet  lark langchain-chromafrom '\n",
      " 'langchain_chroma import Chromafrom langchain_core.documents import '\n",
      " 'Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    '\n",
      " 'Document(        page_content=\"A bunch of scientists bring back dinosaurs '\n",
      " 'and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, '\n",
      " '\"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo '\n",
      " 'DiCaprio gets lost in a dream within a dream within a dream within a '\n",
      " '...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", '\n",
      " '\"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / '\n",
      " 'detective gets lost in a series of dreams within dreams within dreams and '\n",
      " 'Inception reused the idea\",        metadata={\"year\": 2006, \"director\": '\n",
      " '\"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A '\n",
      " 'bunch of normal-sized women are supremely wholesome and some men pine after '\n",
      " 'them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": '\n",
      " '8.3},    ),    Document(        page_content=\"Toys come alive and have a '\n",
      " 'blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    '\n",
      " '),    Document(        page_content=\"Three men walk into the Zone, three men '\n",
      " 'walk out of the Zone\",        metadata={            \"year\": 1979,            '\n",
      " '\"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            '\n",
      " '\"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, '\n",
      " 'OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our '\n",
      " 'self-querying retriever\\u200bNow we can instantiate our retriever. To do '\n",
      " \"this we'll need to provide some information upfront about the metadata \"\n",
      " 'fields that our documents support and a short description of the document '\n",
      " 'contents.from langchain.chains.query_constructor.base import '\n",
      " 'AttributeInfofrom langchain.retrievers.self_query.base import '\n",
      " 'SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info '\n",
      " '= [    AttributeInfo(        name=\"genre\",        description=\"The genre of '\n",
      " \"the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', \"\n",
      " '\\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    '\n",
      " 'AttributeInfo(        name=\"year\",        description=\"The year the movie '\n",
      " 'was released\",        type=\"integer\",    ),    AttributeInfo(        '\n",
      " 'name=\"director\",        description=\"The name of the movie director\",        '\n",
      " 'type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A '\n",
      " '1-10 rating for the movie\", type=\"float\"    ),]document_content_description '\n",
      " '= \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = '\n",
      " 'SelfQueryRetriever.from_llm(    llm,    vectorstore,    '\n",
      " 'document_content_description,    metadata_field_info,)API '\n",
      " 'Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out\\u200bAnd '\n",
      " 'now we can actually try using our retriever!# This example only specifies a '\n",
      " 'filterretriever.invoke(\"I want to watch a movie rated higher than '\n",
      " '8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk '\n",
      " \"out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': \"\n",
      " \"'thriller', 'rating': 9.9, 'year': 1979}), Document(page_content='A \"\n",
      " 'psychologist / detective gets lost in a series of dreams within dreams '\n",
      " \"within dreams and Inception reused the idea', metadata={'director': 'Satoshi \"\n",
      " \"Kon', 'rating': 8.6, 'year': 2006})]# This example specifies a query and a \"\n",
      " 'filterretriever.invoke(\"Has Greta Gerwig directed any movies about '\n",
      " 'women\")[Document(page_content=\\'A bunch of normal-sized women are supremely '\n",
      " \"wholesome and some men pine after them', metadata={'director': 'Greta \"\n",
      " \"Gerwig', 'rating': 8.3, 'year': 2019})]# This example specifies a composite \"\n",
      " 'filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction '\n",
      " 'film?\")[Document(page_content=\\'A psychologist / detective gets lost in a '\n",
      " \"series of dreams within dreams within dreams and Inception reused the idea', \"\n",
      " \"metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}), \"\n",
      " \"Document(page_content='Three men walk into the Zone, three men walk out of \"\n",
      " \"the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', \"\n",
      " \"'rating': 9.9, 'year': 1979})]# This example specifies a query and composite \"\n",
      " 'filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 '\n",
      " \"that's all about toys, and preferably is \"\n",
      " 'animated\")[Document(page_content=\\'Toys come alive and have a blast doing '\n",
      " \"so', metadata={'genre': 'animated', 'year': 1995})]Filter k\\u200bWe can also \"\n",
      " 'use the self query retriever to specify k: the number of documents to '\n",
      " 'fetch.We can do this by passing enable_limit=True to the '\n",
      " 'constructor.retriever = SelfQueryRetriever.from_llm(    llm,    '\n",
      " 'vectorstore,    document_content_description,    metadata_field_info,    '\n",
      " 'enable_limit=True,)# This example only specifies a relevant '\n",
      " 'queryretriever.invoke(\"What are two movies about '\n",
      " 'dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back '\n",
      " \"dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', \"\n",
      " \"'rating': 7.7, 'year': 1993}), Document(page_content='Toys come alive and \"\n",
      " \"have a blast doing so', metadata={'genre': 'animated', 'year': \"\n",
      " \"1995})]Constructing from scratch with LCEL\\u200bTo see what's going on under \"\n",
      " 'the hood, and to have more custom control, we can reconstruct our retriever '\n",
      " 'from scratch.First, we need to create a query-construction chain. This chain '\n",
      " 'will take a user query and generated a StructuredQuery object which captures '\n",
      " 'the filters specified by the user. We provide some helper functions for '\n",
      " 'creating a prompt and output parser. These have a number of tunable params '\n",
      " \"that we'll ignore here for simplicity.from \"\n",
      " 'langchain.chains.query_constructor.base import (    '\n",
      " 'StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = '\n",
      " 'get_query_constructor_prompt(    document_content_description,    '\n",
      " 'metadata_field_info,)output_parser = '\n",
      " 'StructuredQueryOutputParser.from_components()query_constructor = prompt | '\n",
      " 'llm | output_parserAPI '\n",
      " \"Reference:StructuredQueryOutputParserget_query_constructor_promptLet's look \"\n",
      " 'at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to '\n",
      " \"structure the user's query to match the request schema provided below.<< \"\n",
      " 'Structured Request Schema >>When responding use a markdown code snippet with '\n",
      " 'a JSON object formatted in the following schema:```json{    \"query\": string '\n",
      " '\\\\ text string to compare to document contents    \"filter\": string \\\\ '\n",
      " 'logical condition statement for filtering documents}The query string should '\n",
      " 'contain only text that is expected to match the contents of documents. Any '\n",
      " 'conditions in the filter should not be mentioned in the query as well.A '\n",
      " 'logical condition statement is composed of one or more comparison and '\n",
      " 'logical operation statements.A comparison statement takes the form: '\n",
      " 'comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | '\n",
      " 'nin): comparatorattr (string):  name of attribute to apply the comparison '\n",
      " 'toval (string): is the comparison valueA logical operation statement takes '\n",
      " 'the form op(statement1, statement2, ...):op (and | or | not): logical '\n",
      " 'operatorstatement1, statement2, ... (comparison statements or logical '\n",
      " 'operation statements): one or more statements to apply the operation toMake '\n",
      " 'sure that you only use the comparators and logical operators listed above '\n",
      " 'and no others.\\n'\n",
      " 'Make sure that filters only refer to attributes that exist in the data '\n",
      " 'source.\\n'\n",
      " 'Make sure that filters only use the attributed names with its function names '\n",
      " 'if there are functions applied on them.\\n'\n",
      " 'Make sure that filters only use format YYYY-MM-DD when handling date data '\n",
      " 'typed values.\\n'\n",
      " 'Make sure that filters take into account the descriptions of attributes and '\n",
      " 'only make comparisons that are feasible given the type of data being '\n",
      " 'stored.\\n'\n",
      " 'Make sure that filters are only used as needed. If there are no filters that '\n",
      " 'should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        '\n",
      " '\"artist\": {            \"type\": \"string\",            \"description\": \"Name of '\n",
      " 'the song artist\"        },        \"length\": {            \"type\": '\n",
      " '\"integer\",            \"description\": \"Length of the song in seconds\"        '\n",
      " '},        \"genre\": {            \"type\": \"string\",            \"description\": '\n",
      " '\"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\n'\n",
      " 'What are songs by Taylor Swift or Katy Perry about teenage romance under 3 '\n",
      " 'minutes long in the dance pop genreStructured Request:{    \"query\": '\n",
      " '\"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), '\n",
      " 'eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", '\n",
      " '\\\\\"pop\\\\\"))\"}<< Example 2. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        '\n",
      " '\"artist\": {            \"type\": \"string\",            \"description\": \"Name of '\n",
      " 'the song artist\"        },        \"length\": {            \"type\": '\n",
      " '\"integer\",            \"description\": \"Length of the song in seconds\"        '\n",
      " '},        \"genre\": {            \"type\": \"string\",            \"description\": '\n",
      " '\"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\n'\n",
      " 'What are songs that were not published on SpotifyStructured Request:{    '\n",
      " '\"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": '\n",
      " '{    \"genre\": {        \"description\": \"The genre of the movie. One of '\n",
      " \"['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', \"\n",
      " '\\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        '\n",
      " '\"description\": \"The year the movie was released\",        \"type\": '\n",
      " '\"integer\"    },    \"director\": {        \"description\": \"The name of the '\n",
      " 'movie director\",        \"type\": \"string\"    },    \"rating\": {        '\n",
      " '\"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    '\n",
      " '}}}User Query:\\n'\n",
      " 'dummy questionStructured Request:And what our full chain '\n",
      " 'produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are '\n",
      " \"some sci-fi movies from the 90's directed by Luc Besson about taxi \"\n",
      " 'drivers\"    })StructuredQuery(query=\\'taxi driver\\', '\n",
      " \"filter=Operation(operator=<Operator.AND: 'and'>, \"\n",
      " \"arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', \"\n",
      " \"value='science fiction'), Operation(operator=<Operator.AND: 'and'>, \"\n",
      " \"arguments=[Comparison(comparator=<Comparator.GTE: 'gte'>, attribute='year', \"\n",
      " \"value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', \"\n",
      " \"value=2000)]), Comparison(comparator=<Comparator.EQ: 'eq'>, \"\n",
      " \"attribute='director', value='Luc Besson')]), limit=None)The query \"\n",
      " 'constructor is the key element of the self-query retriever. To make a great '\n",
      " \"retrieval system you'll need to make sure your query constructor works well. \"\n",
      " 'Often this requires adjusting the prompt, the examples in the prompt, the '\n",
      " 'attribute descriptions, etc. For an example that walks through refining a '\n",
      " 'query constructor on some hotel inventory data, check out this cookbook.The '\n",
      " 'next key element is the structured query translator. This is the object '\n",
      " 'responsible for translating the generic StructuredQuery object into a '\n",
      " \"metadata filter in the syntax of the vector store you're using. LangChain \"\n",
      " 'comes with a number of built-in translators. To see them all head to the '\n",
      " 'Integrations section.from langchain.retrievers.self_query.chroma import '\n",
      " 'ChromaTranslatorretriever = SelfQueryRetriever(    '\n",
      " 'query_constructor=query_constructor,    vectorstore=vectorstore,    '\n",
      " 'structured_query_translator=ChromaTranslator(),)API '\n",
      " 'Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 '\n",
      " \"but before 2005 that's all about toys, and preferably is \"\n",
      " 'animated\")[Document(page_content=\\'Toys come alive and have a blast doing '\n",
      " \"so', metadata={'genre': 'animated', 'year': 1995})]Help us out by providing \"\n",
      " 'feedback on this documentation page:PreviousParent Document '\n",
      " 'RetrieverNextTime-weighted vector store retrieverGet startedCreating our '\n",
      " 'self-querying retrieverTesting it outFilter kConstructing from scratch with '\n",
      " 'LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n']\n"
     ]
    }
   ],
   "source": [
    "pprint(docs_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate No. of tokens for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[774, 1156, 3124]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts= [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets plot histogram to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins= 50, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token count\")\n",
    "plt.xlabel(\"Token count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5054"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COncat all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Number of tokens in all content: 5060'\n"
     ]
    }
   ],
   "source": [
    "d_sorted= sorted(docs, key= lambda x: x.metadata['source'])\n",
    "d_reversed= list(reversed(d_sorted))\n",
    "concatenated_content= \"\\n\\n\\n ----- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "pprint(\"Number of tokens in all content: %s\"\n",
    "       % num_tokens_from_string(concatenated_content, \"cl100k_base\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://python.langchain.com/docs/expression_language/',\n",
       " 'content_type': 'text/html; charset=utf-8',\n",
       " 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 LangChain',\n",
       " 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_sorted[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok= 2000\n",
    "text_splitter= RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size= chunk_size_tok,\n",
    "                                                                    chunk_overlap=0)\n",
    "\n",
    "text_splits= text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quickstart | 🦜️🔗 LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old '\n",
      " 'v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput '\n",
      " 'ParsersCustom Output ParserstypesRetrievalDocument loadersText '\n",
      " 'splittersEmbedding modelsVector '\n",
      " 'storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel '\n",
      " 'I/OOutput parsersQuickstartOn this pageQuickstartLanguage models output '\n",
      " 'text. But many times you may want to get more structured information than '\n",
      " 'just text back. This is where output parsers come in.Output parsers are '\n",
      " 'classes that help structure language model responses. There are two main '\n",
      " 'methods an output parser must implement:\"Get format instructions\": A method '\n",
      " 'which returns a string containing instructions for how the output of a '\n",
      " 'language model should be formatted.\"Parse\": A method which takes in a string '\n",
      " '(assumed to be the response from a language model) and parses it into some '\n",
      " 'structure.And then one optional one:\"Parse with prompt\": A method which '\n",
      " 'takes in a string (assumed to be the response from a language model) and a '\n",
      " 'prompt (assumed to be the prompt that generated such a response) and parses '\n",
      " 'it into some structure. The prompt is largely provided in the event the '\n",
      " 'OutputParser wants to retry or fix the output in some way, and needs '\n",
      " 'information from the prompt to do so.Get started\\u200bBelow we go over the '\n",
      " 'main type of output parser, the PydanticOutputParser.from '\n",
      " 'langchain.output_parsers import PydanticOutputParserfrom '\n",
      " 'langchain_core.prompts import PromptTemplatefrom langchain_core.pydantic_v1 '\n",
      " 'import BaseModel, Field, validatorfrom langchain_openai import OpenAImodel = '\n",
      " 'OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your '\n",
      " 'desired data structure.class Joke(BaseModel):    setup: str = '\n",
      " 'Field(description=\"question to set up a joke\")    punchline: str = '\n",
      " 'Field(description=\"answer to resolve the joke\")    # You can add custom '\n",
      " 'validation logic easily with Pydantic.    @validator(\"setup\")    def '\n",
      " 'question_ends_with_question_mark(cls, field):        if field[-1] != '\n",
      " '\"?\":            raise ValueError(\"Badly formed question!\")        return '\n",
      " 'field# Set up a parser + inject instructions into the prompt template.parser '\n",
      " '= PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    '\n",
      " 'template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    '\n",
      " 'input_variables=[\"query\"],    partial_variables={\"format_instructions\": '\n",
      " 'parser.get_format_instructions()},)# And a query intended to prompt a '\n",
      " 'language model to populate the data structure.prompt_and_model = prompt | '\n",
      " 'modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a '\n",
      " 'joke.\"})parser.invoke(output)API '\n",
      " \"Reference:PydanticOutputParserPromptTemplateOpenAIJoke(setup='Why did the \"\n",
      " \"chicken cross the road?', punchline='To get to the other \"\n",
      " \"side!')LCEL\\u200bOutput parsers implement the Runnable interface, the basic \"\n",
      " 'building block of the LangChain Expression Language (LCEL). This means they '\n",
      " 'support invoke, ainvoke, stream, astream, batch, abatch, astream_log '\n",
      " 'calls.Output parsers accept a string or BaseMessage as input and can return '\n",
      " \"an arbitrary type.parser.invoke(output)Joke(setup='Why did the chicken cross \"\n",
      " \"the road?', punchline='To get to the other side!')Instead of manually \"\n",
      " \"invoking the parser, we also could've just added it to our Runnable \"\n",
      " 'sequence:chain = prompt | model | parserchain.invoke({\"query\": \"Tell me a '\n",
      " 'joke.\"})Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To '\n",
      " \"get to the other side!')While all parsers support the streaming interface, \"\n",
      " 'only certain parsers can stream through partially parsed objects, since this '\n",
      " 'is highly dependent on the output type. Parsers which cannot construct '\n",
      " 'partial objects will simply yield the fully parsed output.The '\n",
      " 'SimpleJsonOutputParser for example can stream through partial outputs:from '\n",
      " 'langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = '\n",
      " 'PromptTemplate.from_template(    \"Return a JSON object with an `answer` key '\n",
      " 'that answers the following question: {question}\")json_parser = '\n",
      " 'SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI '\n",
      " 'Reference:SimpleJsonOutputParserlist(json_chain.stream({\"question\": \"Who '\n",
      " 'invented the microscope?\"}))[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, '\n",
      " \"{'answer': 'Anton'}, {'answer': 'Antonie'}, {'answer': 'Antonie van'}, \"\n",
      " \"{'answer': 'Antonie van Lee'}, {'answer': 'Antonie van Leeu'}, {'answer': \"\n",
      " \"'Antonie van Leeuwen'}, {'answer': 'Antonie van Leeuwenho'}, {'answer': \"\n",
      " \"'Antonie van Leeuwenhoek'}]While the PydanticOutputParser \"\n",
      " 'cannot:list(chain.stream({\"query\": \"Tell me a joke.\"}))[Joke(setup=\\'Why did '\n",
      " \"the chicken cross the road?', punchline='To get to the other side!')]Help us \"\n",
      " 'out by providing feedback on this documentation page:PreviousOutput '\n",
      " 'ParsersNextOutput ParsersGet '\n",
      " 'startedLCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '© 2024 LangChain, Inc.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " ' ----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Self-querying | 🦜️🔗 LangChain',\n",
      " 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old '\n",
      " 'v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument '\n",
      " 'loadersText splittersEmbedding modelsVector storesRetrieversVector '\n",
      " 'store-backed retrieverRetrieversMultiQueryRetrieverContextual '\n",
      " 'compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector '\n",
      " 'RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store '\n",
      " 'retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn '\n",
      " 'this pageSelf-queryinginfoHead to Integrations for documentation on vector '\n",
      " 'stores with built-in support for self-querying.A self-querying retriever is '\n",
      " 'one that, as the name suggests, has the ability to query itself. '\n",
      " 'Specifically, given any natural language query, the retriever uses a '\n",
      " 'query-constructing LLM chain to write a structured query and then applies '\n",
      " 'that structured query to its underlying VectorStore. This allows the '\n",
      " 'retriever to not only use the user-input query for semantic similarity '\n",
      " 'comparison with the contents of stored documents but to also extract filters '\n",
      " 'from the user query on the metadata of stored documents and to execute those '\n",
      " \"filters.Get started\\u200bFor demonstration purposes we'll use a Chroma \"\n",
      " \"vector store. We've created a small demo set of documents that contain \"\n",
      " 'summaries of movies.Note: The self-query retriever requires you to have lark '\n",
      " 'package installed.%pip install --upgrade --quiet  lark langchain-chromafrom '\n",
      " 'langchain_chroma import Chromafrom langchain_core.documents import '\n",
      " 'Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    '\n",
      " 'Document(        page_content=\"A bunch of scientists bring back dinosaurs '\n",
      " 'and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, '\n",
      " '\"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo '\n",
      " 'DiCaprio gets lost in a dream within a dream within a dream within a '\n",
      " '...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", '\n",
      " '\"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / '\n",
      " 'detective gets lost in a series of dreams within dreams within dreams and '\n",
      " 'Inception reused the idea\",        metadata={\"year\": 2006, \"director\": '\n",
      " '\"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A '\n",
      " 'bunch of normal-sized women are supremely wholesome and some men pine after '\n",
      " 'them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": '\n",
      " '8.3},    ),    Document(        page_content=\"Toys come alive and have a '\n",
      " 'blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    '\n",
      " '),    Document(        page_content=\"Three men walk into the Zone, three men '\n",
      " 'walk out of the Zone\",        metadata={            \"year\": 1979,            '\n",
      " '\"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            '\n",
      " '\"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, '\n",
      " 'OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our '\n",
      " 'self-querying retriever\\u200bNow we can instantiate our retriever. To do '\n",
      " \"this we'll need to provide some information upfront about the metadata \"\n",
      " 'fields that our documents support and a short description of the document '\n",
      " 'contents.from langchain.chains.query_constructor.base import '\n",
      " 'AttributeInfofrom langchain.retrievers.self_query.base import '\n",
      " 'SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info '\n",
      " '= [    AttributeInfo(        name=\"genre\",        description=\"The genre of '\n",
      " \"the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', \"\n",
      " '\\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    '\n",
      " 'AttributeInfo(        name=\"year\",        description=\"The year the movie '\n",
      " 'was released\",        type=\"integer\",    ),    AttributeInfo(        '\n",
      " 'name=\"director\",        description=\"The name of the movie director\",        '\n",
      " 'type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A '\n",
      " '1-10 rating for the movie\", type=\"float\"    ),]document_content_description '\n",
      " '= \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = '\n",
      " 'SelfQueryRetriever.from_llm(    llm,    vectorstore,    '\n",
      " 'document_content_description,    metadata_field_info,)API '\n",
      " 'Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out\\u200bAnd '\n",
      " 'now we can actually try using our retriever!# This example only specifies a '\n",
      " 'filterretriever.invoke(\"I want to watch a movie rated higher than '\n",
      " '8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk '\n",
      " \"out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': \"\n",
      " \"'thriller', 'rating': 9.9, 'year': 1979}), Document(page_content='A \"\n",
      " 'psychologist / detective gets lost in a series of dreams within dreams '\n",
      " \"within dreams and Inception reused the idea', metadata={'director': 'Satoshi \"\n",
      " \"Kon', 'rating': 8.6, 'year': 2006})]# This example specifies a query and a \"\n",
      " 'filterretriever.invoke(\"Has Greta Gerwig directed any movies about '\n",
      " 'women\")[Document(page_content=\\'A bunch of normal-sized women are supremely '\n",
      " \"wholesome and some men pine after them', metadata={'director': 'Greta \"\n",
      " \"Gerwig', 'rating': 8.3, 'year': 2019})]# This example specifies a composite \"\n",
      " 'filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction '\n",
      " 'film?\")[Document(page_content=\\'A psychologist / detective gets lost in a '\n",
      " \"series of dreams within dreams within dreams and Inception reused the idea', \"\n",
      " \"metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}), \"\n",
      " \"Document(page_content='Three men walk into the Zone, three men walk out of \"\n",
      " \"the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', \"\n",
      " \"'rating': 9.9, 'year': 1979})]# This example specifies a query and composite \"\n",
      " 'filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 '\n",
      " \"that's all about toys, and preferably is \"\n",
      " 'animated\")[Document(page_content=\\'Toys come alive and have a blast doing '\n",
      " \"so', metadata={'genre': 'animated', 'year': 1995})]Filter k\\u200bWe can also \"\n",
      " 'use the self query retriever to specify k: the number of documents to '\n",
      " 'fetch.We can do this by passing enable_limit=True to the '\n",
      " 'constructor.retriever = SelfQueryRetriever.from_llm(    llm,    '\n",
      " 'vectorstore,    document_content_description,    metadata_field_info,    '\n",
      " 'enable_limit=True,)# This example only specifies a relevant '\n",
      " 'queryretriever.invoke(\"What are two movies about '\n",
      " 'dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back '\n",
      " \"dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', \"\n",
      " \"'rating': 7.7, 'year': 1993}), Document(page_content='Toys come alive and \"\n",
      " \"have a blast doing so', metadata={'genre': 'animated', 'year': \"\n",
      " \"1995})]Constructing from scratch with LCEL\\u200bTo see what's going on under \"\n",
      " 'the hood, and to have more custom control, we can reconstruct our retriever '\n",
      " 'from scratch.First, we need to create a query-construction chain. This chain '\n",
      " 'will take a user query and generated a',\n",
      " 'StructuredQuery object which captures the filters specified by the user. We '\n",
      " 'provide some helper functions for creating a prompt and output parser. These '\n",
      " \"have a number of tunable params that we'll ignore here for simplicity.from \"\n",
      " 'langchain.chains.query_constructor.base import (    '\n",
      " 'StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = '\n",
      " 'get_query_constructor_prompt(    document_content_description,    '\n",
      " 'metadata_field_info,)output_parser = '\n",
      " 'StructuredQueryOutputParser.from_components()query_constructor = prompt | '\n",
      " 'llm | output_parserAPI '\n",
      " \"Reference:StructuredQueryOutputParserget_query_constructor_promptLet's look \"\n",
      " 'at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to '\n",
      " \"structure the user's query to match the request schema provided below.<< \"\n",
      " 'Structured Request Schema >>When responding use a markdown code snippet with '\n",
      " 'a JSON object formatted in the following schema:```json{    \"query\": string '\n",
      " '\\\\ text string to compare to document contents    \"filter\": string \\\\ '\n",
      " 'logical condition statement for filtering documents}The query string should '\n",
      " 'contain only text that is expected to match the contents of documents. Any '\n",
      " 'conditions in the filter should not be mentioned in the query as well.A '\n",
      " 'logical condition statement is composed of one or more comparison and '\n",
      " 'logical operation statements.A comparison statement takes the form: '\n",
      " 'comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | '\n",
      " 'nin): comparatorattr (string):  name of attribute to apply the comparison '\n",
      " 'toval (string): is the comparison valueA logical operation statement takes '\n",
      " 'the form op(statement1, statement2, ...):op (and | or | not): logical '\n",
      " 'operatorstatement1, statement2, ... (comparison statements or logical '\n",
      " 'operation statements): one or more statements to apply the operation toMake '\n",
      " 'sure that you only use the comparators and logical operators listed above '\n",
      " 'and no others.',\n",
      " 'Make sure that filters only refer to attributes that exist in the data '\n",
      " 'source.\\n'\n",
      " 'Make sure that filters only use the attributed names with its function names '\n",
      " 'if there are functions applied on them.\\n'\n",
      " 'Make sure that filters only use format YYYY-MM-DD when handling date data '\n",
      " 'typed values.\\n'\n",
      " 'Make sure that filters take into account the descriptions of attributes and '\n",
      " 'only make comparisons that are feasible given the type of data being '\n",
      " 'stored.\\n'\n",
      " 'Make sure that filters are only used as needed. If there are no filters that '\n",
      " 'should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        '\n",
      " '\"artist\": {            \"type\": \"string\",            \"description\": \"Name of '\n",
      " 'the song artist\"        },        \"length\": {            \"type\": '\n",
      " '\"integer\",            \"description\": \"Length of the song in seconds\"        '\n",
      " '},        \"genre\": {            \"type\": \"string\",            \"description\": '\n",
      " '\"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\n'\n",
      " 'What are songs by Taylor Swift or Katy Perry about teenage romance under 3 '\n",
      " 'minutes long in the dance pop genreStructured Request:{    \"query\": '\n",
      " '\"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), '\n",
      " 'eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", '\n",
      " '\\\\\"pop\\\\\"))\"}<< Example 2. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        '\n",
      " '\"artist\": {            \"type\": \"string\",            \"description\": \"Name of '\n",
      " 'the song artist\"        },        \"length\": {            \"type\": '\n",
      " '\"integer\",            \"description\": \"Length of the song in seconds\"        '\n",
      " '},        \"genre\": {            \"type\": \"string\",            \"description\": '\n",
      " '\"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\n'\n",
      " 'What are songs that were not published on SpotifyStructured Request:{    '\n",
      " '\"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\n'\n",
      " 'Data Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": '\n",
      " '{    \"genre\": {        \"description\": \"The genre of the movie. One of '\n",
      " \"['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', \"\n",
      " '\\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        '\n",
      " '\"description\": \"The year the movie was released\",        \"type\": '\n",
      " '\"integer\"    },    \"director\": {        \"description\": \"The name of the '\n",
      " 'movie director\",        \"type\": \"string\"    },    \"rating\": {        '\n",
      " '\"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    '\n",
      " '}}}User Query:\\n'\n",
      " 'dummy questionStructured Request:And what our full chain '\n",
      " 'produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are '\n",
      " \"some sci-fi movies from the 90's directed by Luc Besson about taxi \"\n",
      " 'drivers\"    })StructuredQuery(query=\\'taxi driver\\', '\n",
      " \"filter=Operation(operator=<Operator.AND: 'and'>, \"\n",
      " \"arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', \"\n",
      " \"value='science fiction'), Operation(operator=<Operator.AND: 'and'>, \"\n",
      " \"arguments=[Comparison(comparator=<Comparator.GTE: 'gte'>, attribute='year', \"\n",
      " \"value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', \"\n",
      " \"value=2000)]), Comparison(comparator=<Comparator.EQ: 'eq'>, \"\n",
      " \"attribute='director', value='Luc Besson')]), limit=None)The query \"\n",
      " 'constructor is the key element of the self-query retriever. To make a great '\n",
      " \"retrieval system you'll need to make sure your query constructor works well. \"\n",
      " 'Often this requires adjusting the prompt, the examples in the prompt, the '\n",
      " 'attribute descriptions, etc. For an example that walks through refining a '\n",
      " 'query constructor on some hotel inventory data, check out this cookbook.The '\n",
      " 'next key element is the structured query translator. This is the object '\n",
      " 'responsible for translating the generic StructuredQuery object into a '\n",
      " \"metadata filter in the syntax of the vector store you're using. LangChain \"\n",
      " 'comes with a number of built-in translators. To see them all head to the '\n",
      " 'Integrations section.from langchain.retrievers.self_query.chroma import '\n",
      " 'ChromaTranslatorretriever = SelfQueryRetriever(    '\n",
      " 'query_constructor=query_constructor,    vectorstore=vectorstore,    '\n",
      " 'structured_query_translator=ChromaTranslator(),)API '\n",
      " 'Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 '\n",
      " \"but before 2005 that's all about toys, and preferably is \"\n",
      " 'animated\")[Document(page_content=\\'Toys come alive and have a blast doing '\n",
      " \"so', metadata={'genre': 'animated', 'year': 1995})]Help us out by providing \"\n",
      " 'feedback on this documentation page:PreviousParent Document '\n",
      " 'RetrieverNextTime-weighted vector store retrieverGet startedCreating our '\n",
      " 'self-querying retrieverTesting it outFilter kConstructing from scratch with '\n",
      " 'LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '© 2024 LangChain, Inc.',\n",
      " '----- \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'LangChain Expression Language (LCEL) | 🦜️🔗 LangChain\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old '\n",
      " 'v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI '\n",
      " 'ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith '\n",
      " 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS '\n",
      " 'Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with '\n",
      " 'RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A '\n",
      " 'over SQL + CSVMoreExpression LanguageGet startedRunnable '\n",
      " 'interfacePrimitivesAdvantages of LCELStreamingAdd message history '\n",
      " '(memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️LangGraph🦜️🏓 '\n",
      " 'LangServeSecurityExpression LanguageLangChain Expression Language '\n",
      " '(LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily '\n",
      " 'compose chains together.\\n'\n",
      " 'LCEL was designed from day 1 to support putting prototypes in production, '\n",
      " 'with no code changes, from the simplest “prompt + LLM” chain to the most '\n",
      " 'complex chains (we’ve seen folks successfully run LCEL chains with 100s of '\n",
      " 'steps in production). To highlight a few of the reasons you might want to '\n",
      " 'use LCEL:First-class streaming support\\n'\n",
      " 'When you build your chains with LCEL you get the best possible '\n",
      " 'time-to-first-token (time elapsed until the first chunk of output comes '\n",
      " 'out). For some chains this means eg. we stream tokens straight from an LLM '\n",
      " 'to a streaming output parser, and you get back parsed, incremental chunks of '\n",
      " 'output at the same rate as the LLM provider outputs the raw tokens.Async '\n",
      " 'support\\n'\n",
      " 'Any chain built with LCEL can be called both with the synchronous API (eg. '\n",
      " 'in your Jupyter notebook while prototyping) as well as with the asynchronous '\n",
      " 'API (eg. in a LangServe server). This enables using the same code for '\n",
      " 'prototypes and in production, with great performance, and the ability to '\n",
      " 'handle many concurrent requests in the same server.Optimized parallel '\n",
      " 'execution\\n'\n",
      " 'Whenever your LCEL chains have steps that can be executed in parallel (eg if '\n",
      " 'you fetch documents from multiple retrievers) we automatically do it, both '\n",
      " 'in the sync and the async interfaces, for the smallest possible '\n",
      " 'latency.Retries and fallbacks\\n'\n",
      " 'Configure retries and fallbacks for any part of your LCEL chain. This is a '\n",
      " 'great way to make your chains more reliable at scale. We’re currently '\n",
      " 'working on adding streaming support for retries/fallbacks, so you can get '\n",
      " 'the added reliability without any latency cost.Access intermediate results\\n'\n",
      " 'For more complex chains it’s often very useful to access the results of '\n",
      " 'intermediate steps even before the final output is produced. This can be '\n",
      " 'used to let end-users know something is happening, or even just to debug '\n",
      " 'your chain. You can stream intermediate results, and it’s available on every '\n",
      " 'LangServe server.Input and output schemas\\n'\n",
      " 'Input and output schemas give every LCEL chain Pydantic and JSONSchema '\n",
      " 'schemas inferred from the structure of your chain. This can be used for '\n",
      " 'validation of inputs and outputs, and is an integral part of '\n",
      " 'LangServe.Seamless LangSmith tracing\\n'\n",
      " 'As your chains get more and more complex, it becomes increasingly important '\n",
      " 'to understand what exactly is happening at every step.\\n'\n",
      " 'With LCEL, all steps are automatically logged to LangSmith for maximum '\n",
      " 'observability and debuggability.Seamless LangServe deployment\\n'\n",
      " 'Any chain created with LCEL can be easily deployed using LangServe.Help us '\n",
      " 'out by providing feedback on this documentation page:PreviousWeb '\n",
      " 'scrapingNextGet '\n",
      " 'startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright '\n",
      " '© 2024 LangChain, Inc.']\n"
     ]
    }
   ],
   "source": [
    "pprint(text_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd= OpenAIEmbeddings()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model= ChatOpenAI(temperature=0, model= 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree construction\n",
    "The clustering approach is following few algorithms\n",
    "\n",
    "#### GMM (Gaussian Mixture Model)\n",
    "* Model the distribution of points across different clusters\n",
    "* Optimal number of clusters by evaluating model's Bayesian Information Criterion (IBC)\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "* Reduces the dimensionality of high-dimensional data\n",
    "* supports clustering\n",
    "* Helps to hilight the natural group of data points based on their similarities\n",
    "\n",
    "### Local and global clustering\n",
    "* Used to analyse data and different scales\n",
    "* Both fine-grained and border patterns within the data points based on siilarities\n",
    "\n",
    "### Thresholding\n",
    "* Apply in the context of GMM to determine cluster membership\n",
    "* Based on the probability distribution (assingment of data points >= 1 cluster)\n",
    "---\n",
    "Code for GMM and thresholding is from Sarthi et al, as noted in the below two sources:\n",
    "* [original repo](https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py)\n",
    "* [Minor Tweaks](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/clustering.py)\n",
    "\n",
    "Full credit to both the authors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikaslakka/opt/miniconda3/envs/data_privacy/lib/python3.10/site-packages/tqdm-4.66.2-py3.10.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "RANDOM_SEED= 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "* It is basically combining multiple docs of same intention and clubbing them.\n",
    "* Then create a summary out of those docs\n",
    "* We will get one more layer of summary\n",
    "* Combine summaries with same intention\n",
    "* Create summary from those summary and create one more level.\n",
    "* Continue untill you get single summary or the same summary\n",
    "\n",
    "- Use **GMM** as clustering technique\n",
    "- When using GM you might face slowness due to higher dimensionality\n",
    "- Hence we use UMAP to reduce dimension and considerably less loss compared to other algos such as PCA, t-SNE\n",
    "- To Determine optimal number of clusters we use **BIC** as it will penalise model complexity and reward goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim:int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str=\"cosine\",\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function definition:\n",
    "        Perform global dimensionality reduction on the embeddings using UMAP\n",
    "    Parameters:\n",
    "    - Embeddings: input embeddings\n",
    "    - dim: The target dimensionality for reduced space\n",
    "    - n_neighbors: Optional: Number of neighbors consider for each points.\n",
    "                             If not specified, it defaults to square root of number of embeddings\n",
    "    - metric: The distance metric to use for UMAP\n",
    "    \n",
    "    returns:\n",
    "    - A numpy array of the embbedings reduced to the specified dimensionality.        \n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors= int((len(embeddings)-1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors= n_neighbors, n_components= dim, metric= metric\n",
    "    ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    num_neighbors: int = 10,\n",
    "    metric: str = \"cosine\"\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Funcion definition:\n",
    "    Perform local dimensionality reduction. Basically creates local clustering inside global clustering\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: input embeddings as numpy array\n",
    "    - dim: Targer dimensionality for reduced space (to how much dimension do we have to reduce)\n",
    "    - num_neighbors: Number of neighbors to consider for each point\n",
    "    - metric: Distance to be used for UMAP\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of embeddings reduced to specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(n_neighbors=num_neighbors, n_components=dim,\n",
    "                    metric= metric).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int= RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"Determine the optimal number of clusters using Bayesian Information Criterion (BIC) with a \n",
    "       Gaussian Mixture Model\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): input embeddings of numpy array\n",
    "        max_clusters (int, optional): maximum number of clusters to consider. Defaults to 50.\n",
    "        random_state (int, optional): seed for reproducibility. Defaults to RANDOM_SEED.\n",
    "\n",
    "    Returns:\n",
    "        int: result with optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters= min(max_clusters, len(embeddings))\n",
    "    n_clusters= np.arange(1, max_clusters)\n",
    "    bics= []\n",
    "    for n in n_clusters:\n",
    "        gm= GaussianMixture(n_components=n, random_state= random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GMM Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float,\n",
    "                random_state: int = RANDOM_SEED) -> Tuple:\n",
    "    \"\"\"Cluster embedding using GMM based on probability threshold.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): input embeddings as numpy array\n",
    "        threshold (float): the probability threshold for assigning an embedding to a cluster\n",
    "        random_state (int, optional): Seed for reproducibility. Defaults to RANDOM_SEED.\n",
    "    Returns:\n",
    "            - A tuple containing cluster labels and number of clusters determined.\n",
    "    \"\"\"\n",
    "    ## Get optimal number of clusters using GMM\n",
    "    n_clusters= get_optimal_clusters(embeddings)\n",
    "    ##Create labels and clusters using GMM again\n",
    "    gm= GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs= gm.predict_proba(embeddings)\n",
    "    labels= [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,    \n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"Steps:\n",
    "    1. Perform clustering on embeddings by first reducing dimensionality locally.\n",
    "    2. Global Clustering using Gaussian Mixture Model.\n",
    "    3. Performing local clustering within each global cluster.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Input embedding as numpy array\n",
    "        dim (int): The target dimensionality for UMAP reduction\n",
    "        threshold (float): The probability threshold for assigning an embedding to a cluster in GMM\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: List of numpy array, where each array contains the cluster Ids for each embedding.\n",
    "    \"\"\"\n",
    "    ## Avoid clustering if there is insufficient data\n",
    "    if len(embeddings) <= dim +1:\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "    \n",
    "    ## Global Dimensionality reduction\n",
    "    reduced_embeddings_global= global_cluster_embeddings(embeddings, dim)\n",
    "    \n",
    "    ## Global Clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "    \n",
    "    all_local_clusters= [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters=0\n",
    "    \n",
    "    ## Iterate through global clustering to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        ## Extract embeddings that belong to global clustes using ids\n",
    "        global_cluster_embeddings_= embeddings[np.array([i in gc for gc in global_clusters])]\n",
    "        \n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim+1:\n",
    "            ## Handle small clusters with direct assignments\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters=1\n",
    "        else:\n",
    "            ##Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local= local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "        \n",
    "        ## Assign local ClusterId, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_= global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j+  total_clusters\n",
    "                )\n",
    "        total_clusters += n_local_clusters\n",
    "    return all_local_clusters\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts: List) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for list of text documents\n",
    "\n",
    "    Args:\n",
    "        texts (List): list of Text documents to be embedded\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: array of embeddings for given text documents\n",
    "    \"\"\"\n",
    "    text_embeddings= embd.embed_documents(texts)\n",
    "    text_embeddings_np= np.array(text_embeddings)\n",
    "    return text_embeddings_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_texts(texts: List):\n",
    "    \"\"\"Embedds list of texts and clusters them, return data frame with texts, their embeddins and cluster labels.\n",
    "    This function combines embedding generation and clustering in a single step. as the function `perform_clustering`\n",
    "    takes care of clustering on the embeddings\n",
    "\n",
    "    Args:\n",
    "        texts (List): list of text documents to be processed.\n",
    "    \n",
    "    Returns:\n",
    "    - Pandas dataframe: Data frame containing original texts, their embeddings and assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np= embed(texts)## Generate embeddings\n",
    "    cluster_labels= perform_clustering(text_embeddings_np, 10, 0.1)# Perform clustering on embeddings\n",
    "    df= pd.DataFrame() ## initialize data frame\n",
    "    df['text']= texts ## Store original texts\n",
    "    df['embd']= list(text_embeddings_np)##Embedded text\n",
    "    df['cluster']= cluster_labels ## Cluster labels\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_txt(df: pd.DataFrame)-> str:\n",
    "    \"\"\"Formats text documents from data frame to text string\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data frame containing 'text' column with text documents to format\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        str: Single string where all the text documents are joined by a specific delimeter\n",
    "    \"\"\"\n",
    "    unique_txt= df['text'].tolist()\n",
    "    return \"--- --- \\n --- ---\".join(unique_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed cluster summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str],\n",
    "    level: int,\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Embeds, clusters and summarize the list of texts. \n",
    "    1. This function first generates embeddings for the texts,\n",
    "    2. clusters them based on similarity\n",
    "    3. expands the cluster assignments for easier processing\n",
    "    4. summarize the content within each cluster.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): A list of text documents to be processed\n",
    "        level (int): integer that defines the depth or detail of processing\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: \n",
    "        1. first data frame: df_clusters: includes original texts, their embeddings and cluster assignments\n",
    "        2. second data frame: df_summary: contains summaries of each cluster, specified level of detail\n",
    "            and cluster identifiers\n",
    "    \"\"\"\n",
    "    # Embed and cluster the texts, resulting in a dataframe with 'text', 'embd', and cluster columns\n",
    "    df_clusters= embed_cluster_texts(texts)\n",
    "    \n",
    "    # Prepare to expand the dataframe for easier manipulation of clusters\n",
    "    expanded_list= []\n",
    "    \n",
    "    # Expand DataFrame entries to document-cluster pairings for straight-forward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row['cluster']:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row['text'], \"embd\":row['embd'], \"cluster\": cluster}\n",
    "            )\n",
    "    ## Create new data frame from expanded list\n",
    "    expanded_df= pd.DataFrame(expanded_list)\n",
    "    \n",
    "    # Retrieve unique cluster identifiers from processing\n",
    "    all_clusters= expanded_df[\"cluster\"].unique()\n",
    "    \n",
    "    print(f\"---Generated {len(all_clusters)} clusters---\")\n",
    "    \n",
    "    ## Summarization\n",
    "    \n",
    "    template= \"\"\"\n",
    "    Here is the sub-set of langchain expression language document.\n",
    "    Langchain Expression languagee provides a way to compose chain in langchain.\n",
    "    Give a detailed summary of documentation provided.\n",
    "    Documentation:\n",
    "    {context}\n",
    "    \n",
    "    \"\"\"\n",
    "    prompt= ChatPromptTemplate.from_template(template=template)\n",
    "    chain= prompt | model | StrOutputParser()\n",
    "    \n",
    "    ## Format text within each cluster for summarization\n",
    "    summaries= []\n",
    "    \n",
    "    for i in all_clusters:\n",
    "        df_cluster= expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt= fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "        \n",
    "    # Creates a dataframe to store summaries with their corresponding cluster and level\n",
    "    df_summary= pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return df_cluster, df_summary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive embed cluster summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str],\n",
    "    level: int =1,\n",
    "    n_levels: int =3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters and summarizes texts upto a specified level or until the number of\n",
    "    unique clusters become 1, storing the results at each level.\n",
    "    \n",
    "    parameters:\n",
    "    - texts: texts to be processes\n",
    "    - level: current recursion level (starts at 1).\n",
    "    - n_levels: maximum depth of recursion.\n",
    "    \n",
    "    Return:\n",
    "    - Dict[int, Tuple(df, df)]: a dictionary where keys are the recursion levels and \n",
    "    values are tuples containing clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store results at each level\n",
    "    results= {}\n",
    "    \n",
    "    # Perform embedding, clustering, and summarizatoin for current level\n",
    "    df_clusters, df_summary= embed_cluster_summarize_texts(texts, level)\n",
    "    \n",
    "    # Store the results of the current level\n",
    "    results[level]= (df_clusters, df_summary)\n",
    "    \n",
    "    # Determine further recursion is possible and meaningful\n",
    "    unique_clusters= df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters >1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts= df_summary[\"summaries\"].tolist()\n",
    "        next_level_results= recursive_embed_cluster_summarize(\n",
    "            new_texts, level+1, n_levels\n",
    "        )\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Generated 1 clusters---\n"
     ]
    }
   ],
   "source": [
    "leaf_texts= docs_texts\n",
    "results= recursive_embed_cluster_summarize(leaf_texts, level=3, n_levels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  \\\n",
       " 0  \\n\\n\\n\\n\\nLangChain Expression Language (LCEL)...   \n",
       " 1  \\n\\n\\n\\n\\nQuickstart | 🦜️🔗 LangChain\\n\\n\\n\\n\\n...   \n",
       " 2  \\n\\n\\n\\n\\nSelf-querying | 🦜️🔗 LangChain\\n\\n\\n\\...   \n",
       " \n",
       "                                                 embd  cluster  \n",
       " 0  [0.011032938733163685, 0.02104789339824041, -0...        0  \n",
       " 1  [-0.0013542183668599336, 0.029847933478814564,...        0  \n",
       " 2  [-0.01196943593841365, 0.02822730685137361, -0...        0  ,\n",
       "                                            summaries  level  cluster\n",
       " 0  The Langchain Expression Language (LCEL) is a ...      3        0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Langchain Expression Language (LCEL) is a declarative way to compose '\n",
      " 'chains together in Langchain. It was designed to support putting prototypes '\n",
      " 'into production without code changes, from simple chains to complex chains '\n",
      " 'with hundreds of steps. Some key features of LCEL include:\\n'\n",
      " '\\n'\n",
      " '1. First-class streaming support: LCEL provides the best possible '\n",
      " 'time-to-first-token, allowing for streaming tokens straight from a language '\n",
      " 'model to a streaming output parser.\\n'\n",
      " '2. Async support: Chains built with LCEL can be called synchronously or '\n",
      " 'asynchronously, enabling the same code to be used for prototypes and '\n",
      " 'production with great performance.\\n'\n",
      " '3. Optimized parallel execution: LCEL automatically executes steps that can '\n",
      " 'be parallelized, reducing latency.\\n'\n",
      " '4. Retries and fallbacks: Configurable retries and fallbacks make chains '\n",
      " 'more reliable at scale.\\n'\n",
      " '5. Access to intermediate results: Users can access intermediate results of '\n",
      " 'complex chains, stream them, and debug the chain.\\n'\n",
      " '6. Input and output schemas: LCEL chains have Pydantic and JSONSchema '\n",
      " 'schemas inferred from the structure of the chain, allowing for validation of '\n",
      " 'inputs and outputs.\\n'\n",
      " '7. Seamless LangSmith tracing: All steps in LCEL chains are automatically '\n",
      " 'logged to LangSmith for observability and debuggability.\\n'\n",
      " '8. Seamless LangServe deployment: Chains created with LCEL can be easily '\n",
      " 'deployed using LangServe.\\n'\n",
      " '\\n'\n",
      " 'Additionally, the documentation provides information on output parsers, '\n",
      " 'self-querying retrievers, and constructing retrievers from scratch using '\n",
      " 'LCEL. The self-querying retriever allows for querying a vector store with '\n",
      " 'built-in support for self-querying, enabling the retriever to query itself '\n",
      " 'based on natural language queries and apply filters to the stored '\n",
      " 'documents.\\n'\n",
      " '\\n'\n",
      " 'Overall, the documentation provides a comprehensive guide on using LCEL to '\n",
      " 'compose chains, work with output parsers, and implement self-querying '\n",
      " 'retrievers in Langchain.')\n"
     ]
    }
   ],
   "source": [
    "pprint(results[3][1]['summaries'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

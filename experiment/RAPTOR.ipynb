{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAPTOR Overview](../RAPTOR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that return number of tokens from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name:str) -> int:\n",
    "    ## We will use tiktoken to get the token details\n",
    "    encoding= tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens= len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string(\"Hello, I am Vikas\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Langchain docs LCEL docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader= RecursiveUrlLoader(\n",
    "    url= url,\n",
    "    max_depth= 20,\n",
    "    extractor= lambda x:Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL with pydantic output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader= RecursiveUrlLoader(\n",
    "    url= url,\n",
    "    max_depth=1,\n",
    "    extractor= lambda x: Soup(x, \"html.parser\").text\n",
    "\n",
    ")\n",
    "docs_pydantic= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LCEL with self query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader= RecursiveUrlLoader(\n",
    "    url= url,\n",
    "    max_depth=1,\n",
    "    extractor= lambda x: Soup(x, \"html.parser\").text\n",
    "\n",
    ")\n",
    "docs_sq= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append all the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts= [d.page_content for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(docs_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate No. of tokens for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts= [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets plot histogram to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins= 50, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token count\")\n",
    "plt.xlabel(\"Token count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COncat all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sorted= sorted(docs, key= lambda x: x.metadata['source'])\n",
    "d_reversed= list(reversed(d_sorted))\n",
    "concatenated_content= \"\\n\\n\\n ----- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "pprint(\"Number of tokens in all content: %s\"\n",
    "       % num_tokens_from_string(concatenated_content, \"cl100k_base\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok= 2000\n",
    "text_splitter= RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size= chunk_size_tok,\n",
    "                                                                    chunk_overlap=0)\n",
    "\n",
    "text_splits= text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(text_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd= OpenAIEmbeddings()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model= ChatOpenAI(temperature=0, model= 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree construction\n",
    "The clustering approach is following few algorithms\n",
    "\n",
    "#### GMM (Gaussian Mixture Model)\n",
    "* Model the distribution of points across different clusters\n",
    "* Optimal number of clusters by evaluating model's Bayesian Information Criterion (IBC)\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "* Reduces the dimensionality of high-dimensional data\n",
    "* supports clustering\n",
    "* Helps to hilight the natural group of data points based on their similarities\n",
    "\n",
    "### Local and global clustering\n",
    "* Used to analyse data and different scales\n",
    "* Both fine-grained and border patterns within the data points based on siilarities\n",
    "\n",
    "### Thresholding\n",
    "* Apply in the context of GMM to determine cluster membership\n",
    "* Based on the probability distribution (assingment of data points >= 1 cluster)\n",
    "---\n",
    "Code for GMM and thresholding is from Sarthi et al, as noted in the below two sources:\n",
    "* [original repo](https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py)\n",
    "* [Minor Tweaks](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/clustering.py)\n",
    "\n",
    "Full credit to both the authors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "RANDOM_SEED= 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "* It is basically combining multiple docs of same intention and clubbing them.\n",
    "* Then create a summary out of those docs\n",
    "* We will get one more layer of summary\n",
    "* Combine summaries with same intention\n",
    "* Create summary from those summary and create one more level.\n",
    "* Continue untill you get single summary or the same summary\n",
    "\n",
    "- Use **GMM** as clustering technique\n",
    "- When using GM you might face slowness due to higher dimensionality\n",
    "- Hence we use UMAP to reduce dimension and considerably less loss compared to other algos such as PCA, t-SNE\n",
    "- To Determine optimal number of clusters we use **BIC** as it will penalise model complexity and reward goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim:int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str=\"cosine\",\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function definition:\n",
    "        Perform global dimensionality reduction on the embeddings using UMAP\n",
    "    Parameters:\n",
    "    - Embeddings: input embeddings\n",
    "    - dim: The target dimensionality for reduced space\n",
    "    - n_neighbors: Optional: Number of neighbors consider for each points.\n",
    "                             If not specified, it defaults to square root of number of embeddings\n",
    "    - metric: The distance metric to use for UMAP\n",
    "    \n",
    "    returns:\n",
    "    - A numpy array of the embbedings reduced to the specified dimensionality.        \n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors= int((len(embeddings)-1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors= n_neighbors, n_components= dim, metric= metric\n",
    "    ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    num_neighbors: int = 10,\n",
    "    metric: str = \"cosine\"\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Funcion definition:\n",
    "    Perform local dimensionality reduction. Basically creates local clustering inside global clustering\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: input embeddings as numpy array\n",
    "    - dim: Targer dimensionality for reduced space (to how much dimension do we have to reduce)\n",
    "    - num_neighbors: Number of neighbors to consider for each point\n",
    "    - metric: Distance to be used for UMAP\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of embeddings reduced to specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(n_neighbors=num_neighbors, n_components=dim,\n",
    "                    metric= metric).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int= RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"Determine the optimal number of clusters using Bayesian Information Criterion (BIC) with a \n",
    "       Gaussian Mixture Model\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): input embeddings of numpy array\n",
    "        max_clusters (int, optional): maximum number of clusters to consider. Defaults to 50.\n",
    "        random_state (int, optional): seed for reproducibility. Defaults to RANDOM_SEED.\n",
    "\n",
    "    Returns:\n",
    "        int: result with optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters= min(max_clusters, len(embeddings))\n",
    "    n_clusters= np.arange(1, max_clusters)\n",
    "    bics= []\n",
    "    for n in n_clusters:\n",
    "        gm= GaussianMixture(n_components=n, random_state= random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "        return n_clusters[np.argmin(bics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GMM Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float,\n",
    "                random_state: int = RANDOM_SEED) -> Tuple:\n",
    "    \"\"\"Cluster embedding using GMM based on probability threshold.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): input embeddings as numpy array\n",
    "        threshold (float): the probability threshold for assigning an embedding to a cluster\n",
    "        random_state (int, optional): Seed for reproducibility. Defaults to RANDOM_SEED.\n",
    "    Returns:\n",
    "            - A tuple containing cluster labels and number of clusters determined.\n",
    "    \"\"\"\n",
    "    ## Get optimal number of clusters using GMM\n",
    "    n_clusters= get_optimal_clusters(embeddings)\n",
    "    ##Create labels and clusters using GMM again\n",
    "    gm= GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs= gm.predict_proba(embeddings)\n",
    "    labels= [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
